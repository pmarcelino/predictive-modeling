{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf937176",
   "metadata": {},
   "source": [
    "# Lecture 6: Programming Example - Statistical Analysis & Pattern Discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0164a9fb",
   "metadata": {},
   "source": [
    "## Introduction: Building Your Statistical Analysis Toolkit\n",
    "\n",
    "Welcome back, junior data consultant! Your Capital Bikes Washington D.C. client was impressed with your data cleaning work and now they need answers to critical business questions. Today, you'll transform from basic data manipulation to sophisticated statistical analysis - discovering the hidden patterns that drive bike-sharing demand using proven statistical techniques.\n",
    "\n",
    "Think of statistical analysis like being a detective examining evidence. While anyone can collect data, professional consultants know how to measure patterns, calculate relationships, and quantify uncertainty. You'll learn to use numbers as proof - transforming \"we think demand is higher on weekends\" into \"weekends show 34% higher average demand with 95% confidence, suggesting we need 50-75 additional bikes at key stations.\"\n",
    "\n",
    "Your client is counting on you to turn their historical bike-sharing data into evidence-based recommendations worth millions in operational improvements. Every statistical technique you master serves this consulting purpose: providing the mathematical proof that drives confident business decisions.\n",
    "\n",
    "> **üöÄ Interactive Learning Alert**\n",
    ">\n",
    "> This is a hands-on statistical analysis tutorial with real transportation data challenges. For the best experience:\n",
    ">\n",
    "> - **Click \"Open in Colab\"** at the bottom to run code interactively\n",
    "> - **Execute each code cell** by pressing **Shift + Enter**\n",
    "> - **Complete the challenges** to practice your statistical analysis skills\n",
    "> - **Think like a consultant** - every insight impacts client decisions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc351a6",
   "metadata": {},
   "source": [
    "## Step 1: Setting Up Your Statistical Analysis Environment\n",
    "\n",
    "Let's begin by importing the essential tools for statistical analysis and loading our bike-sharing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25a4bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas library for data manipulation (as 'pd' for convenience)\n",
    "import pandas as pd\n",
    "\n",
    "# Import numpy library for numerical operations (as 'np' for convenience)\n",
    "import numpy as np\n",
    "\n",
    "# Load the Washington D.C. bike-sharing dataset from GitHub repository\n",
    "# This CSV file contains hourly bike rental data with weather and temporal features\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\")\n",
    "\n",
    "# Convert the datetime column from text strings to proper datetime objects\n",
    "# This enables time-based operations like extracting hour, day, month, etc.\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "# Display basic information about our dataset for context\n",
    "print(\"=== Capital Bikes Dataset Overview ===\")\n",
    "print(f\"Dataset shape: {df.shape}\")  # Shows (rows, columns) - number of observations and features\n",
    "print(f\"Date range: {df['datetime'].min()} to {df['datetime'].max()}\")  # Time period covered\n",
    "print(f\"Total observations: {len(df):,} hours of bike-sharing data\")  # Total data points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cc342e",
   "metadata": {},
   "source": [
    "**What this establishes:**\n",
    "- `pandas` provides all our data manipulation capabilities\n",
    "- `numpy` handles mathematical operations and statistical calculations\n",
    "- Converting datetime ensures proper temporal analysis capabilities\n",
    "- Understanding dataset scope helps interpret statistical results\n",
    "\n",
    "**Why this matters for your client:**\n",
    "Your Capital Bikes client needs to understand the scale and timeframe of their data. These basic statistics set the context for all subsequent analysis - you're working with nearly two years of hourly bike rental data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0adab4f",
   "metadata": {},
   "source": [
    "### Challenge 1: Dataset Familiarization\n",
    "Explore the basic structure of your dataset to understand what variables are available for statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bfbcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "print(\"=== Dataset Structure ===\")\n",
    "print(_____)  # Display column types and missing values\n",
    "\n",
    "print(\"\\n=== First Few Records ===\")\n",
    "print(_____)  # Show first few rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e5c3ff",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° <strong>Tip</strong> (click to expand)</summary>\n",
    "\n",
    "Structure your dataset exploration around these systematic discovery approaches:\n",
    "- Use `df.info()` to see column data types and check for missing values comprehensively\n",
    "- Use `df.head()` and `df.tail()` to preview actual data values and verify data consistency\n",
    "- Pay attention to which columns are numerical vs categorical for appropriate analysis\n",
    "- Examine data ranges with `df.describe()` to understand typical values and outliers\n",
    "- Don't assume all numerical columns should be analyzed the same way\n",
    "- Remember that some numbers might represent categories (like season or weather codes)\n",
    "- Think like a consultant: What would your client want to know about their data structure before diving into statistics?\n",
    "- Consider data quality issues: Are there any obvious errors or inconsistencies?\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438d166e",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ü§´ <strong>Solution</strong> (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "# Display dataset structure\n",
    "print(\"=== Dataset Structure ===\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n=== First Few Records ===\")\n",
    "print(df.head())\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82269d9a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609303c6",
   "metadata": {},
   "source": [
    "## Step 2: Understanding Descriptive Statistics with .describe()\n",
    "\n",
    "Let's use pandas' `.describe()` method to generate comprehensive summary statistics for our bike-sharing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7839c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\")\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "# Generate comprehensive descriptive statistics for all numerical variables\n",
    "print(\"=== Descriptive Statistics for All Numerical Variables ===\")\n",
    "numerical_stats = df.describe()\n",
    "print(numerical_stats)\n",
    "\n",
    "# Focus on key business metrics for better clarity\n",
    "print(\"\\n=== Key Business Metrics Summary ===\")\n",
    "key_metrics = ['temp', 'humidity', 'windspeed', 'count']\n",
    "business_summary = df[key_metrics].describe()\n",
    "print(business_summary.round(2))  # Round for easier reading\n",
    "\n",
    "# Extract specific insights for client presentation\n",
    "total_demand = df['count'].sum()\n",
    "avg_hourly_demand = df['count'].mean()\n",
    "peak_demand = df['count'].max()\n",
    "min_demand = df['count'].min()\n",
    "\n",
    "print(f\"\\n=== Client-Ready Insights ===\")\n",
    "print(f\"Total bike rentals: {total_demand:,}\")\n",
    "print(f\"Average hourly demand: {avg_hourly_demand:.1f} bikes/hour\")\n",
    "print(f\"Peak hourly demand: {peak_demand} bikes\")\n",
    "print(f\"Minimum hourly demand: {min_demand} bikes\")\n",
    "print(f\"Demand variability (std): {df['count'].std():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92906c8c",
   "metadata": {},
   "source": [
    "**What this does:**\n",
    "- `.describe()` generates eight summary statistics (count, mean, std, min, quartiles, max) for all numerical columns\n",
    "- `count` shows data completeness (number of non-null observations)\n",
    "- `mean` reveals average hourly demand - the baseline reference for typical system utilization\n",
    "- `std` (standard deviation) quantifies demand variability around the mean\n",
    "- `min/max` define the full operational range from quietest to busiest hours\n",
    "- Quartiles (25%, 50%, 75%) reveal demand distribution shape and skewness patterns\n",
    "\n",
    "**Business value:**\n",
    "Your Capital Bikes client can use these statistics to set operational baselines, understand demand variability for capacity planning, and identify whether demand distribution is symmetric or skewed by comparing mean with median (50% quartile).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e889455",
   "metadata": {},
   "source": [
    "### Challenge 2: Interpret Weather Statistics\n",
    "Analyze the descriptive statistics for weather variables and interpret what they mean for bike-sharing operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45f4e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "weather_cols = [_____, _____, _____, _____]  # Define weather columns\n",
    "weather_stats = df[weather_cols]._____()  # Generate descriptive statistics\n",
    "print(\"=== Weather Analysis ===\")\n",
    "print(weather_stats.round(2))\n",
    "\n",
    "# Create business interpretations\n",
    "print(f\"\\n=== Weather Insights for Operations ===\")\n",
    "print(f\"Average temperature: {_____:.1f}¬∞C\")  # Calculate mean temperature\n",
    "print(f\"Temperature range: {_____:.1f}¬∞C to {_____:.1f}¬∞C\")  # Show min to max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e344fb5",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° <strong>Tip</strong> (click to expand)</summary>\n",
    "\n",
    "Structure your weather statistics analysis around these key considerations:\n",
    "- Calculate temperature thresholds: `df[df['temp'] > 25]['count'].mean()` for warm days\n",
    "- Examine humidity extremes: `df['humidity'].quantile([0.1, 0.9])` for operational ranges\n",
    "- Interpret standard deviations: high variability indicates unpredictable conditions\n",
    "- Consider operational impact: extreme temperatures affect bike maintenance and user comfort\n",
    "- Compare seasonal weather patterns using `df.groupby('season')[weather_cols].mean()`\n",
    "- Focus on actionable insights: What weather thresholds should trigger operational changes?\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a775bd",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ü§´ <strong>Solution</strong> (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "# Analyze weather statistics\n",
    "weather_cols = ['temp', 'humidity', 'windspeed', 'weather']\n",
    "weather_stats = df[weather_cols].describe()\n",
    "print(\"=== Weather Analysis ===\")\n",
    "print(weather_stats.round(2))\n",
    "\n",
    "# Create business interpretations\n",
    "print(f\"\\n=== Weather Insights for Operations ===\")\n",
    "print(f\"Average temperature: {df['temp'].mean():.1f}¬∞C\")\n",
    "print(f\"Temperature range: {df['temp'].min():.1f}¬∞C to {df['temp'].max():.1f}¬∞C\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed640e4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24d21b1",
   "metadata": {},
   "source": [
    "## Step 3: Seasonal Statistical Comparison with .groupby()\n",
    "\n",
    "Let's use `.groupby()` to compare statistical patterns across seasons and understand how demand varies throughout the year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab53b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\")\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "# Analyze demand patterns by season\n",
    "print(\"=== Seasonal Demand Analysis ===\")\n",
    "seasonal_stats = df.groupby('season')['count'].describe()\n",
    "print(seasonal_stats.round(1))\n",
    "\n",
    "# Create season labels for better interpretation\n",
    "season_names = {1: 'Spring', 2: 'Summer', 3: 'Fall', 4: 'Winter'}\n",
    "df['season_name'] = df['season'].map(season_names)\n",
    "\n",
    "# Compare average demand by season with business context\n",
    "print(\"\\n=== Seasonal Business Impact ===\")\n",
    "seasonal_averages = df.groupby('season_name')['count'].agg(['mean', 'std', 'max']).round(1)\n",
    "print(seasonal_averages)\n",
    "\n",
    "# Calculate seasonal differences for strategic planning\n",
    "spring_avg = df[df['season'] == 1]['count'].mean()\n",
    "summer_avg = df[df['season'] == 2]['count'].mean()\n",
    "fall_avg = df[df['season'] == 3]['count'].mean()\n",
    "winter_avg = df[df['season'] == 4]['count'].mean()\n",
    "\n",
    "print(f\"\\n=== Seasonal Planning Insights ===\")\n",
    "print(f\"Spring average: {spring_avg:.1f} bikes/hour\")\n",
    "print(f\"Summer average: {summer_avg:.1f} bikes/hour\")\n",
    "print(f\"Fall average: {fall_avg:.1f} bikes/hour\")\n",
    "print(f\"Winter average: {winter_avg:.1f} bikes/hour\")\n",
    "print(f\"Summer vs Winter difference: {summer_avg - winter_avg:.1f} bikes/hour ({((summer_avg - winter_avg)/winter_avg)*100:.1f}% increase)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a831ab3",
   "metadata": {},
   "source": [
    "**What this does:**\n",
    "- `.groupby('season')` separates data into distinct seasonal groups (Spring, Summer, Fall, Winter)\n",
    "- `.describe()` applies statistical functions to each group independently, revealing seasonal patterns\n",
    "- `.agg(['mean', 'std', 'max'])` calculates multiple statistics simultaneously for comprehensive comparison\n",
    "- Creating season labels with `.map()` makes output more readable for business stakeholders\n",
    "- Calculating percentage differences quantifies the magnitude of seasonal variations\n",
    "\n",
    "**Key insights from the data:**\n",
    "The statistics reveal dramatic seasonal variations: Fall shows peak demand (234.4 bikes/hour), while Spring has the lowest demand (116.3 bikes/hour) - a 101% difference! The analysis focuses on Summer vs Winter comparison (8.2% difference) to demonstrate percentage calculations, but the full seasonal pattern shows that bike-sharing is highly weather-dependent, with warm-weather seasons (Summer/Fall) significantly outperforming Spring.\n",
    "\n",
    "**Business value:**\n",
    "Your Capital Bikes client can use these seasonal patterns to plan capacity adjustments (doubling fleet capacity from Spring to Fall), schedule maintenance during the low-demand Spring season, and allocate resources based on expected demand fluctuations throughout the year. The modest Summer-Winter difference suggests relatively stable demand in warmer months, while the Spring drop signals a critical period for cost optimization.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c6189c",
   "metadata": {},
   "source": [
    "### Challenge 3: Workday vs Weekend Statistical Analysis\n",
    "Compare bike-sharing demand patterns between workdays and weekends using groupby analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88ebfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Analyze workingday patterns\n",
    "workday_stats = df.groupby(_____)['count']._____()  # Group by workingday and describe\n",
    "print(\"=== Workday vs Weekend Demand ===\")\n",
    "print(workday_stats.round(1))\n",
    "\n",
    "# Create meaningful labels\n",
    "workday_labels = {0: 'Weekend/Holiday', 1: 'Workday'}\n",
    "df['workday_type'] = df['workingday']._____(_____) # Map labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8674caa1",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° <strong>Tip</strong> (click to expand)</summary>\n",
    "\n",
    "Develop your workday analysis to uncover operational patterns:\n",
    "- Compare peak hour differences: `df.groupby(['workingday', 'hour'])['count'].mean()`\n",
    "- Analyze user type variations: `df.groupby('workingday')[['casual', 'registered']].mean()`\n",
    "- Calculate demand consistency: examine standard deviations between workdays vs weekends\n",
    "- Consider revenue implications: workday commuters vs weekend leisure riders have different payment patterns\n",
    "- Examine time-of-day patterns: rush hours are prominent on workdays but not weekends\n",
    "- Remember that workingday=0 includes holidays, which may have unique demand patterns\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab268f64",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ü§´ <strong>Solution</strong> (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "# Analyze workingday patterns\n",
    "workday_stats = df.groupby('workingday')['count'].describe()\n",
    "print(\"=== Workday vs Weekend Demand ===\")\n",
    "print(workday_stats.round(1))\n",
    "\n",
    "# Create meaningful labels\n",
    "workday_labels = {0: 'Weekend/Holiday', 1: 'Workday'}\n",
    "df['workday_type'] = df['workingday'].map(workday_labels)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6031f7e7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04cfb70",
   "metadata": {},
   "source": [
    "## Step 4: Weather Impact Thresholds and Categorical Analysis\n",
    "\n",
    "Let's analyze how different weather conditions affect bike-sharing demand using categorical weather variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7844f99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\")\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "# Analyze demand by weather categories\n",
    "print(\"=== Weather Impact Analysis ===\")\n",
    "weather_demand = df.groupby('weather')['count'].agg(['count', 'mean', 'std']).round(1)\n",
    "print(weather_demand)\n",
    "\n",
    "# Create weather condition labels for business interpretation\n",
    "weather_labels = {\n",
    "    1: 'Clear/Partly Cloudy',\n",
    "    2: 'Mist/Cloudy',\n",
    "    3: 'Light Snow/Rain',\n",
    "    4: 'Heavy Rain/Snow'\n",
    "}\n",
    "df['weather_condition'] = df['weather'].map(weather_labels)\n",
    "\n",
    "# Business-focused weather analysis\n",
    "print(\"\\n=== Weather Condition Business Impact ===\")\n",
    "weather_business = df.groupby('weather_condition')['count'].agg(['mean', 'std', 'min', 'max']).round(1)\n",
    "print(weather_business)\n",
    "\n",
    "# Calculate weather impact percentages for strategic decisions\n",
    "clear_weather_avg = df[df['weather'] == 1]['count'].mean()\n",
    "rainy_weather_avg = df[df['weather'] == 3]['count'].mean()\n",
    "weather_impact = ((clear_weather_avg - rainy_weather_avg) / clear_weather_avg) * 100\n",
    "\n",
    "print(f\"\\n=== Strategic Weather Insights ===\")\n",
    "print(f\"Clear weather average: {clear_weather_avg:.1f} bikes/hour\")\n",
    "print(f\"Rainy weather average: {rainy_weather_avg:.1f} bikes/hour\")\n",
    "print(f\"Rain reduces demand by: {weather_impact:.1f}%\")\n",
    "\n",
    "# Temperature threshold analysis\n",
    "print(f\"\\n=== Temperature Threshold Analysis ===\")\n",
    "hot_days = df[df['temp'] > 30]['count'].mean()\n",
    "cold_days = df[df['temp'] < 10]['count'].mean()\n",
    "ideal_days = df[(df['temp'] >= 15) & (df['temp'] <= 25)]['count'].mean()\n",
    "\n",
    "print(f\"Hot days (>30¬∞C) average: {hot_days:.1f} bikes/hour\")\n",
    "print(f\"Cold days (<10¬∞C) average: {cold_days:.1f} bikes/hour\")\n",
    "print(f\"Ideal temps (15-25¬∞C) average: {ideal_days:.1f} bikes/hour\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af808280",
   "metadata": {},
   "source": [
    "**What this does:**\n",
    "- `.groupby('weather')` separates data by weather condition categories (1=Clear, 2=Mist, 3=Rain, 4=Heavy Rain)\n",
    "- `.agg(['count', 'mean', 'std'])` calculates multiple statistics simultaneously to understand both demand levels and variability\n",
    "- `.map(weather_labels)` transforms numeric codes into descriptive labels for business-friendly reporting\n",
    "- Boolean filtering `df[df['temp'] > 30]` isolates specific temperature ranges for threshold analysis\n",
    "- Percentage calculations quantify the magnitude of weather impacts on demand\n",
    "\n",
    "**Business value:**\n",
    "Your Capital Bikes client can use these weather thresholds to trigger operational adjustments (reducing bike availability during poor weather), plan marketing campaigns (promoting fair-weather cycling), and set realistic demand expectations across different weather scenarios.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6d6f3b",
   "metadata": {},
   "source": [
    "### Challenge 4: Humidity Threshold Impact\n",
    "Investigate how different humidity levels affect bike-sharing demand by creating meaningful humidity categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b01fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Create humidity categories for analysis\n",
    "df['humidity_category'] = pd.cut(df[_____],\n",
    "                                bins=[_____, _____, _____, _____],\n",
    "                                labels=['Low (0-40%)', 'Medium (40-70%)', 'High (70-100%)'])\n",
    "\n",
    "humidity_analysis = df.groupby(_____)[_____].agg(['mean', 'std', 'count']).round(1)\n",
    "print(\"=== Humidity Impact Analysis ===\")\n",
    "print(humidity_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618b8035",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° <strong>Tip</strong> (click to expand)</summary>\n",
    "\n",
    "Build a comprehensive humidity impact analysis for operational decision-making:\n",
    "- Validate category sizes: ensure each humidity category has sufficient observations for reliable statistics\n",
    "- Cross-analyze with temperature: `df.groupby(['humidity_category', 'temp_range'])['count'].mean()`\n",
    "- Examine user type sensitivity: casual users may be more humidity-sensitive than commuters\n",
    "- Calculate comfort index: combine temperature and humidity effects using heat index concepts\n",
    "- Consider seasonal interactions: summer humidity impacts differ from winter humidity\n",
    "- Focus on actionable thresholds: identify humidity levels that significantly reduce demand for operational planning\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f94d076",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ü§´ <strong>Solution</strong> (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "# Create humidity categories for analysis\n",
    "df['humidity_category'] = pd.cut(df['humidity'],\n",
    "                                bins=[0, 40, 70, 100],\n",
    "                                labels=['Low (0-40%)', 'Medium (40-70%)', 'High (70-100%)'])\n",
    "\n",
    "humidity_analysis = df.groupby('humidity_category', observed=True)['count'].agg(['mean', 'std', 'count']).round(1)\n",
    "print(\"=== Humidity Impact Analysis ===\")\n",
    "print(humidity_analysis)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae80ff8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b762e0ea",
   "metadata": {},
   "source": [
    "## Step 5: Correlation Analysis with .corr()\n",
    "\n",
    "Let's use correlation analysis to quantify relationships between variables and identify which factors most strongly influence bike-sharing demand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107ce994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\")\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "# Calculate correlation matrix for numerical variables\n",
    "print(\"=== Correlation Analysis ===\")\n",
    "numerical_vars = ['temp', 'atemp', 'humidity', 'windspeed', 'casual', 'registered', 'count']\n",
    "correlation_matrix = df[numerical_vars].corr()\n",
    "print(correlation_matrix.round(3))\n",
    "\n",
    "# Focus on demand correlations for business insights\n",
    "print(\"\\n=== Variables Most Correlated with Total Demand ===\")\n",
    "demand_correlations = df[numerical_vars].corr()['count'].sort_values(ascending=False)\n",
    "print(demand_correlations.round(3))\n",
    "\n",
    "# Interpret key correlations for client\n",
    "strongest_positive = demand_correlations.drop('count').max()\n",
    "strongest_positive_var = demand_correlations.drop('count').idxmax()\n",
    "strongest_negative = demand_correlations.drop('count').min()\n",
    "strongest_negative_var = demand_correlations.drop('count').idxmin()\n",
    "\n",
    "print(f\"\\n=== Key Correlation Insights ===\")\n",
    "print(f\"Strongest positive correlation: {strongest_positive_var} ({strongest_positive:.3f})\")\n",
    "print(f\"Strongest negative correlation: {strongest_negative_var} ({strongest_negative:.3f})\")\n",
    "\n",
    "# Business interpretation of correlations\n",
    "print(f\"\\n=== Business Interpretation ===\")\n",
    "temp_corr = df['temp'].corr(df['count'])\n",
    "humidity_corr = df['humidity'].corr(df['count'])\n",
    "\n",
    "if temp_corr > 0:\n",
    "    print(f\"Temperature has a positive relationship with demand (r={temp_corr:.3f})\")\n",
    "    print(\"‚Üí Warmer weather encourages more bike rentals\")\n",
    "else:\n",
    "    print(f\"Temperature has a negative relationship with demand (r={temp_corr:.3f})\")\n",
    "\n",
    "if humidity_corr < 0:\n",
    "    print(f\"Humidity has a negative relationship with demand (r={humidity_corr:.3f})\")\n",
    "    print(\"‚Üí High humidity discourages bike rentals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5cad7a",
   "metadata": {},
   "source": [
    "**What this does:**\n",
    "- `.corr()` calculates Pearson correlation coefficients between all numerical variable pairs, measuring linear relationship strength\n",
    "- Correlation values range from -1.0 (perfect negative) to +1.0 (perfect positive), with 0 indicating no linear relationship\n",
    "- `.sort_values(ascending=False)` ranks variables by their correlation strength with demand\n",
    "- `.idxmax()` and `.idxmin()` identify which variables have the strongest positive and negative correlations\n",
    "- Business interpretations translate statistical findings into actionable insights about weather impacts\n",
    "\n",
    "**Business value:**\n",
    "Your Capital Bikes client can identify which external factors drive demand fluctuations. Temperature shows moderate positive correlation (0.394) - warmer days increase rentals. Humidity shows weak negative correlation (-0.317) - uncomfortable conditions deter riders. Windspeed has minimal impact (0.101). These weather correlations enable demand forecasting and operational planning, while the registered/casual breakdown (0.971/0.690) simply reflects that registered users comprise most total rentals, not a causal relationship.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715ae7c2",
   "metadata": {},
   "source": [
    "### Challenge 5: Advanced Correlation Analysis\n",
    "Explore correlations between weather variables and different user types (casual vs registered)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657dac52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "print(\"=== User Type Correlation Analysis ===\")\n",
    "user_weather_corr = df[[_____, _____, _____, _____, _____]].corr()\n",
    "print(user_weather_corr.round(3))\n",
    "\n",
    "# Compare how weather affects different user types\n",
    "casual_temp_corr = df[_____].corr(df[_____])\n",
    "registered_temp_corr = df[_____].corr(df[_____])\n",
    "\n",
    "print(f\"\\n=== User Type Weather Sensitivity ===\")\n",
    "print(f\"Casual users temperature correlation: {casual_temp_corr:.3f}\")\n",
    "print(f\"Registered users temperature correlation: {registered_temp_corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feea5d9",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° <strong>Tip</strong> (click to expand)</summary>\n",
    "\n",
    "Develop user segmentation insights through correlation analysis:\n",
    "- Compare correlation strength differences: which user type shows stronger weather dependency?\n",
    "- Analyze seasonal correlation patterns: `df[df['season']==2].corr()` for summer-specific relationships\n",
    "- Examine multiple weather factors: create correlation matrix for temp, humidity, windspeed by user type\n",
    "- Consider business implications: weather-sensitive segments need different marketing strategies\n",
    "- Focus on actionable insights: which weather conditions drive different user behaviors for targeted operations?\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334e2112",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ü§´ <strong>Solution</strong> (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "# User type correlation analysis\n",
    "print(\"=== User Type Correlation Analysis ===\")\n",
    "user_weather_corr = df[['temp', 'humidity', 'windspeed', 'casual', 'registered']].corr()\n",
    "print(user_weather_corr.round(3))\n",
    "\n",
    "# Compare how weather affects different user types\n",
    "casual_temp_corr = df['temp'].corr(df['casual'])\n",
    "registered_temp_corr = df['temp'].corr(df['registered'])\n",
    "\n",
    "print(f\"\\n=== User Type Weather Sensitivity ===\")\n",
    "print(f\"Casual users temperature correlation: {casual_temp_corr:.3f}\")\n",
    "print(f\"Registered users temperature correlation: {registered_temp_corr:.3f}\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216c8c57",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9543402",
   "metadata": {},
   "source": [
    "## Step 6: Time-Based Statistical Patterns\n",
    "\n",
    "Let's extract time-based features and analyze hourly and daily demand patterns to identify operational planning opportunities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ef3dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\")\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "# Extract time-based features for analysis\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['day_of_week'] = df['datetime'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "df['month'] = df['datetime'].dt.month\n",
    "\n",
    "# Hourly demand patterns\n",
    "print(\"=== Hourly Demand Patterns ===\")\n",
    "hourly_stats = df.groupby('hour')['count'].agg(['mean', 'std', 'min', 'max']).round(1)\n",
    "print(hourly_stats)\n",
    "\n",
    "# Identify peak hours for operations planning\n",
    "peak_morning_hour = df.groupby('hour')['count'].mean().iloc[6:10].idxmax()\n",
    "peak_evening_hour = df.groupby('hour')['count'].mean().iloc[16:20].idxmax()\n",
    "lowest_hour = df.groupby('hour')['count'].mean().idxmin()\n",
    "highest_hour = df.groupby('hour')['count'].mean().idxmax()\n",
    "\n",
    "print(f\"\\n=== Operational Planning Insights ===\")\n",
    "print(f\"Peak morning hour: {peak_morning_hour}:00 ({df.groupby('hour')['count'].mean().iloc[peak_morning_hour]:.1f} bikes/hour)\")\n",
    "print(f\"Peak evening hour: {peak_evening_hour}:00 ({df.groupby('hour')['count'].mean().iloc[peak_evening_hour]:.1f} bikes/hour)\")\n",
    "print(f\"Lowest demand hour: {lowest_hour}:00 ({df.groupby('hour')['count'].mean().iloc[lowest_hour]:.1f} bikes/hour)\")\n",
    "print(f\"Highest demand hour: {highest_hour}:00 ({df.groupby('hour')['count'].mean().iloc[highest_hour]:.1f} bikes/hour)\")\n",
    "\n",
    "# Day of week analysis\n",
    "print(\"\\n=== Day of Week Analysis ===\")\n",
    "day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "df['day_name'] = df['day_of_week'].map(dict(zip(range(7), day_names)))\n",
    "\n",
    "daily_stats = df.groupby('day_name')['count'].agg(['mean', 'std']).round(1)\n",
    "print(daily_stats)\n",
    "\n",
    "# Weekend vs weekday patterns\n",
    "weekend_avg = df[df['day_of_week'].isin([5, 6])]['count'].mean()\n",
    "weekday_avg = df[df['day_of_week'].isin([0, 1, 2, 3, 4])]['count'].mean()\n",
    "\n",
    "print(f\"\\n=== Weekly Pattern Summary ===\")\n",
    "print(f\"Weekend average: {weekend_avg:.1f} bikes/hour\")\n",
    "print(f\"Weekday average: {weekday_avg:.1f} bikes/hour\")\n",
    "print(f\"Weekend vs weekday difference: {weekend_avg - weekday_avg:.1f} bikes/hour\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f31775f",
   "metadata": {},
   "source": [
    "**What this does:**\n",
    "- `.dt.hour`, `.dt.dayofweek`, and `.dt.month` extract time components from datetime objects for temporal analysis\n",
    "- `.iloc[6:10].idxmax()` identifies the specific hour with highest demand within morning rush hour window (6-10am)\n",
    "- `.isin([5, 6])` filters for weekend days (Saturday=5, Sunday=6) to compare weekend vs weekday patterns\n",
    "- `.map(dict(zip(...)))` creates a mapping dictionary that converts numeric day-of-week values (0-6) into readable day names ('Monday'-'Sunday') for professional business reporting. The `zip(range(7), day_names)` pairs each number with its corresponding day name, `dict()` converts these pairs into a lookup dictionary, and `.map()` applies this translation to transform the numeric column into human-readable labels\n",
    "\n",
    "**Key insights from the data:**\n",
    "The hourly pattern reveals dramatic commuter behavior with sharp peaks at 8am (362.8) and 5pm (468.8), yet the weekend/weekday difference is surprisingly small (-4.0 bikes/hour, only 2%). This suggests the system successfully serves both weekday commuters with concentrated rush-hour demand and weekend recreational riders with more distributed usage throughout the day - essentially maintaining similar total volume through different usage patterns.\n",
    "\n",
    "**Business value:**\n",
    "Your Capital Bikes client can use these temporal patterns to optimize bike redistribution schedules (moving bikes to high-demand stations before morning/evening peaks), adjust staffing levels based on hourly demand curves, and recognize that weekend operations require similar capacity but different distribution strategies compared to weekday commuter-focused positioning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f18dca",
   "metadata": {},
   "source": [
    "### Challenge 6: Monthly Seasonal Patterns\n",
    "Analyze how demand varies across months and identify seasonal trends for annual planning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cff9762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "print(\"=== Monthly Demand Analysis ===\")\n",
    "monthly_stats = df.groupby(_____)[_____].agg([_____, _____, _____]).round(1)\n",
    "print(monthly_stats)\n",
    "\n",
    "# Identify peak and low months\n",
    "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "               'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "monthly_avg = df.groupby('month')['count']._____()\n",
    "peak_month = monthly_avg._____()\n",
    "low_month = monthly_avg._____()\n",
    "\n",
    "print(f\"\\n=== Annual Planning Insights ===\")\n",
    "print(f\"Peak month: {month_names[peak_month-1]} ({monthly_avg.iloc[peak_month-1]:.1f} bikes/hour)\")\n",
    "print(f\"Lowest month: {month_names[low_month-1]} ({monthly_avg.iloc[low_month-1]:.1f} bikes/hour)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24835fee",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° <strong>Tip</strong> (click to expand)</summary>\n",
    "\n",
    "Build comprehensive annual planning intelligence from monthly patterns:\n",
    "- Calculate seasonal capacity requirements: `(peak_month_demand / avg_demand) * 100` for scaling factors\n",
    "- Analyze demand growth trends: compare year-over-year monthly patterns using `df['datetime'].dt.year`\n",
    "- Examine weather correlation by month: how do temperature and humidity patterns align with demand cycles?\n",
    "- Consider operational calendar: align maintenance schedules with low-demand months\n",
    "- Calculate revenue implications: `monthly_demand * avg_revenue_per_ride` for financial planning\n",
    "- Identify transition periods: months with highest variability often signal seasonal shifts requiring operational adjustments\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85db49f",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ü§´ <strong>Solution</strong> (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "# Monthly demand analysis\n",
    "print(\"=== Monthly Demand Analysis ===\")\n",
    "monthly_stats = df.groupby('month')['count'].agg(['mean', 'std', 'count']).round(1)\n",
    "print(monthly_stats)\n",
    "\n",
    "# Identify peak and low months\n",
    "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "               'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "monthly_avg = df.groupby('month')['count'].mean()\n",
    "peak_month = monthly_avg.idxmax()\n",
    "low_month = monthly_avg.idxmin()\n",
    "\n",
    "print(f\"\\n=== Annual Planning Insights ===\")\n",
    "print(f\"Peak month: {month_names[peak_month-1]} ({monthly_avg.iloc[peak_month-1]:.1f} bikes/hour)\")\n",
    "print(f\"Lowest month: {month_names[low_month-1]} ({monthly_avg.iloc[low_month-1]:.1f} bikes/hour)\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079b7ad3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab18e5c",
   "metadata": {},
   "source": [
    "## Step 7: Comprehensive Statistical Summary and Business Insights\n",
    "\n",
    "Let's create a professional statistical report that synthesizes all our analyses into actionable business intelligence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b14bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\")\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "# Create necessary derived features for the report\n",
    "season_names = {1: 'Spring', 2: 'Summer', 3: 'Fall', 4: 'Winter'}\n",
    "df['season_name'] = df['season'].map(season_names)\n",
    "\n",
    "weather_labels = {\n",
    "    1: 'Clear/Partly Cloudy',\n",
    "    2: 'Mist/Cloudy',\n",
    "    3: 'Light Snow/Rain',\n",
    "    4: 'Heavy Rain/Snow'\n",
    "}\n",
    "df['weather_condition'] = df['weather'].map(weather_labels)\n",
    "\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CAPITAL BIKES WASHINGTON D.C. - STATISTICAL ANALYSIS REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Executive Summary Statistics\n",
    "print(\"\\nüìä EXECUTIVE SUMMARY\")\n",
    "print(\"-\" * 40)\n",
    "total_hours = len(df)\n",
    "total_rentals = df['count'].sum()\n",
    "avg_hourly_demand = df['count'].mean()\n",
    "demand_variability = df['count'].std()\n",
    "\n",
    "print(f\"Analysis Period: {df['datetime'].min().strftime('%Y-%m-%d')} to {df['datetime'].max().strftime('%Y-%m-%d')}\")\n",
    "print(f\"Total Data Points: {total_hours:,} hours\")\n",
    "print(f\"Total Bike Rentals: {total_rentals:,} rentals\")\n",
    "print(f\"Average Hourly Demand: {avg_hourly_demand:.1f} bikes/hour\")\n",
    "print(f\"Demand Variability (œÉ): {demand_variability:.1f}\")\n",
    "\n",
    "# Key Performance Indicators\n",
    "print(f\"\\nüìà KEY PERFORMANCE INDICATORS\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Peak Hourly Demand: {df['count'].max()} bikes/hour\")\n",
    "print(f\"95th Percentile Demand: {df['count'].quantile(0.95):.0f} bikes/hour\")\n",
    "print(f\"Median Demand: {df['count'].median():.0f} bikes/hour\")\n",
    "print(f\"Hours with Zero Demand: {(df['count'] == 0).sum()} hours ({((df['count'] == 0).sum()/len(df))*100:.1f}%)\")\n",
    "\n",
    "# Seasonal Business Intelligence\n",
    "print(f\"\\nüå∏ SEASONAL PATTERNS\")\n",
    "print(\"-\" * 40)\n",
    "seasonal_summary = df.groupby('season_name')['count'].agg(['mean', 'std']).round(1)\n",
    "for season, stats in seasonal_summary.iterrows():\n",
    "    print(f\"{season}: {stats['mean']:.1f} ¬± {stats['std']:.1f} bikes/hour\")\n",
    "\n",
    "# Weather Impact Assessment\n",
    "print(f\"\\nüå§Ô∏è  WEATHER IMPACT ASSESSMENT\")\n",
    "print(\"-\" * 40)\n",
    "weather_impact = df.groupby('weather_condition')['count'].mean().round(1)\n",
    "for condition, avg_demand in weather_impact.items():\n",
    "    percentage_of_clear = (avg_demand / weather_impact['Clear/Partly Cloudy']) * 100\n",
    "    print(f\"{condition}: {avg_demand:.1f} bikes/hour ({percentage_of_clear:.0f}% of clear weather)\")\n",
    "\n",
    "# Operational Insights\n",
    "print(f\"\\n‚öôÔ∏è  OPERATIONAL INSIGHTS\")\n",
    "print(\"-\" * 40)\n",
    "rush_morning = df[df['hour'].isin([7, 8, 9])]['count'].mean()\n",
    "rush_evening = df[df['hour'].isin([17, 18, 19])]['count'].mean()\n",
    "off_peak = df[df['hour'].isin([1, 2, 3, 4])]['count'].mean()\n",
    "\n",
    "print(f\"Morning Rush (7-9am): {rush_morning:.1f} bikes/hour\")\n",
    "print(f\"Evening Rush (5-7pm): {rush_evening:.1f} bikes/hour\")\n",
    "print(f\"Off-Peak (1-4am): {off_peak:.1f} bikes/hour\")\n",
    "print(f\"Rush Hour Multiplier: {rush_evening/off_peak:.1f}x off-peak demand\")\n",
    "\n",
    "# User Behavior Analysis\n",
    "print(f\"\\nüë• USER BEHAVIOR ANALYSIS\")\n",
    "print(\"-\" * 40)\n",
    "casual_percentage = (df['casual'].sum() / df['count'].sum()) * 100\n",
    "registered_percentage = (df['registered'].sum() / df['count'].sum()) * 100\n",
    "casual_weather_sensitivity = abs(df[df['weather']==1]['casual'].mean() - df[df['weather']==3]['casual'].mean())\n",
    "registered_weather_sensitivity = abs(df[df['weather']==1]['registered'].mean() - df[df['weather']==3]['registered'].mean())\n",
    "\n",
    "print(f\"Casual Users: {casual_percentage:.1f}% of total demand\")\n",
    "print(f\"Registered Users: {registered_percentage:.1f}% of total demand\")\n",
    "print(f\"Casual Weather Sensitivity: {casual_weather_sensitivity:.1f} bikes/hour difference\")\n",
    "print(f\"Registered Weather Sensitivity: {registered_weather_sensitivity:.1f} bikes/hour difference\")\n",
    "\n",
    "# Strategic Recommendations\n",
    "print(f\"\\nüí° KEY STRATEGIC INSIGHTS\")\n",
    "print(\"-\" * 40)\n",
    "print(\"1. SEASONAL PLANNING:\")\n",
    "peak_season = seasonal_summary['mean'].idxmax()\n",
    "low_season = seasonal_summary['mean'].idxmin()\n",
    "seasonal_difference = seasonal_summary.loc[peak_season, 'mean'] - seasonal_summary.loc[low_season, 'mean']\n",
    "print(f\"   ‚Ä¢ {seasonal_difference:.0f} bikes/hour difference between {peak_season} and {low_season}\")\n",
    "print(f\"   ‚Ä¢ Plan for {seasonal_difference/avg_hourly_demand*100:.0f}% seasonal capacity variation\")\n",
    "\n",
    "print(\"\\n2. WEATHER CONTINGENCY:\")\n",
    "weather_loss = weather_impact['Clear/Partly Cloudy'] - weather_impact['Light Snow/Rain']\n",
    "print(f\"   ‚Ä¢ Poor weather reduces demand by {weather_loss:.0f} bikes/hour\")\n",
    "print(f\"   ‚Ä¢ Represents {(weather_loss/weather_impact['Clear/Partly Cloudy'])*100:.0f}% demand loss in bad weather\")\n",
    "\n",
    "print(\"\\n3. OPERATIONAL EFFICIENCY:\")\n",
    "peak_capacity_need = df['count'].quantile(0.95)\n",
    "avg_capacity_need = df['count'].mean()\n",
    "efficiency_ratio = peak_capacity_need / avg_capacity_need\n",
    "print(f\"   ‚Ä¢ Peak capacity needs are {efficiency_ratio:.1f}x average demand\")\n",
    "print(f\"   ‚Ä¢ Consider dynamic pricing during peak hours ({df.groupby('hour')['count'].mean().idxmax()}:00)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Report generated using pandas statistical analysis methods\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afce32b8",
   "metadata": {},
   "source": [
    "**What this does:**\n",
    "- Synthesizes multiple statistical analyses into a cohesive executive summary with clearly labeled sections\n",
    "- Uses `.quantile(0.95)` to identify capacity planning thresholds that cover 95% of demand scenarios\n",
    "- Calculates weather sensitivity differences to quantify user segment behavioral patterns\n",
    "- Employs `.iterrows()` for formatted iteration through grouped data for professional reporting\n",
    "- Combines descriptive statistics with business interpretations to translate technical findings into strategic recommendations\n",
    "\n",
    "**Business value:**\n",
    "Your Capital Bikes client receives a complete statistical intelligence report that summarizes all key findings, quantifies operational requirements (peak capacity is 2-3x average), identifies revenue optimization opportunities (dynamic pricing during peak hours), and provides evidence-based recommendations for seasonal planning, weather contingency, and user segment targeting.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f7b936",
   "metadata": {},
   "source": [
    "### Challenge 8: Create Your Own Statistical Insight\n",
    "Using the statistical techniques you've learned, investigate one additional pattern or relationship in the data that could provide business value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fee964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "print(\"=== CUSTOM STATISTICAL ANALYSIS ===\")\n",
    "# Example: Analyze the relationship between temperature and user types\n",
    "# Or investigate windspeed impact on different weather conditions\n",
    "# Or examine holiday patterns vs regular days\n",
    "\n",
    "# Choose your own analysis focus:\n",
    "# Option 1: Holiday impact analysis\n",
    "holiday_analysis = df.groupby(_____)[['count', 'casual', 'registered']].mean().round(1)\n",
    "print(\"Holiday Impact Analysis:\")\n",
    "print(holiday_analysis)\n",
    "\n",
    "# Option 2: Temperature-humidity interaction\n",
    "# Create categories and analyze their combined effect\n",
    "# Your statistical investigation here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9bd7b3",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° <strong>Tip</strong> (click to expand)</summary>\n",
    "\n",
    "Design original statistical analysis that delivers unique business value:\n",
    "- Choose a compelling business question: \"How do holidays affect different user segments?\" or \"What weather combinations optimize revenue?\"\n",
    "- Combine multiple techniques: use groupby, correlations, and descriptive statistics together for comprehensive insights\n",
    "- Create interaction analyses: examine how multiple variables work together (e.g., temperature + humidity effects)\n",
    "- Calculate business metrics: convert statistical findings into revenue impact, capacity requirements, or operational costs\n",
    "- Provide actionable recommendations: ensure your analysis leads to specific business decisions\n",
    "- Validate findings: check that your results align with domain expertise and logical expectations\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d65312",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ü§´ <strong>Solution</strong> (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "# Custom statistical analysis example: Holiday impact\n",
    "print(\"=== CUSTOM STATISTICAL ANALYSIS ===\")\n",
    "\n",
    "# Option 1: Holiday impact analysis\n",
    "holiday_analysis = df.groupby('holiday')[['count', 'casual', 'registered']].mean().round(1)\n",
    "print(\"Holiday Impact Analysis:\")\n",
    "print(holiday_analysis)\n",
    "\n",
    "# Option 2: Temperature-humidity interaction analysis\n",
    "print(\"\\n=== Temperature-Humidity Interaction ===\")\n",
    "# Create temperature categories\n",
    "df['temp_category'] = pd.cut(df['temp'],\n",
    "                             bins=[0, 15, 25, 50],\n",
    "                             labels=['Cold (<15¬∞C)', 'Comfortable (15-25¬∞C)', 'Hot (>25¬∞C)'])\n",
    "\n",
    "# Create humidity categories (if not already created)\n",
    "df['humidity_category'] = pd.cut(df['humidity'],\n",
    "                                bins=[0, 40, 70, 100],\n",
    "                                labels=['Low (0-40%)', 'Medium (40-70%)', 'High (70-100%)'])\n",
    "\n",
    "# Analyze combined effects\n",
    "interaction_analysis = df.groupby(['temp_category', 'humidity_category'], observed=True)['count'].mean().round(1)\n",
    "print(interaction_analysis)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4653205b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63be8174",
   "metadata": {},
   "source": [
    "## Summary: Professional Statistical Analysis and Pattern Discovery Techniques\n",
    "\n",
    "**What We've Accomplished**:\n",
    "- Implemented descriptive statistics analysis using `.describe()` to calculate central tendency and variability measures\n",
    "- Applied `.groupby()` analysis for seasonal, temporal, and weather pattern comparisons\n",
    "- Executed correlation analysis with `.corr()` to quantify relationships between weather variables and bike demand\n",
    "- Created categorical analysis frameworks using `pd.cut()` to analyze temperature and humidity thresholds\n",
    "- Built professional statistical reports with executive summaries and operational insights\n",
    "\n",
    "**Key Technical Skills Mastered**:\n",
    "- Central tendency measurement with `.mean()` and `.median()` for demand characterization\n",
    "- Variability measurement with `.std()`, `.min()`, and `.max()` for capacity planning\n",
    "- Multi-dimensional aggregation using `.groupby()` with `.agg()` for seasonal and temporal analysis\n",
    "- Correlation matrix creation and interpretation with `.corr()` for relationship strength assessment\n",
    "- Time-based feature extraction using `.dt.hour`, `.dt.dayofweek`, and `.dt.month` for temporal patterns\n",
    "- Categorical variable creation with `pd.cut()` for threshold analysis\n",
    "- Boolean filtering with conditional statements for category-specific analysis\n",
    "- Statistical ranking with `.sort_values()`, `.idxmax()`, and `.idxmin()` for peak identification\n",
    "\n",
    "**Next Steps**: Next, we'll advance to professional data visualization and chart creation techniques, mastering matplotlib frameworks that transform your statistical discoveries into compelling visual narratives and stakeholder-ready analytical presentations for maximum business impact.\n",
    "\n",
    "Your bike-sharing client now possesses rigorous statistical intelligence backed by mathematical analysis that reveals actionable demand patterns, operational optimization opportunities, and user behavior insights - demonstrating the quantitative analytical expertise that separates professional transportation consultants from basic data analysts!"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
