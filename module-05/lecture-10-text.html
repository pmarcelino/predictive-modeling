<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Lecture 10: Text - Predictive Modeling MOOC</title>

    <style>
      body {
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Roboto",
          "Helvetica", "Arial", sans-serif;
        line-height: 1.6;
        max-width: 1200px;
        margin: 0 auto;
        padding: 20px;
        background-color: #f8f9fa;
      }

      .container {
        background-color: white;
        border-radius: 8px;
        padding: 30px;
        box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
      }

      .nav-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 20px 30px;
        border-radius: 8px 8px 0 0;
        margin: -30px -30px 30px -30px;
        display: flex;
        justify-content: space-between;
        align-items: center;
        flex-wrap: wrap;
        gap: 15px;
      }

      .nav-header h1 {
        margin: 0;
        font-size: 1.5rem;
        font-weight: 600;
      }

      .nav-buttons {
        display: flex;
        gap: 10px;
        flex-wrap: wrap;
      }

      .btn {
        display: inline-block;
        padding: 10px 20px;
        text-decoration: none;
        font-weight: 600;
        border-radius: 6px;
        transition: all 0.3s ease;
        font-size: 0.9rem;
      }

      .btn-colab {
        background-color: #f9ab00;
        color: #1a1a1a;
        border: 2px solid #f9ab00;
      }

      .btn-colab:hover {
        background-color: #ff9900;
        border-color: #ff9900;
        transform: translateY(-2px);
        box-shadow: 0 4px 12px rgba(249, 171, 0, 0.3);
      }

      .btn-github {
        background-color: transparent;
        color: white;
        border: 2px solid white;
      }

      .btn-github:hover {
        background-color: white;
        color: #667eea;
        transform: translateY(-2px);
        box-shadow: 0 4px 12px rgba(255, 255, 255, 0.3);
      }

      .jupyter-content {
        margin-top: 20px;
      }

      /* Notebook cell styling */
      div.cell {
        margin-bottom: 1.5rem;
      }

      div.input_area {
        border-left: 3px solid #667eea;
        background-color: #f8f9fa;
        padding: 10px;
        border-radius: 4px;
      }

      div.output_area {
        padding: 10px;
        border-left: 3px solid #28a745;
        background-color: #f8f9fa;
        border-radius: 4px;
        margin-top: 10px;
      }

      /* Code highlighting */
      .highlight {
        background-color: #f8f9fa;
        border-radius: 4px;
      }

      /* Responsive design */
      @media (max-width: 768px) {
        body {
          padding: 10px;
        }

        .container {
          padding: 15px;
        }

        .nav-header {
          padding: 15px;
          margin: -15px -15px 15px -15px;
        }

        .nav-header h1 {
          font-size: 1.2rem;
        }

        .nav-buttons {
          width: 100%;
          justify-content: center;
        }
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="nav-header">
        <h1>Lecture 10: Text</h1>
        <div class="nav-buttons">
          <a
            href="https://colab.research.google.com/github/pmarcelino/predictive-modeling/blob/main/module-05/lecture-10-text.ipynb"
            target="_blank"
            rel="noopener"
            class="btn btn-colab"
          >
            â–¶ Open in Colab
          </a>
          <a
            href="https://github.com/pmarcelino/predictive-modeling/blob/main/module-05/lecture-10-text.ipynb"
            target="_blank"
            rel="noopener"
            class="btn btn-github"
          >
            ðŸ“‚ View on GitHub
          </a>
        </div>
      </div>
      <div class="jupyter-content">
        <main>
          <div class="border-box-sizing" id="notebook" tabindex="-1">
            <div class="container" id="notebook-container">
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=30b69edc"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h1
                      id="Lecture-10:-Model-Evaluation-&amp;-Performance-Assessment"
                      >Lecture 10: Model Evaluation &amp; Performance
                      Assessment<a
                        class="anchor-link"
                        href="#Lecture-10:-Model-Evaluation-&amp;-Performance-Assessment"
                      ></a
                    ></h1>
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=e0431dac"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h2 id="Learning-Objectives"
                      >Learning Objectives<a
                        class="anchor-link"
                        href="#Learning-Objectives"
                      ></a></h2
                    ><p>By the end of this lecture, you will be able to:</p>
                    <ul>
                      <li
                        >Define key regression metrics (RÂ², MAE, MSE, RMSE) and
                        explain the mathematical foundation behind each
                        evaluation approach for bike-sharing demand
                        forecasting</li
                      >
                      <li
                        >Compare multiple algorithms systematically using
                        consistent evaluation frameworks that enable data-driven
                        model selection decisions</li
                      >
                      <li
                        >Interpret evaluation results within business contexts,
                        translating technical performance metrics into
                        operational decision-making criteria</li
                      >
                    </ul>
                    <hr />
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=2e940ddf"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h2
                      id="1.-Your-Evaluation-Challenge:-From-Good-Models-to-Proven-Performance"
                      >1. Your Evaluation Challenge: From Good Models to Proven
                      Performance<a
                        class="anchor-link"
                        href="#1.-Your-Evaluation-Challenge:-From-Good-Models-to-Proven-Performance"
                      ></a></h2
                    ><p
                      >Ten months into your consulting engagement with Capital
                      City Bikes, your Random Forest models have revolutionized
                      operations. Fleet utilization has improved, customer
                      satisfaction scores lead the industry, and your
                      predictions consistently outperform competitors during
                      complex scenarios. But
                      <strong
                        >success has attracted scrutinyâ€”and raised the
                        stakes</strong
                      >.</p
                    >
                    <p
                      >The company's Series B funding round brought board
                      members demanding <strong>rigorous validation</strong>. A
                      <strong
                        >â‚¬2.3 million municipal contract requires documented
                        performance guarantees</strong
                      >
                      across a six-month pilot. Most critically, expansion into
                      three new cities depends on models that perform reliably
                      across diverse urban environmentsâ€”<strong
                        >precision that demands mathematical proof, not just
                        promising results</strong
                      >.</p
                    >
                    <p
                      >Your CEO presents the challenge: "We've proven
                      sophisticated models give us competitive advantage, but
                      now we need
                      <strong>absolute confidence in their performance</strong>.
                      Our expansion investors want
                      <strong
                        >systematic evaluation, not just good results</strong
                      >. The municipal contract requires performance guarantees
                      that could make or break our growth trajectory. How do we
                      move from 'working well' to 'working optimally' with the
                      reliability that million-dollar contracts demand?"</p
                    >
                    <p
                      >This represents the moment when
                      <strong
                        >technical excellence must be proven through
                        mathematical rigor</strong
                      >. Just as pharmaceutical companies must prove drug
                      efficacy through clinical trials before FDA approval, you
                      must prove model performance through evaluation frameworks
                      that satisfy investors, regulators, and business partners
                      staking millions on your predictions.</p
                    >
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=ed281050"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h2 id="2.-Understanding-Performance-Metrics"
                      >2. Understanding Performance Metrics<a
                        class="anchor-link"
                        href="#2.-Understanding-Performance-Metrics"
                      ></a></h2
                    ><p
                      >This section establishes the
                      <strong
                        >mathematical foundations of regression evaluation
                        metrics</strong
                      >
                      essential for quantifying bike-sharing demand prediction
                      accuracy. We'll explore
                      <strong>four fundamental metrics</strong> - MAE, MSE,
                      RMSE, and RÂ² - demonstrating calculation through code
                      examples and progressing to business interpretation within
                      transportation contexts. Note that some of these metrics
                      were already introduced and used throughout the course,
                      but here we'll provide
                      <strong
                        >comprehensive coverage of their mathematical
                        foundations and business applications</strong
                      >. Each metric provides unique insights into model
                      performance, and understanding their mathematical basis
                      enables confident metric selection for specific
                      operational requirements.</p
                    >
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=37a247f9"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h2
                      id="2.1.-Mean-Absolute-Error-(MAE):-Interpretable-Prediction-Accuracy"
                      >2.1. Mean Absolute Error (MAE): Interpretable Prediction
                      Accuracy<a
                        class="anchor-link"
                        href="#2.1.-Mean-Absolute-Error-(MAE):-Interpretable-Prediction-Accuracy"
                      ></a></h2
                    ><p
                      ><strong
                        >Mean Absolute Error represents the most intuitive
                        evaluation metric</strong
                      >
                      for bike-sharing operations because it
                      <strong
                        >directly measures prediction accuracy in the original
                        bike count units</strong
                      >. Think of MAE like measuring accuracy in archery: if you
                      shoot 10 arrows at a target and calculate the average
                      distance each arrow lands from the bullseye, you get a
                      simple, interpretable measure of your accuracy. MAE works
                      the same wayâ€”it calculates the average distance your
                      predictions land from the actual values. This section
                      introduces MAE through mathematical definition,
                      demonstrates calculation with code examples, and
                      establishes its business value for operational
                      communication.</p
                    >
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=ad86150b"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h3 id="Mathematical-Foundation"
                      >Mathematical Foundation<a
                        class="anchor-link"
                        href="#Mathematical-Foundation"
                      ></a></h3
                    ><p
                      >The Mean Absolute Error
                      <strong
                        >calculates the average absolute difference</strong
                      >
                      between actual and predicted values:</p
                    >
                    <p
                      >$$\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i -
                      \hat{y}_i|$$</p
                    >
                    <p>Where:</p>
                    <ul>
                      <li><strong>n</strong> = number of predictions</li>
                      <li><strong>yáµ¢</strong> = actual bike count at time i</li>
                      <li
                        ><strong>Å·áµ¢</strong> = predicted bike count at time
                        i</li
                      >
                      <li><strong>|...|</strong> = absolute value function</li>
                    </ul>
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=3f74dc10"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h3 id="Computing-MAE-with-Code"
                      >Computing MAE with Code<a
                        class="anchor-link"
                        href="#Computing-MAE-with-Code"
                      ></a></h3
                    ><p
                      >Let's calculate MAE using both manual computation and
                      scikit-learn's metrics module to understand the
                      calculation process and verify our implementation:</p
                    >
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing code_cell rendered"
                id="cell-id=3e1953ca"
              >
                <div class="input">
                  <div class="prompt input_prompt">InÂ [1]:</div>
                  <div class="inner_cell">
                    <div class="input_area">
                      <div class="highlight hl-ipython3">
                        <pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="c1"># mean_absolute_error() automatically computes MAE by implementing the formula:</span>
<span class="c1"># MAE = (1/n) * Î£|actual - predicted|, returning average absolute error</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">mean_absolute_error</span>

<span class="c1"># Load the bike-sharing dataset</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv"</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'datetime'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">'datetime'</span><span class="p">])</span>

<span class="c1"># Extract temporal features</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'hour'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'datetime'</span><span class="p">]</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">hour</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'month'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'datetime'</span><span class="p">]</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">month</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'weekday'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'datetime'</span><span class="p">]</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">weekday</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"=== MEAN ABSOLUTE ERROR (MAE) CALCULATION ===</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>

<span class="c1"># Prepare features and target</span>
<span class="n">feature_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'temp'</span><span class="p">,</span> <span class="s1">'humidity'</span><span class="p">,</span> <span class="s1">'windspeed'</span><span class="p">,</span> <span class="s1">'hour'</span><span class="p">,</span> <span class="s1">'weekday'</span><span class="p">,</span> <span class="s1">'month'</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">feature_cols</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'count'</span><span class="p">]</span>

<span class="c1"># Chronological split: first 80% for training, last 20% for testing</span>
<span class="n">split_idx</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.8</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="n">split_idx</span><span class="p">],</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">split_idx</span><span class="p">:]</span>
<span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="n">split_idx</span><span class="p">],</span> <span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">split_idx</span><span class="p">:]</span>

<span class="c1"># Train Random Forest model</span>
<span class="n">rf_model</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="s1">'sqrt'</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">rf_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Generate predictions</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">rf_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"--- Manual MAE Calculation ---"</span><span class="p">)</span>
<span class="c1"># Calculate absolute errors manually using np.abs()</span>
<span class="c1"># np.abs() is NumPy's absolute value function that works element-wise on arrays,</span>
<span class="c1"># converting each difference to its positive magnitude: |actual - predicted|</span>
<span class="n">absolute_errors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">y_test</span><span class="o">.</span><span class="n">values</span> <span class="o">-</span> <span class="n">predictions</span><span class="p">)</span>
<span class="n">manual_mae</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">absolute_errors</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"First 5 predictions:"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">actual</span> <span class="o">=</span> <span class="n">y_test</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">predictions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">absolute_errors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  Observation </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: Actual=</span><span class="si">{</span><span class="n">actual</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">, Predicted=</span><span class="si">{</span><span class="n">pred</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">, |Error|=</span><span class="si">{</span><span class="n">error</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2"> bikes"</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Total observations: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Sum of absolute errors: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">absolute_errors</span><span class="p">)</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2"> bikes"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Manual MAE: </span><span class="si">{</span><span class="n">manual_mae</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> bikes per hour"</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">--- Scikit-learn MAE Calculation ---"</span><span class="p">)</span>
<span class="c1"># mean_absolute_error() computes MAE automatically</span>
<span class="n">sklearn_mae</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Scikit-learn MAE: </span><span class="si">{</span><span class="n">sklearn_mae</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> bikes per hour"</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">--- Verification ---"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Calculations match: </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">manual_mae</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">sklearn_mae</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mf">0.01</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">--- Business Interpretation ---"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"MAE = </span><span class="si">{</span><span class="n">sklearn_mae</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2"> bikes per hour means:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  â€¢ On average, predictions are off by </span><span class="si">{</span><span class="n">sklearn_mae</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2"> bikes"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  â€¢ Operations should plan for Â±</span><span class="si">{</span><span class="n">sklearn_mae</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2"> bike adjustments"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  â€¢ Fleet repositioning needs </span><span class="si">{</span><span class="n">sklearn_mae</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">-bike buffer"</span><span class="p">)</span>
</pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div class="output_wrapper">
                  <div class="output">
                    <div class="output_area">
                      <div class="prompt"></div>
                      <div
                        class="output_subarea output_stream output_stdout output_text"
                      >
                        <pre>
=== MEAN ABSOLUTE ERROR (MAE) CALCULATION ===

--- Manual MAE Calculation ---
First 5 predictions:
  Observation 1: Actual=19, Predicted=19, |Error|=0 bikes
  Observation 2: Actual=19, Predicted=40, |Error|=21 bikes
  Observation 3: Actual=68, Predicted=150, |Error|=82 bikes
  Observation 4: Actual=108, Predicted=230, |Error|=122 bikes
  Observation 5: Actual=229, Predicted=289, |Error|=60 bikes

Total observations: 2,178
Sum of absolute errors: 196861 bikes
Manual MAE: 90.39 bikes per hour

--- Scikit-learn MAE Calculation ---
Scikit-learn MAE: 90.39 bikes per hour

--- Verification ---
Calculations match: True

--- Business Interpretation ---
MAE = 90 bikes per hour means:
  â€¢ On average, predictions are off by 90 bikes
  â€¢ Operations should plan for Â±90 bike adjustments
  â€¢ Fleet repositioning needs 90-bike buffer
</pre
                        >
                      </div>
                    </div>
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=a4f6bc8f"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <p><strong>What this demonstrates:</strong></p>
                    <ul>
                      <li
                        ><strong>Direct interpretability</strong> - MAE of 90
                        bikes means typical prediction errors of 90 bikes per
                        hour</li
                      >
                      <li
                        ><strong>Operational translation</strong> -
                        redistribution teams should expect average system-wide
                        mismatches of 90 bikes</li
                      >
                      <li
                        ><strong>Manual vs automated calculation</strong>
                        produces identical results, validating our
                        understanding</li
                      >
                      <li
                        ><strong>Business-ready metric</strong> - executives
                        immediately understand "predictions typically off by 90
                        bikes"</li
                      >
                    </ul>
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=06dc6573"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h3 id="Business-Interpretation-and-Operational-Value"
                      >Business Interpretation and Operational Value<a
                        class="anchor-link"
                        href="#Business-Interpretation-and-Operational-Value"
                      ></a></h3
                    ><p
                      >MAE provides
                      <strong>immediate operational relevance</strong> for
                      bike-sharing fleet management. An MAE of 90 bikes per hour
                      <strong
                        >directly translates to redistribution planning</strong
                      >: operations teams can expect average allocation
                      differences of 90 bikes when deploying resources based on
                      model predictions.</p
                    >
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=2deb7b13"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h2
                      id="2.2.-Mean-Squared-Error-(MSE):-Mathematical-Foundation-Metric"
                      >2.2. Mean Squared Error (MSE): Mathematical Foundation
                      Metric<a
                        class="anchor-link"
                        href="#2.2.-Mean-Squared-Error-(MSE):-Mathematical-Foundation-Metric"
                      ></a></h2
                    ><p
                      ><strong
                        >Mean Squared Error serves as the mathematical
                        optimization objective</strong
                      >
                      for many machine learning algorithms and provides
                      essential understanding of model training dynamics. Think
                      of MSE like a teacher grading an exam who
                      <strong>penalizes larger mistakes more heavily</strong>: a
                      student making one 50-point error gets penalized much more
                      severely (50Â² = 2,500 penalty points) than a student
                      making five 10-point errors (5 Ã— 10Â² = 500 penalty
                      points), even though both students got 50 points wrong in
                      total. This
                      <strong>"penalty amplification" for large errors</strong>
                      makes MSE particularly useful for training algorithms that
                      need to avoid large mistakes. This section explores MSE's
                      mathematical properties, demonstrates calculation through
                      code examples, and establishes its relationship to
                      algorithm optimization.</p
                    >
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=b73d60af"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h3 id="Mathematical-Foundation"
                      >Mathematical Foundation<a
                        class="anchor-link"
                        href="#Mathematical-Foundation"
                      ></a></h3
                    ><p
                      >The Mean Squared Error
                      <strong
                        >squares each prediction error before averaging</strong
                      >:</p
                    >
                    <p
                      >$$MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i -
                      \hat{y}_i)^2$$</p
                    >
                    <p
                      >The squaring operation creates important mathematical
                      properties:
                      <strong
                        >larger errors receive disproportionate penalty</strong
                      >, and the metric maintains
                      <strong
                        >mathematical smoothness essential for the training
                        process</strong
                      >. This smoothness allows algorithms like LinearRegression
                      to efficiently find optimal solutions by following a clear
                      path to better predictions, much like how a smooth hill
                      makes it easier to find the valley at the bottom compared
                      to a jagged, rocky terrain.</p
                    >
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=78199773"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h3 id="Computing-MSE-with-Code"
                      >Computing MSE with Code<a
                        class="anchor-link"
                        href="#Computing-MSE-with-Code"
                      ></a></h3
                    ><p
                      >Let's calculate MSE manually and demonstrate how squaring
                      amplifies large errors:</p
                    >
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing code_cell rendered"
                id="cell-id=5f597b7e"
              >
                <div class="input">
                  <div class="prompt input_prompt">InÂ [2]:</div>
                  <div class="inner_cell">
                    <div class="input_area">
                      <div class="highlight hl-ipython3">
                        <pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="c1"># mean_squared_error() automatically calculates MSE by implementing the formula:</span>
<span class="c1"># MSE = (1/n) * Î£(actual - predicted)Â², returning average of squared errors</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="c1"># Load the bike-sharing dataset</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv"</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'datetime'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">'datetime'</span><span class="p">])</span>

<span class="c1"># Extract temporal features</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'hour'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'datetime'</span><span class="p">]</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">hour</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'month'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'datetime'</span><span class="p">]</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">month</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'weekday'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'datetime'</span><span class="p">]</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">weekday</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"=== MEAN SQUARED ERROR (MSE) CALCULATION ===</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>

<span class="c1"># Prepare features and target</span>
<span class="n">feature_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'temp'</span><span class="p">,</span> <span class="s1">'humidity'</span><span class="p">,</span> <span class="s1">'windspeed'</span><span class="p">,</span> <span class="s1">'hour'</span><span class="p">,</span> <span class="s1">'weekday'</span><span class="p">,</span> <span class="s1">'month'</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">feature_cols</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'count'</span><span class="p">]</span>

<span class="c1"># Chronological split</span>
<span class="n">split_idx</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.8</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="n">split_idx</span><span class="p">],</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">split_idx</span><span class="p">:]</span>
<span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="n">split_idx</span><span class="p">],</span> <span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">split_idx</span><span class="p">:]</span>

<span class="c1"># Train Random Forest model</span>
<span class="n">rf_model</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="s1">'sqrt'</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">rf_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Generate predictions</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">rf_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"--- Manual MSE Calculation ---"</span><span class="p">)</span>
<span class="c1"># Calculate squared errors manually</span>
<span class="n">errors</span> <span class="o">=</span> <span class="n">y_test</span><span class="o">.</span><span class="n">values</span> <span class="o">-</span> <span class="n">predictions</span>
<span class="n">squared_errors</span> <span class="o">=</span> <span class="n">errors</span> <span class="o">**</span> <span class="mi">2</span>  <span class="c1"># Square each error</span>
<span class="n">manual_mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">squared_errors</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"First 5 predictions:"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">actual</span> <span class="o">=</span> <span class="n">y_test</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">predictions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">errors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">sq_error</span> <span class="o">=</span> <span class="n">squared_errors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  Obs </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: Actual=</span><span class="si">{</span><span class="n">actual</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">, Predicted=</span><span class="si">{</span><span class="n">pred</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">, Error=</span><span class="si">{</span><span class="n">error</span><span class="si">:</span><span class="s2">+.0f</span><span class="si">}</span><span class="s2">, Squared=</span><span class="si">{</span><span class="n">sq_error</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Total observations: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Sum of squared errors: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">squared_errors</span><span class="p">)</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Manual MSE: </span><span class="si">{</span><span class="n">manual_mse</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">--- Scikit-learn MSE Calculation ---"</span><span class="p">)</span>
<span class="c1"># mean_squared_error() computes MSE automatically</span>
<span class="n">sklearn_mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Scikit-learn MSE: </span><span class="si">{</span><span class="n">sklearn_mse</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">--- Error Penalty Demonstration ---"</span><span class="p">)</span>
<span class="c1"># Compare small vs large errors</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"How squaring penalizes large errors:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  Small error (10 bikes): 10Â² = </span><span class="si">{</span><span class="mi">10</span><span class="o">**</span><span class="mi">2</span><span class="si">}</span><span class="s2"> contribution to MSE"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  Medium error (30 bikes): 30Â² = </span><span class="si">{</span><span class="mi">30</span><span class="o">**</span><span class="mi">2</span><span class="si">}</span><span class="s2"> contribution to MSE"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  Large error (50 bikes): 50Â² = </span><span class="si">{</span><span class="mi">50</span><span class="o">**</span><span class="mi">2</span><span class="si">}</span><span class="s2"> contribution to MSE"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">A 50-bike error contributes </span><span class="si">{</span><span class="mi">50</span><span class="o">**</span><span class="mi">2</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">10</span><span class="o">**</span><span class="mi">2</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">Ã— more to MSE than a 10-bike error,"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"but only </span><span class="si">{</span><span class="mi">50</span><span class="o">/</span><span class="mi">10</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">Ã— more to MAE"</span><span class="p">)</span>
</pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div class="output_wrapper">
                  <div class="output">
                    <div class="output_area">
                      <div class="prompt"></div>
                      <div
                        class="output_subarea output_stream output_stdout output_text"
                      >
                        <pre>
=== MEAN SQUARED ERROR (MSE) CALCULATION ===

--- Manual MSE Calculation ---
First 5 predictions:
  Obs 1: Actual=19, Predicted=19, Error=+0, Squared=0
  Obs 2: Actual=19, Predicted=40, Error=-21, Squared=435
  Obs 3: Actual=68, Predicted=150, Error=-82, Squared=6762
  Obs 4: Actual=108, Predicted=230, Error=-122, Squared=14911
  Obs 5: Actual=229, Predicted=289, Error=-60, Squared=3601

Total observations: 2,178
Sum of squared errors: 39934086
Manual MSE: 18335.21

--- Scikit-learn MSE Calculation ---
Scikit-learn MSE: 18335.21

--- Error Penalty Demonstration ---
How squaring penalizes large errors:
  Small error (10 bikes): 10Â² = 100 contribution to MSE
  Medium error (30 bikes): 30Â² = 900 contribution to MSE
  Large error (50 bikes): 50Â² = 2500 contribution to MSE

A 50-bike error contributes 25Ã— more to MSE than a 10-bike error,
but only 5Ã— more to MAE
</pre
                        >
                      </div>
                    </div>
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=ff4d183b"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <p><strong>What this demonstrates:</strong></p>
                    <ul>
                      <li
                        ><strong>Squared penalty structure</strong> - large
                        errors disproportionately increase MSE</li
                      >
                      <li
                        ><strong>Mathematical optimization focus</strong> - MSE
                        penalizes outliers heavily, driving algorithms to
                        minimize large mistakes</li
                      >
                      <li
                        ><strong>Less intuitive units</strong> - MSE is in
                        "squared bikes" rather than bikes, making interpretation
                        harder</li
                      >
                    </ul>
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=68394379"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h3 id="Business-Interpretation-and-Operational-Value"
                      >Business Interpretation and Operational Value<a
                        class="anchor-link"
                        href="#Business-Interpretation-and-Operational-Value"
                      ></a></h3
                    ><p
                      >In bike-sharing operations,
                      <strong
                        >MSE's large-error penalty aligns with business risk
                        patterns</strong
                      >. A 50-bike prediction error during Monday's 8 AM rush
                      hour has
                      <strong>significantly greater operational impact</strong>
                      than a 10-bike error during Sunday's 2 AM period. MSE's
                      mathematical structure naturally
                      <strong
                        >emphasizes accuracy during periods where large errors
                        create disproportionate business costs</strong
                      >.</p
                    >
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=6b283c14"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h2
                      id="2.3.-Root-Mean-Squared-Error-(RMSE):-An-Industry-Standard-for-Transportation"
                      >2.3. Root Mean Squared Error (RMSE): An Industry Standard
                      for Transportation<a
                        class="anchor-link"
                        href="#2.3.-Root-Mean-Squared-Error-(RMSE):-An-Industry-Standard-for-Transportation"
                      ></a></h2
                    ><p
                      ><strong
                        >Root Mean Squared Error combines MSE's mathematical
                        advantages with MAE's interpretational clarity</strong
                      >, making it an
                      <strong
                        >industry standard for transportation demand forecasting
                        evaluation</strong
                      >. Think of RMSE like converting squared penalty points
                      back to original units: if a teacher penalized errors
                      using squared points (making a 10-point mistake cost 100
                      penalty points), students couldn't easily interpret their
                      scores. Taking the square root converts those 100 penalty
                      points back to a more interpretable 10-point scale, while
                      still maintaining the "large errors hurt more" property of
                      the squared system. RMSE does exactly thisâ€”it
                      <strong
                        >converts MSE's "squared bikes" back to "bikes per hour"
                        while preserving the mathematical benefits</strong
                      >. This section examines RMSE's mathematical relationship
                      to MSE, demonstrates calculation through code, and
                      establishes its role as the primary metric for
                      bike-sharing model comparison.</p
                    >
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=9cb19694"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h3 id="Mathematical-Foundation"
                      >Mathematical Foundation<a
                        class="anchor-link"
                        href="#Mathematical-Foundation"
                      ></a></h3
                    ><p
                      >RMSE
                      <strong
                        >applies square root transformation to MSE, restoring
                        the original measurement scale</strong
                      >:</p
                    >
                    <p
                      >$$\text{RMSE} = \left( \frac{1}{n} \sum_{i=1}^{n} (y_i -
                      \hat{y}_i)^2 \right)^{1/2}$$</p
                    >
                    <p
                      >This transformation provides
                      <strong>scale interpretability</strong> (bikes per hour)
                      while
                      <strong>maintaining MSE's mathematical properties</strong
                      >, particularly the emphasis on large error penalty that
                      reflects business risk patterns in transportation
                      operations.</p
                    >
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=ba4e3a53"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h3 id="Computing-RMSE-with-Code"
                      >Computing RMSE with Code<a
                        class="anchor-link"
                        href="#Computing-RMSE-with-Code"
                      ></a></h3
                    ><p
                      >Let's calculate RMSE and compare it to MAE and MSE to
                      understand their relationships:</p
                    >
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing code_cell rendered"
                id="cell-id=27ee5039"
              >
                <div class="input">
                  <div class="prompt input_prompt">InÂ [3]:</div>
                  <div class="inner_cell">
                    <div class="input_area">
                      <div class="highlight hl-ipython3">
                        <pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">mean_squared_error</span><span class="p">,</span> <span class="n">mean_absolute_error</span>

<span class="c1"># Load the bike-sharing dataset</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv"</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'datetime'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">'datetime'</span><span class="p">])</span>

<span class="c1"># Extract temporal features</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'hour'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'datetime'</span><span class="p">]</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">hour</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'month'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'datetime'</span><span class="p">]</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">month</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'weekday'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'datetime'</span><span class="p">]</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">weekday</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"=== ROOT MEAN SQUARED ERROR (RMSE) CALCULATION ===</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>

<span class="c1"># Prepare features and target</span>
<span class="n">feature_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'temp'</span><span class="p">,</span> <span class="s1">'humidity'</span><span class="p">,</span> <span class="s1">'windspeed'</span><span class="p">,</span> <span class="s1">'hour'</span><span class="p">,</span> <span class="s1">'weekday'</span><span class="p">,</span> <span class="s1">'month'</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">feature_cols</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'count'</span><span class="p">]</span>

<span class="c1"># Chronological split</span>
<span class="n">split_idx</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.8</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="n">split_idx</span><span class="p">],</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">split_idx</span><span class="p">:]</span>
<span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="n">split_idx</span><span class="p">],</span> <span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">split_idx</span><span class="p">:]</span>

<span class="c1"># Train Random Forest model</span>
<span class="n">rf_model</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="s1">'sqrt'</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">rf_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Generate predictions</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">rf_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"--- Manual RMSE Calculation ---"</span><span class="p">)</span>
<span class="c1"># Calculate MSE first, then take square root</span>
<span class="n">mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
<span class="c1"># np.sqrt() is NumPy's square root function, reversing the squaring operation</span>
<span class="c1"># to restore original units (bikes/hour instead of bikesÂ²/hourÂ²)</span>
<span class="n">manual_rmse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mse</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Step 1: Calculate MSE = </span><span class="si">{</span><span class="n">mse</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Step 2: Take square root: RMSE = âˆš</span><span class="si">{</span><span class="n">mse</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> = </span><span class="si">{</span><span class="n">manual_rmse</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> bikes/hour"</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">--- Scikit-learn RMSE Calculation ---"</span><span class="p">)</span>
<span class="c1"># Can use squared=False parameter for direct RMSE</span>
<span class="n">sklearn_rmse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Scikit-learn RMSE: </span><span class="si">{</span><span class="n">sklearn_rmse</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> bikes/hour"</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">--- Comparing All Three Metrics ---"</span><span class="p">)</span>
<span class="n">mae</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"MAE:  </span><span class="si">{</span><span class="n">mae</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> bikes/hour  (average absolute error)"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"RMSE: </span><span class="si">{</span><span class="n">sklearn_rmse</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> bikes/hour  (root mean squared error)"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"MSE:  </span><span class="si">{</span><span class="n">mse</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">          (mean squared error)"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">RMSE vs MAE ratio: </span><span class="si">{</span><span class="n">sklearn_rmse</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">mae</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">Ã—"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"When RMSE &gt; MAE, large errors are present (RMSE penalizes them more)"</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">--- Industry Benchmark Comparison ---"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Transportation industry RMSE benchmarks:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  â€¢ Basic linear models:       80-120 bikes/hour"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  â€¢ Tree-based models:         40-70 bikes/hour"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  â€¢ Optimized ensemble models: 35-50 bikes/hour"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Your model: </span><span class="si">{</span><span class="n">sklearn_rmse</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2"> bikes/hour"</span><span class="p">)</span>

<span class="k">if</span> <span class="n">sklearn_rmse</span> <span class="o">&lt;</span> <span class="mi">50</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"âœ“ Excellent performance - competitive with industry leaders"</span><span class="p">)</span>
<span class="k">elif</span> <span class="n">sklearn_rmse</span> <span class="o">&lt;</span> <span class="mi">70</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"âœ“ Good performance - meets industry standards"</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"âš  Performance below industry standards - needs improvement"</span><span class="p">)</span>
</pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div class="output_wrapper">
                  <div class="output">
                    <div class="output_area">
                      <div class="prompt"></div>
                      <div
                        class="output_subarea output_stream output_stdout output_text"
                      >
                        <pre>
=== ROOT MEAN SQUARED ERROR (RMSE) CALCULATION ===

--- Manual RMSE Calculation ---
Step 1: Calculate MSE = 18335.21
Step 2: Take square root: RMSE = âˆš18335.21 = 135.41 bikes/hour

--- Scikit-learn RMSE Calculation ---
Scikit-learn RMSE: 135.41 bikes/hour

--- Comparing All Three Metrics ---
MAE:  90.39 bikes/hour  (average absolute error)
RMSE: 135.41 bikes/hour  (root mean squared error)
MSE:  18335.21          (mean squared error)

RMSE vs MAE ratio: 1.50Ã—
When RMSE &gt; MAE, large errors are present (RMSE penalizes them more)

--- Industry Benchmark Comparison ---
Transportation industry RMSE benchmarks:
  â€¢ Basic linear models:       80-120 bikes/hour
  â€¢ Tree-based models:         40-70 bikes/hour
  â€¢ Optimized ensemble models: 35-50 bikes/hour

Your model: 135 bikes/hour
âš  Performance below industry standards - needs improvement
</pre
                        >
                      </div>
                    </div>
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=07683a81"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <p><strong>What this demonstrates:</strong></p>
                    <ul>
                      <li
                        ><strong>RMSE restores interpretable units</strong> -
                        back to "bikes per hour" from MSE's "squared bikes"</li
                      >
                      <li
                        ><strong>Penalizes large errors</strong> - RMSE &gt; MAE
                        indicates presence of large prediction errors</li
                      >
                      <li
                        ><strong>Benchmark context</strong> - enables objective
                        comparison against published industry results</li
                      >
                    </ul>
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=ac585ca5"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h3 id="Business-Interpretation-and-Operational-Value"
                      >Business Interpretation and Operational Value<a
                        class="anchor-link"
                        href="#Business-Interpretation-and-Operational-Value"
                      ></a></h3
                    ><p
                      >RMSE serves as one of the
                      <strong
                        >primary communication metric for bike-sharing model
                        performance</strong
                      >
                      because it
                      <strong
                        >combines mathematical rigor with interpretable
                        units</strong
                      >. When presenting model improvements to Capital City
                      Bikes' operations team, reporting "RMSE reduced from 150
                      to 135 bikes per hour" communicates
                      <strong
                        >both the magnitude (15 bikes) and the penalty for large
                        errors</strong
                      >â€”essential for understanding reliability during
                      high-stakes rush hour deployments where prediction
                      failures trigger costly emergency interventions.</p
                    >
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=39da1176"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h2
                      id="2.4.-R-Squared-(R%C2%B2):-Explained-Variance-for-Executive-Communication"
                      >2.4. R-Squared (RÂ²): Explained Variance for Executive
                      Communication<a
                        class="anchor-link"
                        href="#2.4.-R-Squared-(R%C2%B2):-Explained-Variance-for-Executive-Communication"
                      ></a></h2
                    ><p
                      ><strong
                        >The coefficient of determination (RÂ²) quantifies the
                        proportion of demand variation</strong
                      >
                      explained by your model, serving as one of the
                      <strong
                        >most common metrics for comparing model
                        performance</strong
                      >
                      across different algorithms and datasets. Think of RÂ² like
                      measuring puzzle completion: imagine a 1,000-piece jigsaw
                      puzzle representing all the patterns in bike demand. An RÂ²
                      of 0.61 means you've successfully placed 610 pieces (61%
                      of the puzzle) and can see clear patterns emerging, while
                      390 pieces remain missing. An RÂ² of 1.0 would mean you've
                      completed the entire puzzleâ€”every piece of the demand
                      pattern is explained. This
                      <strong
                        >percentage-based interpretation makes RÂ² perfect for
                        communicating with executives</strong
                      >: "Our model captures 61% of demand patterns" is
                      immediately meaningful without statistical background.
                      This section develops RÂ²'s mathematical foundation,
                      demonstrates calculation through code, and establishes its
                      value for business communication.</p
                    >
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=120a4157"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h3 id="Mathematical-Foundation"
                      >Mathematical Foundation<a
                        class="anchor-link"
                        href="#Mathematical-Foundation"
                      ></a></h3
                    ><p
                      >RÂ²
                      <strong
                        >measures explained variance by comparing model
                        predictions to baseline performance</strong
                      >
                      using mean prediction:</p
                    >
                    <p>$$RÂ² = 1 - \frac{SS_{res}}{SS_{tot}}$$</p>
                    <p>Where:</p>
                    <ul>
                      <li
                        ><strong>$SS_{res}$</strong> = $\sum_{i=1}^{n} (y_i -
                        \hat{y}_i)^2$ (sum of squared residuals -
                        <strong>your model's squared prediction errors</strong
                        >)</li
                      >
                      <li
                        ><strong>$SS_{tot}$</strong> = $\sum_{i=1}^{n} (y_i -
                        \bar{y})^2$ (total sum of squares -
                        <strong
                          >squared errors from always predicting the
                          average</strong
                        >)</li
                      >
                      <li
                        ><strong>$\bar{y}$</strong> = $\frac{1}{n}
                        \sum_{i=1}^{n} y_i$ (mean/average of actual bike
                        counts)</li
                      >
                    </ul>
                    <p
                      >In simpler terms:
                      <strong
                        >RÂ² compares how wrong your model is (SS_res) to how
                        wrong a naive "always predict the average" approach
                        would be (SS_tot)</strong
                      >. If your model is much better than the naive approach,
                      RÂ² approaches 1.0. If your model is no better than
                      guessing the average, RÂ² is near 0.</p
                    >
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=d3ed0bb2"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h3 id="Computing-R%C2%B2-with-Code"
                      >Computing RÂ² with Code<a
                        class="anchor-link"
                        href="#Computing-R%C2%B2-with-Code"
                      ></a></h3
                    ><p
                      >Let's calculate RÂ² manually to understand what "explained
                      variance" means:</p
                    >
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing code_cell rendered"
                id="cell-id=b9c5248a"
              >
                <div class="input">
                  <div class="prompt input_prompt">InÂ [4]:</div>
                  <div class="inner_cell">
                    <div class="input_area">
                      <div class="highlight hl-ipython3">
                        <pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="c1"># r2_score() computes the coefficient of determination (RÂ²), measuring the proportion</span>
<span class="c1"># of variance in the target variable that is explained by the model's predictions</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">r2_score</span>

<span class="c1"># Load the bike-sharing dataset</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv"</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'datetime'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">'datetime'</span><span class="p">])</span>

<span class="c1"># Extract temporal features</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'hour'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'datetime'</span><span class="p">]</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">hour</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'month'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'datetime'</span><span class="p">]</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">month</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'weekday'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'datetime'</span><span class="p">]</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">weekday</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"=== R-SQUARED (RÂ²) CALCULATION ===</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>

<span class="c1"># Prepare features and target</span>
<span class="n">feature_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'temp'</span><span class="p">,</span> <span class="s1">'humidity'</span><span class="p">,</span> <span class="s1">'windspeed'</span><span class="p">,</span> <span class="s1">'hour'</span><span class="p">,</span> <span class="s1">'weekday'</span><span class="p">,</span> <span class="s1">'month'</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">feature_cols</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'count'</span><span class="p">]</span>

<span class="c1"># Chronological split</span>
<span class="n">split_idx</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.8</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="n">split_idx</span><span class="p">],</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">split_idx</span><span class="p">:]</span>
<span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="n">split_idx</span><span class="p">],</span> <span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">split_idx</span><span class="p">:]</span>

<span class="c1"># Train Random Forest model</span>
<span class="n">rf_model</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="s1">'sqrt'</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">rf_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">rf_predictions</span> <span class="o">=</span> <span class="n">rf_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"--- Manual RÂ² Calculation ---"</span><span class="p">)</span>
<span class="c1"># Calculate total sum of squares (baseline: predict mean)</span>
<span class="n">y_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span>
<span class="n">ss_tot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y_test</span><span class="o">.</span><span class="n">values</span> <span class="o">-</span> <span class="n">y_mean</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># Calculate residual sum of squares (your model's errors)</span>
<span class="n">ss_res</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y_test</span><span class="o">.</span><span class="n">values</span> <span class="o">-</span> <span class="n">rf_predictions</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># RÂ² = 1 - (residual errors / total variance)</span>
<span class="n">manual_r2</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">ss_res</span> <span class="o">/</span> <span class="n">ss_tot</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Mean of test data: </span><span class="si">{</span><span class="n">y_mean</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> bikes/hour"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Total sum of squares (SS_tot): </span><span class="si">{</span><span class="n">ss_tot</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Residual sum of squares (SS_res): </span><span class="si">{</span><span class="n">ss_res</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"RÂ² = 1 - (</span><span class="si">{</span><span class="n">ss_res</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2"> / </span><span class="si">{</span><span class="n">ss_tot</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">) = </span><span class="si">{</span><span class="n">manual_r2</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">--- Scikit-learn RÂ² Calculation ---"</span><span class="p">)</span>
<span class="n">sklearn_r2</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">rf_predictions</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Scikit-learn RÂ²: </span><span class="si">{</span><span class="n">sklearn_r2</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">--- Interpretation for Capital City Bikes ---"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"RÂ² = </span><span class="si">{</span><span class="n">sklearn_r2</span><span class="si">:</span><span class="s2">.2%</span><span class="si">}</span><span class="s2"> means:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  â€¢ Your model explains </span><span class="si">{</span><span class="n">sklearn_r2</span><span class="si">:</span><span class="s2">.1%</span><span class="si">}</span><span class="s2"> of demand variation"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  â€¢ </span><span class="si">{</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">sklearn_r2</span><span class="p">)</span><span class="si">:</span><span class="s2">.1%</span><span class="si">}</span><span class="s2"> of variation remains unexplained"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  â€¢ </span><span class="si">{</span><span class="n">sklearn_r2</span><span class="si">:</span><span class="s2">.1%</span><span class="si">}</span><span class="s2"> improvement over always predicting the mean (</span><span class="si">{</span><span class="n">y_mean</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2"> bikes)"</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">--- Comparing Linear Regression vs Random Forest ---"</span><span class="p">)</span>
<span class="c1"># Train a simple linear regression for comparison</span>
<span class="n">lr_model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">lr_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">lr_predictions</span> <span class="o">=</span> <span class="n">lr_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">lr_r2</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">lr_predictions</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Linear Regression RÂ²:  </span><span class="si">{</span><span class="n">lr_r2</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">lr_r2</span><span class="si">:</span><span class="s2">.1%</span><span class="si">}</span><span class="s2"> variance explained)"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Random Forest RÂ²:      </span><span class="si">{</span><span class="n">sklearn_r2</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">sklearn_r2</span><span class="si">:</span><span class="s2">.1%</span><span class="si">}</span><span class="s2"> variance explained)"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Random Forest improvement: </span><span class="si">{</span><span class="p">(</span><span class="n">sklearn_r2</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">lr_r2</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="p">(</span><span class="n">sklearn_r2</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">lr_r2</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> percentage points)"</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">--- RÂ² Range Interpretation ---"</span><span class="p">)</span>
<span class="k">if</span> <span class="n">sklearn_r2</span> <span class="o">&gt;=</span> <span class="mf">0.8</span><span class="p">:</span>
    <span class="n">assessment</span> <span class="o">=</span> <span class="s2">"Excellent - Strong predictive performance"</span>
<span class="k">elif</span> <span class="n">sklearn_r2</span> <span class="o">&gt;=</span> <span class="mf">0.6</span><span class="p">:</span>
    <span class="n">assessment</span> <span class="o">=</span> <span class="s2">"Good - Solid predictive capability"</span>
<span class="k">elif</span> <span class="n">sklearn_r2</span> <span class="o">&gt;=</span> <span class="mf">0.4</span><span class="p">:</span>
    <span class="n">assessment</span> <span class="o">=</span> <span class="s2">"Moderate - Useful but room for improvement"</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">assessment</span> <span class="o">=</span> <span class="s2">"Weak - Significant improvement needed"</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Your model (</span><span class="si">{</span><span class="n">sklearn_r2</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">): </span><span class="si">{</span><span class="n">assessment</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div class="output_wrapper">
                  <div class="output">
                    <div class="output_area">
                      <div class="prompt"></div>
                      <div
                        class="output_subarea output_stream output_stdout output_text"
                      >
                        <pre>
=== R-SQUARED (RÂ²) CALCULATION ===

--- Manual RÂ² Calculation ---
Mean of test data: 260.27 bikes/hour
Total sum of squares (SS_tot): 103124821
Residual sum of squares (SS_res): 39934086
RÂ² = 1 - (39934086 / 103124821) = 0.6128

--- Scikit-learn RÂ² Calculation ---
Scikit-learn RÂ²: 0.6128

--- Interpretation for Capital City Bikes ---
RÂ² = 61.28% means:
  â€¢ Your model explains 61.3% of demand variation
  â€¢ 38.7% of variation remains unexplained
  â€¢ 61.3% improvement over always predicting the mean (260 bikes)

--- Comparing Linear Regression vs Random Forest ---
Linear Regression RÂ²:  0.1457 (14.6% variance explained)
Random Forest RÂ²:      0.6128 (61.3% variance explained)
Random Forest improvement: 0.4670 (46.7 percentage points)

--- RÂ² Range Interpretation ---
Your model (0.61): Good - Solid predictive capability
</pre
                        >
                      </div>
                    </div>
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=ff6779d7"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <p><strong>What this demonstrates:</strong></p>
                    <ul>
                      <li
                        ><strong>Explained variance</strong> - RÂ² of 0.61 means
                        the model explains 61% of demand variation</li
                      >
                      <li
                        ><strong>Baseline comparison</strong> - shows
                        improvement over naive "always predict mean"
                        approach</li
                      >
                      <li
                        ><strong>Algorithm comparison</strong> - Random Forest
                        (61%) significantly outperforms Linear Regression
                        (15%)</li
                      >
                      <li
                        ><strong>Executive communication</strong> - "Our model
                        captures 61% of demand patterns" is clear stakeholder
                        language</li
                      >
                    </ul>
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=a6c8b6ea"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h3 id="Business-Interpretation-and-Operational-Value"
                      >Business Interpretation and Operational Value<a
                        class="anchor-link"
                        href="#Business-Interpretation-and-Operational-Value"
                      ></a></h3
                    ><p
                      >RÂ² provides
                      <strong
                        >essential context for strategic planning
                        confidence</strong
                      >. An RÂ² of 0.61 enables executives to
                      <strong
                        >justify infrastructure investments, competitive
                        positioning, and operational planning</strong
                      >
                      based on quantified model reliability. This
                      <strong
                        >explained variance percentage directly translates to
                        confidence levels</strong
                      >
                      for business decisions dependent on demand forecasting
                      accuracy.</p
                    >
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=68013595"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h2 id="3.-Systematic-Algorithm-Comparison"
                      >3. Systematic Algorithm Comparison<a
                        class="anchor-link"
                        href="#3.-Systematic-Algorithm-Comparison"
                      ></a></h2
                    ><p
                      >This section establishes
                      <strong
                        >frameworks for comparing multiple machine learning
                        algorithms</strong
                      >
                      to identify optimal models for bike-sharing demand
                      forecasting. We'll build systematic comparison
                      methodologies, demonstrate performance benchmarking with
                      code examples, and develop decision criteria that balance
                      accuracy with operational constraints. Understanding
                      systematic comparison enables
                      <strong
                        >confident model selection based on empirical evidence
                        rather than intuition</strong
                      >.</p
                    >
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=458b4c41"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h3 id="3.1.-Comparison-Framework-Design"
                      >3.1. Comparison Framework Design<a
                        class="anchor-link"
                        href="#3.1.-Comparison-Framework-Design"
                      ></a></h3
                    ><p
                      ><strong
                        >Systematic algorithm comparison requires consistent
                        evaluation methodologies across all models.</strong
                      >
                      Think of this like comparing car performance: you wouldn't
                      test one car on a racetrack and another in city traffic,
                      or give one premium fuel and the other regular gas. Fair
                      comparison demands <strong>identical conditions</strong>:
                      same test track (dataset), same fuel grade (features),
                      same evaluation metrics (0-60 mph time, fuel efficiency).
                      Similarly, comparing machine learning algorithms requires
                      <strong
                        >identical training and testing datasets, chronological
                        train-test splits, and consistent feature sets</strong
                      >. This ensures performance differences reflect
                      <strong>algorithmic capabilities</strong>â€”the car's actual
                      engineeringâ€”rather than data advantages or evaluation
                      inconsistencies, while metrics capture operationally
                      relevant aspects of predictive performance.</p
                    >
                    <p
                      ><strong
                        >Evaluation metrics must capture multiple performance
                        dimensions simultaneously.</strong
                      >
                      While RMSE provides overall accuracy assessment, MAE
                      offers interpretability advantages, and RÂ² contextualizes
                      performance relative to baseline approaches. Comparing
                      algorithms across all three metrics reveals nuanced
                      performance trade-offs.</p
                    >
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=4c313930"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h3 id="3.2.-Multi-Algorithm-Performance-Benchmarking"
                      >3.2. Multi-Algorithm Performance Benchmarking<a
                        class="anchor-link"
                        href="#3.2.-Multi-Algorithm-Performance-Benchmarking"
                      ></a></h3
                    ><p
                      >Let's implement systematic comparison between Linear
                      Regression and Random Forest to demonstrate evidence-based
                      algorithm selection:</p
                    >
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing code_cell rendered"
                id="cell-id=167ce7a7"
              >
                <div class="input">
                  <div class="prompt input_prompt">InÂ [5]:</div>
                  <div class="inner_cell">
                    <div class="input_area">
                      <div class="highlight hl-ipython3">
                        <pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">mean_absolute_error</span><span class="p">,</span> <span class="n">mean_squared_error</span><span class="p">,</span> <span class="n">r2_score</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># Load the bike-sharing dataset</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv"</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'datetime'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">'datetime'</span><span class="p">])</span>

<span class="c1"># Extract temporal features</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'hour'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'datetime'</span><span class="p">]</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">hour</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'month'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'datetime'</span><span class="p">]</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">month</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'weekday'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'datetime'</span><span class="p">]</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">weekday</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"=== SYSTEMATIC ALGORITHM COMPARISON ===</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>

<span class="c1"># Prepare features and target</span>
<span class="n">feature_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'temp'</span><span class="p">,</span> <span class="s1">'humidity'</span><span class="p">,</span> <span class="s1">'windspeed'</span><span class="p">,</span> <span class="s1">'hour'</span><span class="p">,</span> <span class="s1">'weekday'</span><span class="p">,</span> <span class="s1">'month'</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">feature_cols</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'count'</span><span class="p">]</span>

<span class="c1"># Chronological split (same for all algorithms)</span>
<span class="n">split_idx</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.8</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="n">split_idx</span><span class="p">],</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">split_idx</span><span class="p">:]</span>
<span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="n">split_idx</span><span class="p">],</span> <span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">split_idx</span><span class="p">:]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Training set: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2"> observations"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Testing set:  </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2"> observations</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>

<span class="c1"># Train Linear Regression</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"--- Training Linear Regression ---"</span><span class="p">)</span>
<span class="n">lr_model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">lr_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">lr_predictions</span> <span class="o">=</span> <span class="n">lr_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Calculate Linear Regression metrics</span>
<span class="n">lr_mae</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">lr_predictions</span><span class="p">)</span>
<span class="n">lr_rmse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">lr_predictions</span><span class="p">))</span>
<span class="n">lr_r2</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">lr_predictions</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Linear Regression trained"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  MAE:  </span><span class="si">{</span><span class="n">lr_mae</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> bikes/hour"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  RMSE: </span><span class="si">{</span><span class="n">lr_rmse</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> bikes/hour"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  RÂ²:   </span><span class="si">{</span><span class="n">lr_r2</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">lr_r2</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">% variance explained)"</span><span class="p">)</span>

<span class="c1"># Train Random Forest</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">--- Training Random Forest ---"</span><span class="p">)</span>
<span class="n">rf_model</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="s1">'sqrt'</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">rf_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">rf_predictions</span> <span class="o">=</span> <span class="n">rf_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Calculate Random Forest metrics</span>
<span class="n">rf_mae</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">rf_predictions</span><span class="p">)</span>
<span class="n">rf_rmse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">rf_predictions</span><span class="p">))</span>
<span class="n">rf_r2</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">rf_predictions</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Random Forest trained"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  MAE:  </span><span class="si">{</span><span class="n">rf_mae</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> bikes/hour"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  RMSE: </span><span class="si">{</span><span class="n">rf_rmse</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> bikes/hour"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  RÂ²:   </span><span class="si">{</span><span class="n">rf_r2</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">rf_r2</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">% variance explained)"</span><span class="p">)</span>

<span class="c1"># Comparative Analysis</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">--- COMPARATIVE ANALYSIS ---"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Metric          Linear Reg    Random Forest    RF Advantage"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="s1">'='</span><span class="o">*</span><span class="mi">60</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"MAE (lower=better)   </span><span class="si">{</span><span class="n">lr_mae</span><span class="si">:</span><span class="s2">&gt;6.1f</span><span class="si">}</span><span class="s2">        </span><span class="si">{</span><span class="n">rf_mae</span><span class="si">:</span><span class="s2">&gt;6.1f</span><span class="si">}</span><span class="s2">          </span><span class="si">{</span><span class="n">lr_mae</span><span class="o">-</span><span class="n">rf_mae</span><span class="si">:</span><span class="s2">&gt;6.1f</span><span class="si">}</span><span class="s2"> bikes (</span><span class="si">{</span><span class="p">(</span><span class="n">lr_mae</span><span class="o">-</span><span class="n">rf_mae</span><span class="p">)</span><span class="o">/</span><span class="n">lr_mae</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%)"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"RMSE (lower=better)  </span><span class="si">{</span><span class="n">lr_rmse</span><span class="si">:</span><span class="s2">&gt;6.1f</span><span class="si">}</span><span class="s2">        </span><span class="si">{</span><span class="n">rf_rmse</span><span class="si">:</span><span class="s2">&gt;6.1f</span><span class="si">}</span><span class="s2">          </span><span class="si">{</span><span class="n">lr_rmse</span><span class="o">-</span><span class="n">rf_rmse</span><span class="si">:</span><span class="s2">&gt;6.1f</span><span class="si">}</span><span class="s2"> bikes (</span><span class="si">{</span><span class="p">(</span><span class="n">lr_rmse</span><span class="o">-</span><span class="n">rf_rmse</span><span class="p">)</span><span class="o">/</span><span class="n">lr_rmse</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%)"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"RÂ² (higher=better)   </span><span class="si">{</span><span class="n">lr_r2</span><span class="si">:</span><span class="s2">&gt;6.4f</span><span class="si">}</span><span class="s2">      </span><span class="si">{</span><span class="n">rf_r2</span><span class="si">:</span><span class="s2">&gt;6.4f</span><span class="si">}</span><span class="s2">        +</span><span class="si">{</span><span class="n">rf_r2</span><span class="o">-</span><span class="n">lr_r2</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="p">(</span><span class="n">rf_r2</span><span class="o">-</span><span class="n">lr_r2</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> pp)"</span><span class="p">)</span>

<span class="c1"># Visualization</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Plot 1: MAE Comparison</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">([</span><span class="s1">'Linear</span><span class="se">\n</span><span class="s1">Regression'</span><span class="p">,</span> <span class="s1">'Random</span><span class="se">\n</span><span class="s1">Forest'</span><span class="p">],</span> <span class="p">[</span><span class="n">lr_mae</span><span class="p">,</span> <span class="n">rf_mae</span><span class="p">],</span>
            <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s1">'#3498DB'</span><span class="p">,</span> <span class="s1">'#2ECC71'</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'MAE (bikes/hour)'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'Mean Absolute Error</span><span class="se">\n</span><span class="s1">(Lower is Better)'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">'bold'</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">'y'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Plot 2: RMSE Comparison</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">([</span><span class="s1">'Linear</span><span class="se">\n</span><span class="s1">Regression'</span><span class="p">,</span> <span class="s1">'Random</span><span class="se">\n</span><span class="s1">Forest'</span><span class="p">],</span> <span class="p">[</span><span class="n">lr_rmse</span><span class="p">,</span> <span class="n">rf_rmse</span><span class="p">],</span>
            <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s1">'#3498DB'</span><span class="p">,</span> <span class="s1">'#2ECC71'</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'RMSE (bikes/hour)'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'Root Mean Squared Error</span><span class="se">\n</span><span class="s1">(Lower is Better)'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">'bold'</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">'y'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Plot 3: RÂ² Comparison</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">([</span><span class="s1">'Linear</span><span class="se">\n</span><span class="s1">Regression'</span><span class="p">,</span> <span class="s1">'Random</span><span class="se">\n</span><span class="s1">Forest'</span><span class="p">],</span> <span class="p">[</span><span class="n">lr_r2</span><span class="p">,</span> <span class="n">rf_r2</span><span class="p">],</span>
            <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s1">'#3498DB'</span><span class="p">,</span> <span class="s1">'#2ECC71'</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'RÂ² Score'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'R-Squared</span><span class="se">\n</span><span class="s1">(Higher is Better)'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">'bold'</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">'y'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">--- RECOMMENDATION ---"</span><span class="p">)</span>
<span class="k">if</span> <span class="n">rf_r2</span> <span class="o">&gt;</span> <span class="n">lr_r2</span> <span class="o">*</span> <span class="mf">1.2</span><span class="p">:</span>  <span class="c1"># 20% improvement</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"âœ“ Random Forest provides substantial performance advantage"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"  Recommendation: Deploy Random Forest for production"</span><span class="p">)</span>
<span class="k">elif</span> <span class="n">rf_r2</span> <span class="o">&gt;</span> <span class="n">lr_r2</span> <span class="o">*</span> <span class="mf">1.05</span><span class="p">:</span>  <span class="c1"># 5% improvement</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"âœ“ Random Forest shows moderate improvement"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"  Recommendation: Consider RF if interpretability not critical"</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"âš  Marginal improvement may not justify RF complexity"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"  Recommendation: Consider Linear Regression for interpretability"</span><span class="p">)</span>
</pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div class="output_wrapper">
                  <div class="output">
                    <div class="output_area">
                      <div class="prompt"></div>
                      <div
                        class="output_subarea output_stream output_stdout output_text"
                      >
                        <pre>
=== SYSTEMATIC ALGORITHM COMPARISON ===

Training set: 8,708 observations
Testing set:  2,178 observations

--- Training Linear Regression ---
Linear Regression trained
  MAE:  139.35 bikes/hour
  RMSE: 201.12 bikes/hour
  RÂ²:   0.1457 (14.6% variance explained)

--- Training Random Forest ---
Random Forest trained
  MAE:  90.39 bikes/hour
  RMSE: 135.41 bikes/hour
  RÂ²:   0.6128 (61.3% variance explained)

--- COMPARATIVE ANALYSIS ---

Metric          Linear Reg    Random Forest    RF Advantage
============================================================
MAE (lower=better)    139.4          90.4            49.0 bikes (35.1%)
RMSE (lower=better)   201.1         135.4            65.7 bikes (32.7%)
RÂ² (higher=better)   0.1457      0.6128        +0.4670 (46.7 pp)
</pre
                        >
                      </div>
                    </div>
                    <div class="output_area">
                      <div class="prompt"></div>
                      <div class="output_png output_subarea">
                        <img
                          alt="No description has been provided for this image"
                          src="data:image/png;base64,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"
                        />
                      </div>
                    </div>
                    <div class="output_area">
                      <div class="prompt"></div>
                      <div
                        class="output_subarea output_stream output_stdout output_text"
                      >
                        <pre>
--- RECOMMENDATION ---
âœ“ Random Forest provides substantial performance advantage
  Recommendation: Deploy Random Forest for production
</pre
                        >
                      </div>
                    </div>
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=485c371a"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <p><strong>What this demonstrates:</strong></p>
                    <ul>
                      <li
                        ><strong>Systematic comparison</strong> - identical
                        data, consistent evaluation across algorithms</li
                      >
                      <li
                        ><strong>Multi-metric assessment</strong> - MAE, RMSE,
                        and RÂ² reveal different performance aspects</li
                      >
                      <li
                        ><strong>Quantified advantages</strong> - Random Forest
                        reduces MAE by 49 bikes (35% improvement)</li
                      >
                      <li
                        ><strong>Evidence-based selection</strong> - clear
                        recommendation based on the results</li
                      >
                    </ul>
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=857a9350"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h3 id="3.3.-Selection-Criteria-for-Business-Contexts"
                      >3.3. Selection Criteria for Business Contexts<a
                        class="anchor-link"
                        href="#3.3.-Selection-Criteria-for-Business-Contexts"
                      ></a></h3
                    ><p
                      ><strong
                        >Algorithm selection must balance multiple criteria
                        beyond raw predictive accuracy.</strong
                      >
                      For Capital City Bikes' municipal contract requiring
                      proven performance with operational constraints, decision
                      criteria include:</p
                    >
                    <ul>
                      <li
                        ><strong>Performance Requirements</strong>: Random
                        Forest's 61% RÂ² versus Linear Regression's 15% RÂ²
                        represents
                        <strong>47-percentage-point improvement</strong>,
                        directly supporting contract requirements.</li
                      >
                      <li
                        ><strong>Interpretability Trade-offs</strong>: Linear
                        models provide transparent coefficients for regulatory
                        explanations. Random Forest sacrifices some
                        interpretability for superior accuracyâ€”<strong
                          >acceptable when performance requirements
                          dominate</strong
                        >.</li
                      >
                      <li
                        ><strong>Computational Constraints</strong>: Linear
                        Regression trains in seconds; Random Forest requires
                        minutes. For hourly retraining schedules,
                        <strong>both meet operational timelines</strong>.</li
                      >
                      <li
                        ><strong>Maintenance Complexity</strong>: Linear models
                        require minimal tuning. Random Forest benefits from
                        periodic hyperparameter optimization but
                        <strong>provides stability once configured</strong>.</li
                      >
                    </ul>
                    <p
                      >For high-stakes contracts where prediction accuracy
                      determines â‚¬2.3M annual revenue,
                      <strong
                        >Random Forest's superior performance justifies modest
                        interpretability and complexity trade-offs</strong
                      >.</p
                    >
                    <hr />
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=084abcfb"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h2
                      id="Summary-and-Transition-to-Programming-Implementation"
                      >Summary and Transition to Programming Implementation<a
                        class="anchor-link"
                        href="#Summary-and-Transition-to-Programming-Implementation"
                      ></a></h2
                    ><p
                      >Your mastery of
                      <strong
                        >model evaluation and performance assessment</strong
                      >
                      establishes the analytical foundation essential for
                      professional transportation consulting. Understanding
                      <strong
                        >regression metrics (MAE, MSE, RMSE, RÂ²), systematic
                        algorithm comparison, and business impact
                        translation</strong
                      >
                      provides the evaluation expertise necessary for
                      data-driven decision-making and stakeholder
                      communication.</p
                    >
                    <p
                      >Crucially, you've learned to
                      <strong
                        >evaluate models honestly and translate results
                        professionally</strong
                      >. Your Random Forest achieving 61% RÂ² represents a
                      <strong>47-percentage-point improvement</strong> over
                      Linear Regression's 15% â€” concrete evidence supporting
                      Capital City Bikes' â‚¬2.3M municipal contract requirements.
                      You can now quantify that
                      <strong
                        >35% MAE improvement translates to significant monthly
                        savings</strong
                      >
                      and justify algorithm selection with empirical evidence
                      rather than intuition.</p
                    >
                    <p
                      >Your ability to implement systematic comparisons,
                      interpret performance trade-offs, and assess business
                      impact prepares you to build increasingly sophisticated
                      evaluation workflows. More importantly, you understand
                      that
                      <strong
                        >professional ML consulting requires translating
                        technical excellence into business value</strong
                      >
                      - connecting RMSE improvements to revenue optimization and
                      RÂ² metrics to investment confidence.</p
                    >
                    <p
                      >In the programming example, you'll implement these
                      evaluation concepts through hands-on coding exercises,
                      building an
                      <strong
                        >end-to-end pipeline to develop and compare two
                        different machine learning models</strong
                      >
                      for bike-sharing demand forecasting.</p
                    >
                  </div>
                </div>
              </div>
            </div>
          </div>
        </main>
      </div>
    </div>
  </body>
</html>
