{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b2ed49b",
   "metadata": {},
   "source": [
    "# Lecture 8: Linear Models for Prediction - Building Your First Predictive Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf850e8",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this lecture, you will be able to:\n",
    "- Understand linear regression fundamentals and their mathematical foundation\n",
    "- Recognize linear relationships in transportation and urban mobility data\n",
    "- Implement linear regression using scikit-learn's LinearRegression class\n",
    "- Create proper train-test splits to evaluate model performance on unseen data\n",
    "- Apply chronological splitting strategies for time series forecasting problems\n",
    "- Use cross-validation techniques to obtain robust performance estimates\n",
    "- Interpret model coefficients and communicate insights to business stakeholders\n",
    "- Evaluate model performance using RMSE, R-squared, and MAE metrics\n",
    "- Position linear regression as a foundation for advanced modeling approaches\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cf447e",
   "metadata": {},
   "source": [
    "## 1. Your Predictive Modeling Journey Begins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1f5f82",
   "metadata": {},
   "source": [
    "### The Consultant's Next Challenge: From Data to Predictions\n",
    "\n",
    "Six months into your consulting engagement with Capital City Bikes, your expertise in data preparation and feature engineering has transformed raw operational data into a sophisticated analytical foundation. The client's CEO approaches you with an urgent new challenge: \"We need to predict demand for next week to optimize our bike distribution and staffing decisions. Our investors want to see concrete forecasting capabilities before our Series A funding round.\"\n",
    "\n",
    "This is **the moment every data consultant anticipates** - moving beyond descriptive analysis to build predictive capabilities that directly drive business decisions. Just as a bridge engineer progresses from understanding materials and forces to designing structures that bear actual loads, you're transitioning from data preparation to building models that must perform reliably in real business conditions.\n",
    "\n",
    "Your client needs more than just predictions - they need predictions they can understand, trust, and act upon. Board members will question the methodology. Operations managers will base daily decisions on your forecasts. Investors will evaluate the company's analytical sophistication based on your work. This requires not just technical accuracy, but **clear communication and robust methodology** that stakeholders can comprehend and defend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4b5549",
   "metadata": {},
   "source": [
    "### Why Linear Regression: The Foundation of Predictive Intelligence\n",
    "\n",
    "Think of linear regression as learning to drive with a manual transmission before advancing to complex automated systems. While sophisticated machine learning algorithms like neural networks and ensemble methods can achieve impressive accuracy, they often function as \"black boxes\" that provide predictions without explanation. Linear regression, by contrast, offers **transparency that's essential for building stakeholder confidence** and business understanding.\n",
    "\n",
    "Linear regression serves multiple crucial purposes in your consulting toolkit. First, it **establishes baseline performance** that more complex models must exceed to justify their additional complexity. Second, it provides interpretable insights that help stakeholders understand which factors actually drive their business outcomes. Third, it offers rapid development cycles that enable quick iteration and learning, particularly valuable in fast-moving startup environments.\n",
    "\n",
    "These qualities become your competitive advantage as a consultant. While competitors might deliver accurate predictions through complex algorithms, you'll provide **predictions plus understanding**, enabling your client to make informed decisions and confidently communicate their analytical capabilities to investors and partners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c4c53d",
   "metadata": {},
   "source": [
    "## 2. Linear Regression Fundamentals\n",
    "\n",
    "This section establishes the mathematical and conceptual foundation of linear regression before exploring its implementation. We'll start by defining linear regression from first principles, then examine how linear relationships manifest in real data, understand the mathematics behind the regression line, and finally connect these concepts to practical business applications in urban mobility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0f2009",
   "metadata": {},
   "source": [
    "### 2.1. What is Linear Regression\n",
    "\n",
    "Let's begin by establishing a clear definition of linear regression and understanding why this foundational technique remains essential for modern predictive modeling. We'll explore the core concept, examine how it represents relationships mathematically, and understand why its transparency makes it particularly valuable for business applications. This conceptual foundation will prepare you to implement linear regression effectively in transportation consulting scenarios.\n",
    "\n",
    "Linear regression is **a statistical method that models the relationship between a dependent variable** (what we want to predict) **and one or more independent variables** (what we use to make predictions) by fitting a straight line through the data points. The fundamental assumption of linear regression is that the relationship between variables can be expressed as a linear equation, meaning that changes in the independent variables produce proportional changes in the dependent variable.\n",
    "\n",
    "At its core, linear regression seeks to find the \"best-fit\" line that minimizes the distance between the actual data points and the predicted values on the line. This line represents the average relationship between the input variables and the target variable, allowing us to make predictions for new, unseen data points.\n",
    "\n",
    "The power of linear regression lies in **its simplicity and interpretability**. Unlike complex algorithms that act as \"black boxes,\" linear regression provides clear, understandable relationships that can be easily communicated to stakeholders. Each coefficient in the model tells us exactly how much the target variable is expected to change when the corresponding input variable increases by one unit, holding all other variables constant.\n",
    "\n",
    "In the context of predictive modeling, linear regression serves as both a practical prediction tool and a baseline for comparison with more sophisticated algorithms. Its transparency makes it particularly valuable in business contexts where understanding the \"why\" behind predictions is as important as the predictions themselves.\n",
    "\n",
    "Let's see how linear relationships appear in real bike-sharing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5b9fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Load the bike-sharing dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\")\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "# Create a simple scatter plot showing temperature-demand relationship\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['temp'], df['count'], alpha=0.3, s=20)\n",
    "plt.xlabel('Temperature (°C)', fontsize=12)\n",
    "plt.ylabel('Hourly Bike Rentals', fontsize=12)\n",
    "plt.title('Linear Relationship: Temperature vs. Bike Demand', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate correlation to quantify relationship strength\n",
    "correlation = df['temp'].corr(df['count'])\n",
    "print(f\"Temperature-Demand Correlation: {correlation:.3f}\")\n",
    "print(f\"Temperature explains {(correlation**2)*100:.1f}% of demand variation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cf1f87",
   "metadata": {},
   "source": [
    "**What this demonstrates:**\n",
    "- **Visual evidence of linear relationships** in transportation data - warmer temperatures generally associate with higher demand\n",
    "- The scatter plot reveals a clear upward trend, confirming that linear regression is an appropriate modeling approach\n",
    "- The moderate correlation (r ≈ 0.39) indicates that while temperature matters, other factors also influence demand significantly\n",
    "- This visual validation justifies using linear regression for demand prediction in bike-sharing systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53abed8",
   "metadata": {},
   "source": [
    "### 2.2. Linear Relationships in Data\n",
    "\n",
    "Now that we've defined linear regression, let's explore what constitutes a linear relationship and how to identify these patterns in transportation data. We'll examine the characteristics of linear relationships, understand positive versus negative associations, and learn to recognize when linear modeling is appropriate. This understanding will guide your feature selection and model design decisions.\n",
    "\n",
    "A linear relationship exists when **two variables change together at a constant rate**. In mathematical terms, this means that as one variable increases or decreases, the other variable changes by a consistent amount. When plotted on a graph, these relationships appear as straight lines, hence the term \"linear.\"\n",
    "\n",
    "Linear relationships can be positive or negative. In a positive linear relationship, both variables move in the same direction - as one increases, the other increases proportionally. In a negative linear relationship, the variables move in opposite directions - as one increases, the other decreases proportionally. **The strength of a linear relationship is measured by how closely the data points cluster around the best-fit line.**\n",
    "\n",
    "Real-world data rarely exhibits perfect linear relationships, but many phenomena demonstrate approximately linear patterns that can be effectively modeled using linear regression. The key is identifying variables that have reasonably consistent relationships with each other, even if some variability exists around the general trend.\n",
    "\n",
    "In transportation and urban mobility contexts, several relationships tend to be approximately linear. Weather conditions, time patterns, and seasonal factors often exhibit linear relationships with transportation demand. For example, as temperature increases within a comfortable range, bike-sharing usage typically increases at a relatively consistent rate. Similarly, as the working day progresses during morning rush hour, subway ridership increases in a predictable linear pattern.\n",
    "\n",
    "The identification of linear relationships is crucial for successful linear regression modeling. Variables with strong linear relationships will produce more accurate and reliable models, while variables with weak or non-linear relationships may require transformation or alternative modeling approaches.\n",
    "\n",
    "Let's visualize both positive and negative linear relationships in bike-sharing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296d1f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Load the bike-sharing dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\")\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "# Create side-by-side plots showing positive and negative relationships\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Panel 1: Positive relationship (Temperature vs. Demand)\n",
    "axes[0].scatter(df['temp'], df['count'], alpha=0.3, s=15, color='#2ECC71')\n",
    "slope1, intercept1, r1, _, _ = stats.linregress(df['temp'], df['count'])\n",
    "line_x1 = np.array([df['temp'].min(), df['temp'].max()])\n",
    "axes[0].plot(line_x1, slope1 * line_x1 + intercept1, 'r--', linewidth=2.5,\n",
    "             label=f'Trend Line (r = {r1:.3f})')\n",
    "axes[0].set_xlabel('Temperature (°C)', fontsize=11)\n",
    "axes[0].set_ylabel('Hourly Bike Rentals', fontsize=11)\n",
    "axes[0].set_title('Positive Linear Relationship\\nHigher Temp → Higher Demand',\n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 2: Negative relationship (Humidity vs. Demand)\n",
    "axes[1].scatter(df['humidity'], df['count'], alpha=0.3, s=15, color='#E74C3C')\n",
    "slope2, intercept2, r2, _, _ = stats.linregress(df['humidity'], df['count'])\n",
    "line_x2 = np.array([df['humidity'].min(), df['humidity'].max()])\n",
    "axes[1].plot(line_x2, slope2 * line_x2 + intercept2, 'b--', linewidth=2.5,\n",
    "             label=f'Trend Line (r = {r2:.3f})')\n",
    "axes[1].set_xlabel('Humidity (%)', fontsize=11)\n",
    "axes[1].set_ylabel('Hourly Bike Rentals', fontsize=11)\n",
    "axes[1].set_title('Negative Linear Relationship\\nHigher Humidity → Lower Demand',\n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=== Linear Relationship Analysis ===\")\n",
    "print(f\"Temperature (positive): r = {r1:.3f}, slope = {slope1:.2f} rides/°C\")\n",
    "print(f\"Humidity (negative): r = {r2:.3f}, slope = {slope2:.2f} rides/%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6126e55a",
   "metadata": {},
   "source": [
    "**What this demonstrates:**\n",
    "- **Positive linear relationships** show upward trends - as temperature increases, demand increases consistently\n",
    "- **Negative linear relationships** show downward trends - as humidity increases, demand decreases\n",
    "- The correlation coefficients (r values) quantify relationship strength - temperature shows moderate positive correlation, humidity shows moderate negative correlation\n",
    "- The slopes tell the business story: each 1°C temperature increase adds approximately 9.2 rides per hour, while each 1% humidity increase reduces demand by approximately 2.2 rides per hour\n",
    "- These quantified relationships enable **concrete operational planning** and weather-responsive capacity adjustments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f235997",
   "metadata": {},
   "source": [
    "### 2.3. The Regression Line\n",
    "\n",
    "Having identified linear relationships, let's now understand how to mathematically represent these patterns through the regression line. We'll explore the equation structure, interpret slope and intercept parameters, and learn how regression finds the optimal line that best fits the data. This mathematical foundation will help you understand what your models are actually calculating.\n",
    "\n",
    "The regression line, also known as **the line of best fit**, is the mathematical representation of the linear relationship between variables. For simple linear regression with one independent variable, this line is expressed using the familiar equation: y = mx + b, where y is the dependent variable, x is the independent variable, m is the slope, and b is the y-intercept.\n",
    "\n",
    "The slope (m) represents the rate of change - how much y increases or decreases for every one-unit increase in x. The y-intercept (b) represents the value of y when x equals zero. Together, these parameters define the position and angle of the line that best represents the relationship in the data.\n",
    "\n",
    "For multiple linear regression, which involves more than one independent variable, the equation extends to: y = b₀ + b₁x₁ + b₂x₂ + ... + bₙxₙ, where b₀ is the intercept, and b₁, b₂, through bₙ are the coefficients for each independent variable x₁, x₂, through xₙ. Each coefficient represents the expected change in y for a one-unit increase in the corresponding x variable, assuming all other variables remain constant.\n",
    "\n",
    "The process of finding the best regression line involves **minimizing the sum of squared differences** between the actual data points and the predicted values on the line. This method, called ordinary least squares, ensures that the line represents the best possible fit given the available data. The resulting line minimizes prediction errors across all data points, providing the most accurate representation of the underlying relationship.\n",
    "\n",
    "The quality of the regression line is measured by how well it explains the variability in the data. A perfect fit would have all data points lying exactly on the line, while a poor fit would show data points scattered widely around the line with no clear pattern.\n",
    "\n",
    "Let's calculate and visualize a regression line with its equation parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cff54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Load the bike-sharing dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\")\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "# Calculate regression line parameters\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(df['temp'], df['count'])\n",
    "\n",
    "# Create visualization showing the regression line\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.scatter(df['temp'], df['count'], alpha=0.2, s=20, label='Actual Data')\n",
    "\n",
    "# Plot the regression line\n",
    "line_x = np.array([df['temp'].min(), df['temp'].max()])\n",
    "line_y = slope * line_x + intercept\n",
    "plt.plot(line_x, line_y, 'r-', linewidth=3, label='Regression Line')\n",
    "\n",
    "# Add equation annotation\n",
    "equation_text = f'y = {slope:.2f}x + {intercept:.2f}\\n'\n",
    "equation_text += f'Correlation: r = {r_value:.3f}\\n'\n",
    "equation_text += f'R² = {r_value**2:.3f}'\n",
    "plt.text(0.05, 0.95, equation_text, transform=plt.gca().transAxes,\n",
    "         fontsize=12, verticalalignment='top',\n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "plt.xlabel('Temperature (°C)', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Hourly Bike Rentals', fontsize=12, fontweight='bold')\n",
    "plt.title('The Regression Line: Mathematical Representation of Temperature-Demand Relationship',\n",
    "          fontsize=13, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Demonstrate prediction using the regression equation\n",
    "example_temps = [10, 15, 20, 25, 30]\n",
    "print(\"\\n=== Using the Regression Equation for Predictions ===\")\n",
    "print(f\"Regression Equation: Demand = {slope:.2f} × Temperature + {intercept:.2f}\\n\")\n",
    "for temp in example_temps:\n",
    "    predicted_demand = slope * temp + intercept\n",
    "    print(f\"At {temp}°C: Predicted demand = {predicted_demand:.0f} rides per hour\")\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"- Slope ({slope:.2f}): Each 1°C increase adds ~{slope:.0f} rides per hour\")\n",
    "print(f\"- Intercept ({intercept:.2f}): Baseline demand at 0°C would be ~{intercept:.0f} rides\")\n",
    "print(f\"- R² ({r_value**2:.3f}): Temperature explains {(r_value**2)*100:.1f}% of demand variation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b33a274",
   "metadata": {},
   "source": [
    "**What this demonstrates:**\n",
    "- **The regression line equation** (y = mx + b) translates into concrete business predictions\n",
    "- The slope parameter quantifies the temperature effect: approximately 9.2 additional rides per hour for each degree Celsius increase\n",
    "- The intercept provides the theoretical baseline demand (though 0°C is outside typical operating conditions)\n",
    "- **R² (coefficient of determination)** reveals that temperature alone explains only 15.5% of demand variation, indicating other factors matter significantly\n",
    "- The equation enables **scenario planning**: managers can estimate demand at different forecasted temperatures\n",
    "- Visualization shows both the general trend (line) and variability around it (scatter), helping stakeholders understand prediction uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e479c58",
   "metadata": {},
   "source": [
    "### 2.4. Business Applications\n",
    "\n",
    "With the mathematical foundation established, let's explore how linear regression translates into practical business value. We'll examine real-world applications across industries, then focus specifically on transportation and urban mobility use cases. Understanding these applications will help you identify opportunities to apply linear regression in your consulting engagements.\n",
    "\n",
    "Linear regression finds **extensive application in business contexts** due to its interpretability and practical utility. Organizations use linear regression for demand forecasting, price optimization, resource allocation, and performance analysis. The clear relationship between input variables and outcomes makes linear regression particularly valuable for strategic planning and decision-making processes.\n",
    "\n",
    "In demand forecasting, businesses use linear regression to predict future sales, customer traffic, or service utilization based on historical patterns and influencing factors. The model coefficients provide actionable insights into which factors most significantly drive demand, enabling targeted interventions and resource optimization.\n",
    "\n",
    "Price optimization represents another key application area. Linear regression can model the relationship between pricing strategies and sales volumes, helping businesses identify optimal price points that maximize revenue or market share. The interpretable nature of the results allows managers to understand exactly how price changes will impact demand.\n",
    "\n",
    "Resource allocation decisions benefit from **linear regression's ability to quantify relationships** between input investments and output results. Organizations can model the relationship between staffing levels and service quality, marketing spend and customer acquisition, or facility capacity and operational efficiency.\n",
    "\n",
    "In urban mobility and transportation contexts, linear regression supports route planning, fleet optimization, and service scheduling decisions. Transportation authorities use linear regression to understand how weather conditions, special events, and seasonal patterns affect ridership, enabling proactive service adjustments and capacity planning.\n",
    "\n",
    "Let's apply linear regression to a concrete business scenario for Capital City Bikes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d67cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Load the bike-sharing dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\")\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "# Business Scenario: Predict demand based on temperature and working day status\n",
    "# Prepare features for multiple linear regression\n",
    "X = df[['temp', 'workingday']]\n",
    "y = df['count']\n",
    "\n",
    "# Train a simple linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Extract model parameters\n",
    "temp_coefficient = model.coef_[0]\n",
    "workday_coefficient = model.coef_[1]\n",
    "intercept = model.intercept_\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X)\n",
    "r2 = r2_score(y, predictions)\n",
    "\n",
    "print(\"=== LINEAR REGRESSION BUSINESS APPLICATION ===\")\n",
    "print(\"\\nModel Equation:\")\n",
    "print(f\"Demand = {intercept:.1f} + ({temp_coefficient:.1f} × Temperature) + ({workday_coefficient:.1f} × Working Day)\")\n",
    "\n",
    "print(\"\\n--- Business Insights ---\")\n",
    "print(f\"Temperature Impact: Each 1°C increase adds {temp_coefficient:.0f} rides per hour\")\n",
    "print(f\"Working Day Effect: Working days generate {workday_coefficient:.0f} more rides than weekends/holidays\")\n",
    "print(f\"Model Accuracy: Explains {r2*100:.1f}% of demand variation (R² = {r2:.3f})\")\n",
    "\n",
    "# Scenario Planning: What-if analysis for business decisions\n",
    "print(\"\\n--- Scenario Planning for Capital City Bikes ---\")\n",
    "scenarios = [\n",
    "    {\"name\": \"Cold Weekend\", \"temp\": 10, \"workday\": 0},\n",
    "    {\"name\": \"Warm Weekend\", \"temp\": 25, \"workday\": 0},\n",
    "    {\"name\": \"Cold Weekday\", \"temp\": 10, \"workday\": 1},\n",
    "    {\"name\": \"Warm Weekday\", \"temp\": 25, \"workday\": 1}\n",
    "]\n",
    "\n",
    "for scenario in scenarios:\n",
    "    predicted_demand = intercept + (temp_coefficient * scenario['temp']) + (workday_coefficient * scenario['workday'])\n",
    "    print(f\"{scenario['name']:15s}: {predicted_demand:.0f} rides per hour expected\")\n",
    "\n",
    "# Operational recommendation\n",
    "optimal_staffing_weekday = intercept + (temp_coefficient * 20) + (workday_coefficient * 1)\n",
    "optimal_staffing_weekend = intercept + (temp_coefficient * 20) + (workday_coefficient * 0)\n",
    "print(f\"\\nOperational Recommendation (20°C conditions):\")\n",
    "print(f\"  Weekday staffing target: {optimal_staffing_weekday:.0f} bikes per station\")\n",
    "print(f\"  Weekend staffing target: {optimal_staffing_weekend:.0f} bikes per station\")\n",
    "print(f\"  Weekday requires {((optimal_staffing_weekday/optimal_staffing_weekend - 1)*100):.1f}% more capacity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc90c16",
   "metadata": {},
   "source": [
    "**What this demonstrates:**\n",
    "- **Multiple linear regression** combines several business factors (temperature, working day status) into a single predictive model\n",
    "- Model coefficients translate directly into business language: \"working days need 30 more bikes than weekends\"\n",
    "- **Scenario planning** enables proactive capacity decisions based on weather forecasts and calendar information\n",
    "- The what-if analysis shows demand ranging from 52 rides (cold weekend) to 291 rides (warm weekday) - **a 5.6-fold variation** requiring flexible operations\n",
    "- Quantified insights support **evidence-based resource allocation**: weekday operations require 17% more capacity than weekends at similar temperatures\n",
    "- This transparency enables stakeholder confidence - executives can understand exactly how the model makes recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b030ccac",
   "metadata": {},
   "source": [
    "## 3. Machine Learning Implementation\n",
    "\n",
    "This section transitions from mathematical theory to practical implementation using machine learning tools. We'll build your understanding step by step: first mastering the LinearRegression tool itself, then understanding why proper evaluation matters, learning to create train-test splits, adapting for time series data, gaining robust performance estimates through cross-validation, and finally diving deep into performance metrics. This systematic progression ensures you can confidently deploy linear regression models for real-world demand forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dd8972",
   "metadata": {},
   "source": [
    "### 3.1. From Mathematics to Code\n",
    "\n",
    "The transition from mathematical concepts to computational implementation represents **a crucial step in applying linear regression to real-world problems**. While the underlying mathematics remains the same, machine learning libraries provide optimized implementations that handle the computational complexity and offer additional functionality for model management and evaluation.\n",
    "\n",
    "**The scikit-learn library** provides a comprehensive implementation of linear regression that automates the process of finding optimal coefficients through mathematical optimization algorithms. These implementations efficiently handle large datasets, multiple variables, and various data types while maintaining numerical stability and computational efficiency.\n",
    "\n",
    "The key advantage of scikit-learn is its **seamless connection with the entire data science process**. Think of it like LEGO bricks: every piece, whether it's a tiny 1x1 block or a specialized wheel, connects the same way using the same bumps and holes. Once you learn how the pieces fit together, you can build anything - a house, a car, or a spaceship - using the same connection mechanism. Scikit-learn provides standardized interfaces that work the same way whether you're preparing data, training models, making predictions, or measuring performance. This consistency means once you learn the pattern for linear regression, you can apply the same approach to decision trees, neural networks, or any other algorithm - the `.fit()` and `.predict()` methods work identically across all of them, like LEGO pieces that always snap together the same way.\n",
    "\n",
    "Let's see the complete journey from manual calculation to automated scikit-learn implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4d2182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy import stats\n",
    "\n",
    "# Load the bike-sharing dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\")\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "print(\"=== FROM MATHEMATICS TO CODE ===\\n\")\n",
    "\n",
    "# Method 1: Manual calculation using scipy (shows the mathematics)\n",
    "print(\"--- Method 1: Mathematical Approach (scipy) ---\")\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(df['temp'], df['count'])\n",
    "print(f\"Equation: Demand = {slope:.2f} × Temperature + {intercept:.2f}\")\n",
    "print(f\"R-squared: {r_value**2:.3f}\")\n",
    "print(f\"Standard error: {std_err:.2f}\")\n",
    "\n",
    "# Manual prediction\n",
    "example_temp = 20\n",
    "manual_prediction = slope * example_temp + intercept\n",
    "print(f\"Manual prediction at {example_temp}°C: {manual_prediction:.0f} rides\\n\")\n",
    "\n",
    "# Method 2: Machine learning approach (scikit-learn)\n",
    "print(\"--- Method 2: Machine Learning Approach (scikit-learn) ---\")\n",
    "# Prepare data in scikit-learn format (2D array for features)\n",
    "X = df[['temp']]  # Must be 2D for scikit-learn\n",
    "y = df['count']\n",
    "\n",
    "# Create and train the model\n",
    "ml_model = LinearRegression()\n",
    "ml_model.fit(X, y)\n",
    "\n",
    "# Extract parameters (same as manual calculation)\n",
    "ml_slope = ml_model.coef_[0]\n",
    "ml_intercept = ml_model.intercept_\n",
    "print(f\"Equation: Demand = {ml_slope:.2f} × Temperature + {ml_intercept:.2f}\")\n",
    "\n",
    "# Calculate R-squared\n",
    "ml_predictions = ml_model.predict(X)\n",
    "from sklearn.metrics import r2_score\n",
    "ml_r2 = r2_score(y, ml_predictions)\n",
    "print(f\"R-squared: {ml_r2:.3f}\")\n",
    "\n",
    "# ML prediction\n",
    "ml_prediction = ml_model.predict([[example_temp]])[0]\n",
    "print(f\"ML prediction at {example_temp}°C: {ml_prediction:.0f} rides\\n\")\n",
    "\n",
    "# Verify equivalence\n",
    "print(\"--- Verification: Both Methods Produce Identical Results ---\")\n",
    "print(f\"Slopes match: {abs(slope - ml_slope) < 0.01}\")\n",
    "print(f\"Intercepts match: {abs(intercept - ml_intercept) < 0.01}\")\n",
    "print(f\"Predictions match: {abs(manual_prediction - ml_prediction) < 0.01}\")\n",
    "\n",
    "print(\"\\n--- Key Advantages of ML Implementation ---\")\n",
    "print(\"✓ Standardized interface works with all scikit-learn algorithms\")\n",
    "print(\"✓ Easy extension to multiple features (just add columns)\")\n",
    "print(\"✓ Built-in performance metrics and cross-validation\")\n",
    "print(\"✓ Numerical stability for large datasets\")\n",
    "print(\"✓ Integration with pipelines and preprocessing tools\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85f6ce8",
   "metadata": {},
   "source": [
    "**What this demonstrates:**\n",
    "- **Mathematical and machine learning approaches produce identical results** - the ML implementation is simply automating the mathematics we've learned\n",
    "- The scikit-learn interface provides a **consistent pattern** (fit, predict, score) that works across all algorithms, simplifying future learning\n",
    "- ML implementations handle edge cases and numerical stability automatically, making them production-ready\n",
    "- The 2D array format requirement (X must be `[[value]]` not `[value]`) reflects ML's design for multi-feature models\n",
    "- **Transitioning to ML tools** enables scaling from simple models (one feature) to complex models (many features) without changing your code structure\n",
    "- This equivalence validation builds confidence: you understand both the mathematics and the implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612e15e2",
   "metadata": {},
   "source": [
    "### 3.2. Basic Model Usage\n",
    "\n",
    "Now that we understand how scikit-learn automates the mathematics, let's master the LinearRegression class mechanics before diving into evaluation complexities. We'll focus purely on the tool itself: how to create models, train them, make predictions, and interpret the learned parameters. Think of this as learning to drive a car before worrying about navigation or traffic rules.\n",
    "\n",
    "The scikit-learn LinearRegression class provides **a standardized interface** that makes machine learning accessible and consistent. The beauty of this interface is its simplicity: every model in scikit-learn follows the same pattern, which means once you master linear regression, you already know how to use decision trees, neural networks, and hundreds of other algorithms.\n",
    "\n",
    "**The core LinearRegression workflow** consists of just three essential steps: create the model object, fit it to data, and use it to make predictions. This pattern remains identical whether you're building simple single-feature models or complex multi-feature systems, providing a solid foundation for all your machine learning work.\n",
    "\n",
    "Let's start by understanding **the key components** of the LinearRegression class. When you create a model, you're instantiating an object that can learn relationships from data. When you call `.fit()`, the model analyzes your data to find optimal coefficients. When you call `.predict()`, it applies those learned relationships to new data. These three operations form the backbone of machine learning with scikit-learn.\n",
    "\n",
    "**After fitting**, the model stores its learned parameters in special attributes. The `.coef_` attribute contains the coefficients (slopes) for each feature, showing how much each factor influences your predictions. The `.intercept_` attribute stores the baseline value when all features are zero. Together, these parameters define the linear equation the model discovered in your data.\n",
    "\n",
    "Let's explore the LinearRegression class step by step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3196f88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load the bike-sharing dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\")\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "print(\"=== BASIC LINEAR REGRESSION USAGE ===\\n\")\n",
    "\n",
    "# Step 1: Prepare data\n",
    "print(\"--- Step 1: Data Preparation ---\")\n",
    "feature_columns = ['temp', 'humidity', 'windspeed', 'workingday']\n",
    "X = df[feature_columns]  # Features must be a 2D DataFrame\n",
    "y = df['count']          # Target is a 1D Series\n",
    "\n",
    "print(f\"Dataset size: {len(df):,} hourly observations\")\n",
    "print(f\"Feature matrix: {X.shape[0]} rows × {X.shape[1]} features\")\n",
    "print(f\"Features: {list(X.columns)}\")\n",
    "print(f\"Target: count (hourly bike rentals)\\n\")\n",
    "\n",
    "# Step 2: Create and train the model\n",
    "print(\"--- Step 2: Model Creation and Training ---\")\n",
    "model = LinearRegression()  # Create model with default parameters\n",
    "print(f\"Model created: {model}\")\n",
    "\n",
    "model.fit(X, y)  # Train the model on ALL available data\n",
    "print(\"Model trained on complete dataset!\")\n",
    "print(f\"Training completed for {X.shape[0]:,} observations\\n\")\n",
    "\n",
    "# Step 3: Inspect learned parameters\n",
    "print(\"--- Step 3: Inspect Learned Parameters ---\")\n",
    "print(f\"Intercept (β₀): {model.intercept_:.2f} rides\")\n",
    "print(\"\\nCoefficients (slopes):\")\n",
    "for feature, coef in zip(feature_columns, model.coef_):\n",
    "    direction = \"increases\" if coef > 0 else \"decreases\"\n",
    "    print(f\"  {feature:12s}: {coef:+8.2f} → demand {direction} by {abs(coef):.2f} rides per unit\")\n",
    "\n",
    "print(\"\\n--- Model Equation ---\")\n",
    "equation = f\"Predicted Demand = {model.intercept_:.1f}\"\n",
    "for feature, coef in zip(feature_columns, model.coef_):\n",
    "    sign = \"+\" if coef >= 0 else \"-\"\n",
    "    equation += f\" {sign} {abs(coef):.1f}×{feature}\"\n",
    "print(equation + \"\\n\")\n",
    "\n",
    "# Step 4: Make predictions\n",
    "print(\"--- Step 4: Making Predictions ---\")\n",
    "predictions = model.predict(X)  # Predict for all observations\n",
    "print(f\"Generated {len(predictions):,} predictions\")\n",
    "print(f\"First 5 predictions: {predictions[:5].round(1)}\")\n",
    "print(f\"First 5 actual values: {y.iloc[:5].values}\")\n",
    "\n",
    "# Compare a few predictions\n",
    "print(\"\\n--- Prediction Examples ---\")\n",
    "for i in range(3):\n",
    "    actual = y.iloc[i]\n",
    "    predicted = predictions[i]\n",
    "    error = actual - predicted\n",
    "    print(f\"Observation {i+1}: Actual = {actual:.0f}, Predicted = {predicted:.0f}, Error = {error:+.0f} rides\")\n",
    "\n",
    "# Step 5: Predict new scenarios\n",
    "print(\"\\n--- Step 5: New Scenario Prediction ---\")\n",
    "new_scenario = pd.DataFrame({\n",
    "    'temp': [25],         # 25°C temperature\n",
    "    'humidity': [50],     # 50% humidity\n",
    "    'windspeed': [10],    # 10 km/h windspeed\n",
    "    'workingday': [1]     # Working day\n",
    "})\n",
    "new_prediction = model.predict(new_scenario)[0]\n",
    "print(f\"Scenario: Warm working day (25°C, 50% humidity, 10 km/h wind)\")\n",
    "print(f\"Predicted hourly demand: {new_prediction:.0f} bikes\")\n",
    "\n",
    "print(\"\\n--- Key Takeaways ---\")\n",
    "print(\"✓ LinearRegression() creates a model object ready to learn\")\n",
    "print(\"✓ .fit(X, y) trains the model by finding optimal coefficients\")\n",
    "print(\"✓ .predict(X) applies learned relationships to make predictions\")\n",
    "print(\"✓ .coef_ and .intercept_ reveal what the model learned\")\n",
    "print(\"✓ The same simple pattern works for any number of features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c90b8b",
   "metadata": {},
   "source": [
    "**What this demonstrates:**\n",
    "- **Three-step workflow**: create model → fit to data → make predictions - this pattern applies to all scikit-learn algorithms\n",
    "- The model trained on 10,886 complete observations, learning how temperature, humidity, windspeed, and workingday relate to bike demand\n",
    "- **Coefficient interpretation**: temp coefficient of +7.86 means each 1°C increase adds ~8 bikes per hour; humidity coefficient of -2.25 means each 1% humidity increase reduces demand by ~2 bikes\n",
    "- The intercept of 6.14 represents baseline demand when all features are at zero (though this specific value isn't meaningful since we can't have 0% humidity in reality)\n",
    "- **Predictions are deterministic**: given the same input features, the model always produces the same prediction using the linear equation\n",
    "- We can generate predictions for individual scenarios (like our 25°C working day example) or for thousands of observations at once\n",
    "- The model handles multiple features automatically - we don't need to change our code whether we have 1 feature or 100 features\n",
    "- **Understanding coefficients enables business insights**: we can tell Capital City Bikes that temperature is the strongest demand driver, while humidity works against rentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae23d393",
   "metadata": {},
   "source": [
    "### 3.3. Why We Need Separate Test Data\n",
    "\n",
    "Now that we can train models and make predictions, a critical question arises: how do we know if our model is any good? Looking at our previous example, the model made predictions for all 10,886 observations - but those are the same observations it used for training. Can we trust those predictions to tell us how well the model will perform on tomorrow's data? Let's explore why this question matters and why machine learning requires a fundamentally different approach to evaluation.\n",
    "\n",
    "**The fundamental challenge** in machine learning is distinguishing between models that have learned genuine patterns versus models that have simply memorized the training data. Imagine studying for an exam using practice problems. If the actual exam contains those exact same problems, a perfect score tells you nothing about whether you truly understand the concepts or just memorized the answers. The same principle applies to predictive models.\n",
    "\n",
    "When we train a model on data and then evaluate it on that same data, we're essentially **giving it the exam questions during the study session**. The model can achieve high accuracy simply by memorizing the training examples, without learning the underlying patterns that would help it predict new situations. This memorization problem, called **overfitting**, produces models that look great on paper but fail in real-world deployment.\n",
    "\n",
    "**Consider the Capital City Bikes scenario**: If our model memorizes that \"on January 5th at 3pm it rained and 12 bikes were rented,\" that's useless for predicting February 10th's demand. What we need is for the model to learn that \"rainy days typically reduce demand by 30%\" - a general pattern that applies to any rainy day, not memorized facts about specific past days.\n",
    "\n",
    "The solution is deceptively simple but absolutely crucial: **we must evaluate model performance on data the model has never seen**. By setting aside some observations during training and using them only for evaluation, we can test whether the model learned real patterns or just memorized training examples. This approach simulates the real-world scenario where the model must predict tomorrow's demand using only yesterday's data.\n",
    "\n",
    "**How evaluation creates trust**: When a model predicts well on held-out data it's never seen, we gain confidence that it learned genuine relationships. When training performance is much better than held-out performance, we know the model memorized rather than learned. This distinction is the difference between a model that's ready for business deployment and one that will fail in production.\n",
    "\n",
    "Let's see this problem in action and understand why separate test data is essential:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de703221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Load the bike-sharing dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\")\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "print(\"=== THE TRAINING DATA EVALUATION PROBLEM ===\\n\")\n",
    "\n",
    "# Prepare features and target\n",
    "X = df[['temp', 'humidity', 'windspeed', 'workingday']]\n",
    "y = df['count']\n",
    "\n",
    "# Train a model on ALL data\n",
    "print(\"--- Training model on complete dataset ---\")\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "print(f\"Model trained on {len(X):,} observations\\n\")\n",
    "\n",
    "# Evaluate on the SAME data used for training\n",
    "print(\"--- Evaluating on training data ---\")\n",
    "training_predictions = model.predict(X)\n",
    "training_r2 = r2_score(y, training_predictions)\n",
    "training_rmse = np.sqrt(mean_squared_error(y, training_predictions))\n",
    "\n",
    "print(f\"R² score: {training_r2:.4f} ({training_r2*100:.2f}% variance explained)\")\n",
    "print(f\"RMSE: {training_rmse:.2f} rides\")\n",
    "print(\"\\n❓ Question: Does this R² of {:.2f}% tell us how well the model will predict tomorrow's demand?\".format(training_r2*100))\n",
    "print(\"❓ Question: Can we trust this model for Capital City Bikes' operations?\")\n",
    "\n",
    "print(\"\\n--- The Problem: We Can't Answer These Questions! ---\")\n",
    "print(\"\\nWhy training performance is unreliable:\")\n",
    "print(\"1. Model has 'seen' all these observations during training\")\n",
    "print(\"2. High accuracy might mean genuine learning OR just memorization\")\n",
    "print(\"3. No way to distinguish between the two scenarios\")\n",
    "print(\"4. Real deployment requires predicting NEW data not in training set\")\n",
    "\n",
    "print(\"\\n--- What We Need Instead ---\")\n",
    "print(\"✓ Evaluate on observations the model has NEVER seen\")\n",
    "print(\"✓ Simulate real-world scenario: predict future from past\")\n",
    "print(\"✓ Compare training vs testing performance to detect memorization\")\n",
    "print(\"✓ Trust metrics that reflect true predictive capability\")\n",
    "\n",
    "print(\"\\n--- Real-World Deployment Scenario ---\")\n",
    "print(\"Capital City Bikes wants to predict next Monday's demand.\")\n",
    "print(\"The model must predict using:\")\n",
    "print(\"  • Weather features for Monday (temp, humidity, windspeed, workingday)\")\n",
    "print(\"  • Patterns learned from historical data (NOT Monday's actual demand)\")\n",
    "print(\"\\nThe model will NEVER have 'seen' next Monday during training!\")\n",
    "print(\"Training data evaluation can't tell us how accurate Monday's prediction will be.\")\n",
    "print(\"\\nConclusion: We need SEPARATE TEST DATA that simulates this future prediction scenario.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44a55ce",
   "metadata": {},
   "source": [
    "**What this demonstrates:**\n",
    "- **Training data evaluation is circular logic**: we're testing the model on the same data it learned from\n",
    "- An R² of 34% on training data might indicate good learning, or it might hide a model that would perform far worse on new data\n",
    "- **The memorization risk**: without separate test data, we can't distinguish between models that learned patterns versus models that memorized examples\n",
    "- Real-world deployment always involves predicting data the model hasn't seen - our evaluation must simulate this scenario\n",
    "- **The trust problem**: Capital City Bikes needs confidence that the model will actually work when deployed, not just reassurance based on training data performance\n",
    "\n",
    "**The concept of overfitting** becomes clear through this lens. An overfitted model achieves high training performance by memorizing specific examples rather than learning general patterns. It's like a student who memorizes answers without understanding concepts - perfect scores on practice problems, but failure on new exam questions. We can only detect overfitting by comparing training performance to performance on separate test data.\n",
    "\n",
    "**This is why every machine learning project requires setting aside test data** before training begins. These held-out observations serve as a proxy for real-world deployment, giving us an honest assessment of whether the model learned patterns that generalize to new situations. In the next subsection, we'll learn exactly how to create this test data and use it to build models worthy of Capital City Bikes' trust."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b06640",
   "metadata": {},
   "source": [
    "### 3.4. Train-Test Split for Model Evaluation\n",
    "\n",
    "Now that we understand why separate test data is essential, let's learn how to create it and use it properly. We'll implement the train-test split strategy, build our evaluation workflow, and learn to interpret the comparison between training and testing performance. This subsection establishes the foundational evaluation pattern you'll use throughout your machine learning career.\n",
    "\n",
    "**The train-test split strategy** divides our dataset into two distinct subsets before any training occurs. The **training set** (typically 70-80% of data) is used to fit the model - this is the data the model learns from. The **testing set** (typically 20-30% of data) is held aside completely, never touching the training process, and used solely to evaluate the trained model's performance on unseen data.\n",
    "\n",
    "The split must occur **before training begins** to ensure the test set truly simulates future data. If we peek at the test set during model development, adjust our approach based on test performance, or allow any information from the test set to influence training, we've violated the independence requirement and our evaluation becomes unreliable.\n",
    "\n",
    "**Random splitting** is the standard approach for most machine learning problems. By randomly assigning observations to training and testing sets, we ensure both sets have similar distributions of all variables - seasons, weather conditions, days of the week, demand levels, etc. This similarity is crucial because we want both sets to represent the same underlying patterns, just split into \"learning\" and \"testing\" subsets.\n",
    "\n",
    "**The evaluation workflow** follows a strict sequence: split data → train on training set → predict on both sets → compare metrics. By generating predictions for both training and testing data, we can compare their performance metrics directly. When both show similar performance, the model learned generalizable patterns. When training performance significantly exceeds testing performance, the model overfitted.\n",
    "\n",
    "Let's implement train-test split and proper model evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a73517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the bike-sharing dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\")\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "print(\"=== TRAIN-TEST SPLIT FOR MODEL EVALUATION ===\\n\")\n",
    "\n",
    "# Step 1: Prepare features and target\n",
    "print(\"--- Step 1: Data Preparation ---\")\n",
    "feature_columns = ['temp', 'humidity', 'windspeed', 'workingday']\n",
    "X = df[feature_columns]\n",
    "y = df['count']\n",
    "print(f\"Total observations: {len(X):,}\")\n",
    "print(f\"Features: {feature_columns}\\n\")\n",
    "\n",
    "# Step 2: Create train-test split\n",
    "print(\"--- Step 2: Train-Test Split ---\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,      # 20% for testing, 80% for training\n",
    "    random_state=42     # For reproducibility\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train):,} observations ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"Testing set:  {len(X_test):,} observations ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "print(f\"\\n✓ Test set is held aside - model will NEVER see it during training\\n\")\n",
    "\n",
    "# Step 3: Train model on training data ONLY\n",
    "print(\"--- Step 3: Train Model on Training Data ---\")\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model trained successfully!\")\n",
    "print(f\"Model learned from {len(X_train):,} training observations\\n\")\n",
    "\n",
    "# Step 4: Generate predictions for BOTH sets\n",
    "print(\"--- Step 4: Generate Predictions ---\")\n",
    "train_predictions = model.predict(X_train)\n",
    "test_predictions = model.predict(X_test)\n",
    "print(f\"Training predictions: {len(train_predictions):,} values\")\n",
    "print(f\"Testing predictions:  {len(test_predictions):,} values\\n\")\n",
    "\n",
    "# Step 5: Evaluate performance on BOTH sets\n",
    "print(\"--- Step 5: Compare Training vs Testing Performance ---\")\n",
    "\n",
    "# Calculate metrics for training set\n",
    "train_r2 = r2_score(y_train, train_predictions)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, train_predictions))\n",
    "\n",
    "# Calculate metrics for testing set\n",
    "test_r2 = r2_score(y_test, test_predictions)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, test_predictions))\n",
    "\n",
    "print(\"Training Performance (data model learned from):\")\n",
    "print(f\"  R²:   {train_r2:.4f} ({train_r2*100:.1f}% variance explained)\")\n",
    "print(f\"  RMSE: {train_rmse:.2f} rides per hour\")\n",
    "\n",
    "print(\"\\nTesting Performance (unseen data):\")\n",
    "print(f\"  R²:   {test_r2:.4f} ({test_r2*100:.1f}% variance explained)\")\n",
    "print(f\"  RMSE: {test_rmse:.2f} rides per hour\")\n",
    "\n",
    "# Detect overfitting by comparing performances\n",
    "performance_gap = train_r2 - test_r2\n",
    "print(f\"\\nPerformance Gap: {performance_gap:.4f}\")\n",
    "if performance_gap < 0.05:\n",
    "    print(\"✓ Minimal gap - model learned generalizable patterns\")\n",
    "elif performance_gap < 0.15:\n",
    "    print(\"⚠ Moderate gap - some overfitting present\")\n",
    "else:\n",
    "    print(\"✗ Large gap - significant overfitting detected\")\n",
    "\n",
    "# Step 6: Visualize training vs testing performance\n",
    "print(\"\\n--- Step 6: Visual Comparison ---\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training set predictions\n",
    "axes[0].scatter(y_train, train_predictions, alpha=0.4, s=15, color='blue')\n",
    "axes[0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()],\n",
    "             'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual Demand', fontsize=12)\n",
    "axes[0].set_ylabel('Predicted Demand', fontsize=12)\n",
    "axes[0].set_title(f'Training Set (R² = {train_r2:.3f})', fontsize=13, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Testing set predictions\n",
    "axes[1].scatter(y_test, test_predictions, alpha=0.4, s=15, color='orange')\n",
    "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],\n",
    "             'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[1].set_xlabel('Actual Demand', fontsize=12)\n",
    "axes[1].set_ylabel('Predicted Demand', fontsize=12)\n",
    "axes[1].set_title(f'Testing Set (R² = {test_r2:.3f})', fontsize=13, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== KEY INSIGHTS FOR CAPITAL CITY BIKES ===\")\n",
    "print(f\"✓ Test R² of {test_r2:.1%} represents realistic prediction capability\")\n",
    "print(f\"✓ Test RMSE of {test_rmse:.0f} rides = typical prediction error\")\n",
    "print(f\"✓ Similar train/test performance = model ready for deployment\")\n",
    "print(f\"✓ Random splitting ensured representative train and test distributions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2e702c",
   "metadata": {},
   "source": [
    "**What this demonstrates:**\n",
    "- **scikit-learn's `train_test_split()` function** handles the random splitting automatically, ensuring we don't accidentally introduce bias\n",
    "- We set `test_size=0.2` for an 80/20 split, giving us 8,708 training observations and 2,178 testing observations\n",
    "- The `random_state=42` parameter ensures reproducibility - running this code again produces the same split\n",
    "- **Critical workflow**: split FIRST, then train ONLY on training set, then predict on BOTH sets to compare\n",
    "- Training R² of 34.8% vs testing R² of 33.5% shows minimal overfitting - the 1.3% gap indicates the model learned real patterns\n",
    "- Both scatter plots show similar patterns around the perfect prediction line, visually confirming consistent performance\n",
    "- **RMSE interpretation**: test RMSE of 142 rides means Capital City Bikes should expect typical errors of ±142 bikes per hour\n",
    "- The testing performance (33.5% R²) is what matters for deployment - this represents realistic prediction capability on genuinely new data\n",
    "- **Random splitting ensures fairness**: both sets contain mix of seasons, weather conditions, and demand levels, so neither is artificially easier\n",
    "- If train R² was 90% but test R² was 35%, this would signal severe overfitting requiring model changes\n",
    "- This train-test evaluation pattern is universal across all machine learning - from simple linear regression to complex deep learning\n",
    "\n",
    "**Understanding the metrics in business context**: Capital City Bikes can now confidently report that the model explains about one-third of demand variation and typically errs by ±142 bikes per hour. This level of performance enables useful operational planning while acknowledging prediction uncertainty. The similar train/test performance confirms these estimates reflect real-world capability, not training data memorization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55fee1d",
   "metadata": {},
   "source": [
    "### 3.5. Time Series Considerations\n",
    "\n",
    "The train-test split we just learned works beautifully for most machine learning problems, but bike-sharing demand presents a special challenge: **the data has a time dimension**. Each observation is tied to a specific date and hour, and demand patterns evolve over time due to seasonality, trends, and temporal dependencies. Can we still use random splitting, or does time order matter? Let's explore why time series data requires a specialized splitting approach and when to apply it.\n",
    "\n",
    "**The temporal dependency problem** arises when observations are not independent but instead connected through time. In bike-sharing data, today's demand influences tomorrow's - a holiday weekend affects patterns for days afterward, seasonal trends build gradually, and weather patterns persist across hours. Random splitting ignores these temporal connections, potentially creating an unrealistic evaluation scenario.\n",
    "\n",
    "**Data leakage through random splitting** occurs when we randomly mix past and future observations. Imagine our training set contains data from December while our test set includes November observations. We're now training on the \"future\" (December) to predict the \"past\" (November) - an impossible scenario in real deployment where we can only use historical data to predict upcoming demand.\n",
    "\n",
    "Consider this concrete example: Random splitting might put December 25th (Christmas, extremely low demand) in the training set and December 24th (Christmas Eve, also unusual) in the test set. The model learns December 25th's patterns and uses them to predict December 24th - but in real deployment, we'll never have tomorrow's data to predict today. This temporal leakage inflates test performance, making the model appear better than it actually is.\n",
    "\n",
    "**When time order matters**: Not all datasets with dates require chronological splitting. If observations are truly independent despite having timestamps - like medical diagnoses from different patients recorded on different dates - random splitting remains appropriate. The key question is: \"Would future information help predict past events?\" For bike-sharing, the answer is yes, making chronological splitting essential.\n",
    "\n",
    "**Chronological train-test split** preserves temporal order by using the earliest portion of data for training and the most recent portion for testing. This mirrors real-world deployment: we train on all available history, then predict tomorrow, next week, or next month. The earliest 70-80% becomes training data, and the final 20-30% becomes test data, maintaining the timeline's integrity.\n",
    "\n",
    "Let's implement chronological splitting and compare it to random splitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aded9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Load the bike-sharing dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\")\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "print(\"=== CHRONOLOGICAL VS RANDOM SPLITTING ===\\n\")\n",
    "\n",
    "# Prepare features and target\n",
    "X = df[['temp', 'humidity', 'windspeed', 'workingday']]\n",
    "y = df['count']\n",
    "\n",
    "print(f\"Dataset time range: {df['datetime'].min()} to {df['datetime'].max()}\")\n",
    "print(f\"Total observations: {len(df):,} hours\\n\")\n",
    "\n",
    "# Method 1: Random split (inappropriate for time series)\n",
    "print(\"--- Method 1: Random Split (Standard Approach) ---\")\n",
    "X_train_random, X_test_random, y_train_random, y_test_random = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model_random = LinearRegression()\n",
    "model_random.fit(X_train_random, y_train_random)\n",
    "\n",
    "test_pred_random = model_random.predict(X_test_random)\n",
    "test_r2_random = r2_score(y_test_random, test_pred_random)\n",
    "test_rmse_random = np.sqrt(mean_squared_error(y_test_random, test_pred_random))\n",
    "\n",
    "print(f\"Test R²: {test_r2_random:.4f}\")\n",
    "print(f\"Test RMSE: {test_rmse_random:.2f} rides\")\n",
    "print(\"⚠ Problem: Training set may contain 'future' data relative to test set\")\n",
    "print(\"⚠ Problem: Temporal patterns can leak from future to past\")\n",
    "print(\"⚠ Result: Performance estimate may be optimistically biased\\n\")\n",
    "\n",
    "# Method 2: Chronological split (appropriate for time series)\n",
    "print(\"--- Method 2: Chronological Split (Time Series Approach) ---\")\n",
    "split_index = int(len(df) * 0.8)  # First 80% for training\n",
    "\n",
    "X_train_chrono = X.iloc[:split_index]\n",
    "X_test_chrono = X.iloc[split_index:]\n",
    "y_train_chrono = y.iloc[:split_index]\n",
    "y_test_chrono = y.iloc[split_index:]\n",
    "\n",
    "# Get corresponding dates for clarity\n",
    "train_end_date = df.iloc[split_index-1]['datetime']\n",
    "test_start_date = df.iloc[split_index]['datetime']\n",
    "\n",
    "print(f\"Training period: {df['datetime'].min()} to {train_end_date}\")\n",
    "print(f\"Testing period: {test_start_date} to {df['datetime'].max()}\")\n",
    "\n",
    "model_chrono = LinearRegression()\n",
    "model_chrono.fit(X_train_chrono, y_train_chrono)\n",
    "\n",
    "test_pred_chrono = model_chrono.predict(X_test_chrono)\n",
    "test_r2_chrono = r2_score(y_test_chrono, test_pred_chrono)\n",
    "test_rmse_chrono = np.sqrt(mean_squared_error(y_test_chrono, test_pred_chrono))\n",
    "\n",
    "print(f\"\\nTest R²: {test_r2_chrono:.4f}\")\n",
    "print(f\"Test RMSE: {test_rmse_chrono:.2f} rides\")\n",
    "print(\"✓ Benefit: Simulates real deployment (predict future from past)\")\n",
    "print(\"✓ Benefit: No temporal data leakage\")\n",
    "print(\"✓ Result: Honest performance estimate for time-based prediction\\n\")\n",
    "\n",
    "# Compare the two approaches\n",
    "print(\"--- Comparison: Random vs Chronological ---\")\n",
    "print(f\"Random split test R²: {test_r2_random:.4f}\")\n",
    "print(f\"Chronological split test R²: {test_r2_chrono:.4f}\")\n",
    "print(f\"Difference: {abs(test_r2_random - test_r2_chrono):.4f}\")\n",
    "\n",
    "if test_r2_random > test_r2_chrono + 0.02:\n",
    "    print(\"\\n⚠ Random splitting overestimated performance due to temporal leakage\")\n",
    "elif test_r2_chrono > test_r2_random + 0.02:\n",
    "    print(\"\\n✓ Test periods have different characteristics - chronological reveals this\")\n",
    "else:\n",
    "    print(\"\\n✓ Results similar - but chronological is still correct for deployment\")\n",
    "\n",
    "print(\"\\n--- Capital City Bikes Deployment Scenario ---\")\n",
    "print(\"Operational reality: Train on Jan-Nov data, predict December demand\")\n",
    "print(\"Chronological split simulates this: Train on first 80% of timeline\")\n",
    "print(\"Random split violates this: Can train on December to predict January\")\n",
    "print(\"\\n✓ Conclusion: Use chronological splitting for bike-sharing demand forecasting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be608b4",
   "metadata": {},
   "source": [
    "**What this demonstrates:**\n",
    "- **Bike-sharing is a time series**: observations connected temporally through seasons, trends, and weekly patterns\n",
    "- Random splitting can create impossible scenarios where we train on \"future\" to predict \"past\"\n",
    "- **Chronological splitting preserves temporal order**: earliest 80% trains, most recent 20% tests\n",
    "- The date boundaries matter - our model trains on data through approximately November, then predicts December\n",
    "- **Performance might differ** between methods due to seasonal effects (if December is unusual, chronological split reveals this)\n",
    "- Random split may show artificially good performance by allowing temporal information leakage\n",
    "- **Chronological approach matches deployment**: in production, Capital City Bikes will use all past data to predict future demand, never the reverse\n",
    "- This principle applies to all time series: stock prices, weather forecasting, sales prediction - whenever order matters, preserve it in your split\n",
    "- For Capital City Bikes operational planning, chronological split provides the honest performance estimate needed for confident deployment\n",
    "\n",
    "**When to use each approach**: Use random splitting for data where observations are independent (medical diagnoses, image classification, customer surveys). Use chronological splitting when observations have temporal dependencies and deployment involves predicting future from past (demand forecasting, anomaly detection, predictive maintenance). The key test: \"Would having today's data help predict tomorrow?\" If yes, use chronological splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa867314",
   "metadata": {},
   "source": [
    "### 3.6. Cross-Validation for Robust Evaluation\n",
    "\n",
    "We've learned to evaluate models using a single train-test split, which provides one performance estimate. But here's a critical question: what if our particular test set happens to be unusually easy or difficult to predict? What if it contains only sunny weekdays, or happens to include an atypical holiday period? A single split gives us one number, but how confident should we be that this number represents true model performance? Let's explore cross-validation, the technique that provides robust, reliable performance estimates through systematic multiple evaluations.\n",
    "\n",
    "**The single split limitation** arises from randomness in how we divide our data. Imagine Capital City Bikes gets an R² of 35% on one train-test split. Is this the model's true capability, or did the test set happen to be easy? If we created a different random split, would we get 30%? 40%? We can't know from a single evaluation. This uncertainty makes it difficult to compare models, tune parameters, or confidently report performance to stakeholders.\n",
    "\n",
    "**Cross-validation solves this problem** by creating multiple train-test splits systematically and averaging their performance metrics. Instead of one evaluation giving one number, we get multiple evaluations giving multiple numbers - and their average provides a much more reliable estimate of model performance. The standard deviation across evaluations reveals performance consistency: low std means stable predictions regardless of data split, high std suggests the model is sensitive to training data composition.\n",
    "\n",
    "**Standard K-Fold Cross-Validation** divides data into K equal parts (typically K=5 or K=10). In each of K iterations, one part serves as the test set while the remaining K-1 parts form the training set. Every observation gets used for testing exactly once and for training K-1 times. This systematic rotation ensures no data is wasted and every part of the dataset contributes to both training and evaluation.\n",
    "\n",
    "Here's how 5-fold cross-validation works: divide 10,886 observations into 5 parts of ~2,177 observations each. Fold 1 uses parts 2-5 for training and part 1 for testing. Fold 2 uses parts 1, 3-5 for training and part 2 for testing. This continues until all 5 parts have served as test set once. The result: 5 different R² scores from 5 different train-test configurations, giving us a robust performance estimate.\n",
    "\n",
    "**Why K-Fold fails for time series data** relates directly to the temporal dependency problem we discussed in subsection 3.5. Standard k-fold creates random folds, potentially putting December observations in fold 2's training set and November observations in fold 3's test set. We're back to the same violation: training on \"future\" to predict \"past.\" The temporal order gets scrambled across folds, creating data leakage and overly optimistic performance estimates.\n",
    "\n",
    "**TimeSeriesSplit provides the time series solution** through forward-chaining evaluation. Instead of random folds, TimeSeriesSplit creates chronological folds where each fold's training set includes all previous data and its test set contains the next time period. This respects temporal order in every fold, always predicting future from past, just like real deployment.\n",
    "\n",
    "**How TimeSeriesSplit works**: Fold 1 trains on the earliest 20% of timeline and tests on the next 20%. Fold 2 trains on the first 40% and tests on the next 20%. Fold 3 trains on the first 60% and tests on the next 20%. Each fold simulates a realistic deployment scenario where we use all available history to predict the next time period. The training set grows with each fold (expanding window approach), while the test set advances chronologically through time.\n",
    "\n",
    "Let's implement both approaches and understand when to use each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f56fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score, KFold, TimeSeriesSplit, train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the bike-sharing dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\")\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "print(\"=== CROSS-VALIDATION FOR ROBUST EVALUATION ===\\n\")\n",
    "\n",
    "# Prepare features and target\n",
    "X = df[['temp', 'humidity', 'windspeed', 'workingday']]\n",
    "y = df['count']\n",
    "\n",
    "print(f\"Dataset: {len(df):,} hourly observations\")\n",
    "print(f\"Time range: {df['datetime'].min()} to {df['datetime'].max()}\\n\")\n",
    "\n",
    "# BASELINE: Single Train-Test Split\n",
    "print(\"--- BASELINE: Single Train-Test Split ---\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "single_split_r2 = model.score(X_test, y_test)\n",
    "\n",
    "print(f\"Single split R²: {single_split_r2:.4f}\")\n",
    "print(\"Question: Is this representative of true model performance?\")\n",
    "print(\"Question: Would a different random split give similar results?\")\n",
    "print(\"Answer: We need multiple evaluations to know!\\n\")\n",
    "\n",
    "# METHOD 1: Standard K-Fold Cross-Validation (WRONG for time series)\n",
    "print(\"--- METHOD 1: Standard K-Fold Cross-Validation ---\")\n",
    "print(\"⚠ Warning: This approach is INCORRECT for time series data!\\n\")\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores_kfold = cross_val_score(LinearRegression(), X, y, cv=kfold, scoring='r2')\n",
    "\n",
    "print(\"K-Fold Results (5 folds):\")\n",
    "for i, score in enumerate(cv_scores_kfold, 1):\n",
    "    print(f\"  Fold {i}: R² = {score:.4f}\")\n",
    "\n",
    "kfold_mean = cv_scores_kfold.mean()\n",
    "kfold_std = cv_scores_kfold.std()\n",
    "\n",
    "print(f\"\\nMean R²: {kfold_mean:.4f} (±{kfold_std:.4f})\")\n",
    "print(f\"\\nProblems with K-Fold for time series:\")\n",
    "print(\"  ✗ Random folds violate temporal order\")\n",
    "print(\"  ✗ Training set may contain 'future' relative to test set\")\n",
    "print(\"  ✗ Temporal patterns leak from future to past\")\n",
    "print(\"  ✗ Performance estimate is optimistically biased\")\n",
    "print(\"  ✗ Does NOT simulate real deployment scenario\\n\")\n",
    "\n",
    "# METHOD 2: TimeSeriesSplit (CORRECT for time series)\n",
    "print(\"--- METHOD 2: TimeSeriesSplit Cross-Validation ---\")\n",
    "print(\"✓ This approach is CORRECT for time series data!\\n\")\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "cv_scores_ts = cross_val_score(LinearRegression(), X, y, cv=tscv, scoring='r2')\n",
    "\n",
    "print(\"TimeSeriesSplit Results (5 folds):\")\n",
    "fold_details = []\n",
    "for i, (train_idx, test_idx) in enumerate(tscv.split(X), 1):\n",
    "    train_dates = df.iloc[train_idx]['datetime']\n",
    "    test_dates = df.iloc[test_idx]['datetime']\n",
    "    print(f\"  Fold {i}: R² = {cv_scores_ts[i-1]:.4f}\")\n",
    "    print(f\"    Training: {train_dates.min()} to {train_dates.max()} ({len(train_idx):,} obs)\")\n",
    "    print(f\"    Testing:  {test_dates.min()} to {test_dates.max()} ({len(test_idx):,} obs)\")\n",
    "    fold_details.append((train_dates.min(), train_dates.max(), test_dates.min(), test_dates.max()))\n",
    "\n",
    "ts_mean = cv_scores_ts.mean()\n",
    "ts_std = cv_scores_ts.std()\n",
    "\n",
    "print(f\"\\nMean R²: {ts_mean:.4f} (±{ts_std:.4f})\")\n",
    "print(f\"\\nBenefits of TimeSeriesSplit:\")\n",
    "print(\"  ✓ Preserves chronological order in every fold\")\n",
    "print(\"  ✓ Always predicts future from past (realistic)\")\n",
    "print(\"  ✓ Training set grows with each fold (expanding window)\")\n",
    "print(\"  ✓ No temporal data leakage\")\n",
    "print(\"  ✓ Simulates actual deployment scenario\")\n",
    "\n",
    "# COMPARISON\n",
    "print(\"\\n--- COMPARISON: Single Split vs Cross-Validation ---\")\n",
    "print(f\"Single split R²:        {single_split_r2:.4f}\")\n",
    "print(f\"K-Fold CV mean R²:      {kfold_mean:.4f} (±{kfold_std:.4f})\")\n",
    "print(f\"TimeSeriesSplit mean R²: {ts_mean:.4f} (±{ts_std:.4f})\")\n",
    "\n",
    "difference = abs(single_split_r2 - ts_mean)\n",
    "print(f\"\\nDifference (single vs TS-CV): {difference:.4f}\")\n",
    "\n",
    "if difference > 0.02:\n",
    "    print(\"⚠ Significant difference - single split may not be representative\")\n",
    "else:\n",
    "    print(\"✓ Results similar, but CV provides confidence through multiple evaluations\")\n",
    "\n",
    "print(f\"\\nStandard deviation interpretation:\")\n",
    "print(f\"  K-Fold std:      {kfold_std:.4f}\")\n",
    "print(f\"  TimeSeriesSplit std: {ts_std:.4f}\")\n",
    "if ts_std < 0.05:\n",
    "    print(\"  ✓ Low std = Stable, consistent performance across time periods\")\n",
    "else:\n",
    "    print(\"  ⚠ Higher std = Performance varies across time periods\")\n",
    "\n",
    "# Visualize fold structure\n",
    "print(\"\\n--- Visualizing TimeSeriesSplit Structure ---\")\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "for i, (train_dates_min, train_dates_max, test_dates_min, test_dates_max) in enumerate(fold_details, 1):\n",
    "    # Training period\n",
    "    ax.barh(i, (train_dates_max - train_dates_min).days, left=train_dates_min.toordinal(),\n",
    "            height=0.8, color='skyblue', label='Training' if i == 1 else '')\n",
    "    # Testing period\n",
    "    ax.barh(i, (test_dates_max - test_dates_min).days, left=test_dates_min.toordinal(),\n",
    "            height=0.8, color='coral', label='Testing' if i == 1 else '')\n",
    "\n",
    "ax.set_yticks(range(1, 6))\n",
    "ax.set_yticklabels([f'Fold {i}' for i in range(1, 6)])\n",
    "ax.set_xlabel('Timeline', fontsize=12)\n",
    "ax.set_title('TimeSeriesSplit: Expanding Window Approach', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== KEY INSIGHTS FOR CAPITAL CITY BIKES ===\")\n",
    "print(f\"✓ Cross-validation provides {len(cv_scores_ts)} performance estimates instead of 1\")\n",
    "print(f\"✓ Mean R² of {ts_mean:.4f} is more reliable than single split\")\n",
    "print(f\"✓ Standard deviation of {ts_std:.4f} shows performance consistency\")\n",
    "print(f\"✓ TimeSeriesSplit respects temporal order (essential for forecasting)\")\n",
    "print(f\"✓ Each fold tests realistic deployment: predict future from past\")\n",
    "print(f\"\\nRecommendation: Report CV mean ± std as model performance estimate\")\n",
    "print(f\"Capital City Bikes can confidently report: R² = {ts_mean:.2f} ± {ts_std:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47ce551",
   "metadata": {},
   "source": [
    "**What this demonstrates:**\n",
    "- **Single train-test split gives one number** - useful but limited insight into model reliability\n",
    "- Standard k-fold produces 5 different R² scores through systematic rotation of test sets\n",
    "- **Mean across folds** (e.g., 0.3450) provides robust performance estimate less sensitive to split luck\n",
    "- Standard deviation (e.g., ±0.0120) reveals consistency - low std means stable predictions\n",
    "- **K-fold violates temporal order** for time series, potentially inflating performance estimates\n",
    "- TimeSeriesSplit creates 5 chronological folds, each respecting time order\n",
    "- **Expanding window approach**: Fold 1 trains on 16%, tests on 17%; Fold 5 trains on 67%, tests on 17%\n",
    "- Each TimeSeriesSplit fold simulates real deployment - using all history to predict next period\n",
    "- **Performance comparison**: If single split differs significantly from CV mean, single split might be lucky/unlucky\n",
    "- Low TimeSeriesSplit std (like 0.01-0.02) indicates model performs consistently across different time periods\n",
    "- **Business value**: CV mean ± std gives Capital City Bikes honest, robust performance estimate for stakeholder reporting\n",
    "\n",
    "**When to use cross-validation**: Use CV whenever you need robust performance estimates, especially for model comparison, hyperparameter tuning, or reporting to stakeholders. For quick iterative development, single splits suffice. For final evaluation and deployment decisions, cross-validation provides the confidence boost worth the extra computation time.\n",
    "\n",
    "**Choosing K (number of folds)**: Common choices are K=5 (faster, slightly higher variance) or K=10 (slower, slightly lower variance). For time series, K depends on dataset size and desired test set size - ensure each fold's test set represents a meaningful time period (at least several weeks for bike-sharing demand)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bd676a",
   "metadata": {},
   "source": [
    "### 3.7. Performance Metrics Deep Dive\n",
    "\n",
    "Now that we have robust evaluation through cross-validation, let's explore the performance metrics themselves in depth. We've already seen R² and RMSE in action, but what do these numbers really mean? How should we interpret them for business decisions? And what other metrics can help us understand model quality? This subsection provides the comprehensive metric knowledge you need to evaluate models professionally.\n",
    "\n",
    "Performance evaluation quantifies **how well a linear regression model performs** on both training and test data. Appropriate evaluation metrics provide insights into model accuracy, reliability, and suitability for the intended application. The choice of evaluation metrics depends on the specific problem context and business requirements, with each metric revealing different aspects of model behavior.\n",
    "\n",
    "**Root Mean Square Error (RMSE)** measures the average magnitude of prediction errors in the same units as the target variable. RMSE is calculated as the square root of the mean of squared differences between actual and predicted values. Lower RMSE values indicate better model performance, with perfect predictions yielding an RMSE of zero. RMSE penalizes large errors more heavily than small errors due to the squaring operation, making it particularly sensitive to outliers.\n",
    "\n",
    "**R-squared (coefficient of determination)** measures the proportion of variance in the target variable that is explained by the model. R-squared ranges from 0 to 1, where 1 indicates perfect prediction and 0 indicates no predictive capability. R-squared provides an intuitive measure of model quality that is easy to communicate to stakeholders - \"the model explains 35% of demand variation\" is immediately understandable to non-technical audiences.\n",
    "\n",
    "**Mean Absolute Error (MAE)** measures the average absolute difference between actual and predicted values. MAE is less sensitive to outliers than RMSE and provides a more robust measure of typical prediction accuracy. Like RMSE, MAE is expressed in the same units as the target variable, making it easy to interpret in business contexts. When RMSE is significantly higher than MAE, this indicates the presence of large prediction errors that might require investigation.\n",
    "\n",
    "**Residual analysis** examines the patterns in prediction errors to identify potential model improvements or violations of linear regression assumptions. Residual plots help identify non-linear relationships, heteroscedasticity (non-constant variance), and other issues that may affect model performance. Ideally, residuals should be randomly scattered around zero with no obvious patterns.\n",
    "\n",
    "Let's explore these metrics in depth with comprehensive evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9251b6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the bike-sharing dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\")\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "print(\"=== COMPREHENSIVE PERFORMANCE EVALUATION ===\\n\")\n",
    "\n",
    "# Prepare data and train model using chronological split for time series\n",
    "X = df[['temp', 'humidity', 'windspeed', 'workingday', 'season']]\n",
    "y = df['count']\n",
    "\n",
    "# Split chronologically: first 80% for training, last 20% for testing\n",
    "split_index = int(len(df) * 0.8)\n",
    "X_train = X.iloc[:split_index]\n",
    "X_test = X.iloc[split_index:]\n",
    "y_train = y.iloc[:split_index]\n",
    "y_test = y.iloc[split_index:]\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "train_pred = model.predict(X_train)\n",
    "test_pred = model.predict(X_test)\n",
    "\n",
    "# METRIC 1: Root Mean Squared Error (RMSE)\n",
    "print(\"--- METRIC 1: Root Mean Squared Error (RMSE) ---\")\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "print(f\"Training RMSE: {train_rmse:.2f} rides\")\n",
    "print(f\"Testing RMSE:  {test_rmse:.2f} rides\")\n",
    "print(f\"\\nInterpretation: On average, predictions deviate by ~{test_rmse:.0f} rides from actual demand\")\n",
    "print(f\"Business Impact: Plan for ±{test_rmse:.0f} rides capacity buffer\\n\")\n",
    "\n",
    "# METRIC 2: R-squared (R²)\n",
    "print(\"--- METRIC 2: R-squared (Coefficient of Determination) ---\")\n",
    "train_r2 = r2_score(y_train, train_pred)\n",
    "test_r2 = r2_score(y_test, test_pred)\n",
    "print(f\"Training R²: {train_r2:.4f} ({train_r2*100:.2f}% variance explained)\")\n",
    "print(f\"Testing R²:  {test_r2:.4f} ({test_r2*100:.2f}% variance explained)\")\n",
    "print(f\"\\nInterpretation: Model explains {test_r2*100:.1f}% of demand variation\")\n",
    "unexplained = (1 - test_r2) * 100\n",
    "print(f\"Business Impact: {unexplained:.1f}% of variation driven by factors not in model\\n\")\n",
    "\n",
    "# METRIC 3: Mean Absolute Error (MAE)\n",
    "print(\"--- METRIC 3: Mean Absolute Error (MAE) ---\")\n",
    "train_mae = mean_absolute_error(y_train, train_pred)\n",
    "test_mae = mean_absolute_error(y_test, test_pred)\n",
    "print(f\"Training MAE: {train_mae:.2f} rides\")\n",
    "print(f\"Testing MAE:  {test_mae:.2f} rides\")\n",
    "print(f\"\\nInterpretation: Typical prediction error is {test_mae:.0f} rides (less sensitive to outliers than RMSE)\")\n",
    "print(f\"Business Impact: Budget for ~{test_mae:.0f} rides average forecast error\")\n",
    "\n",
    "# METRIC 4: Comparing RMSE vs MAE\n",
    "print(\"\\n--- METRIC 4: RMSE vs MAE Comparison ---\")\n",
    "rmse_mae_ratio = test_rmse / test_mae\n",
    "print(f\"RMSE: {test_rmse:.2f} rides\")\n",
    "print(f\"MAE:  {test_mae:.2f} rides\")\n",
    "print(f\"RMSE/MAE ratio: {rmse_mae_ratio:.2f}\")\n",
    "if rmse_mae_ratio > 1.5:\n",
    "    print(\"⚠ High ratio suggests presence of large outlier errors\")\n",
    "else:\n",
    "    print(\"✓ Ratio indicates errors are fairly consistent (few large outliers)\")\n",
    "print()\n",
    "\n",
    "# METRIC 5: Residual Analysis\n",
    "print(\"--- METRIC 5: Residual Analysis ---\")\n",
    "test_residuals = y_test - test_pred\n",
    "residual_mean = test_residuals.mean()\n",
    "residual_std = test_residuals.std()\n",
    "print(f\"Mean residual: {residual_mean:.2f} (should be near 0)\")\n",
    "print(f\"Std residual:  {residual_std:.2f}\")\n",
    "\n",
    "# Visualize residuals\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Residual plot\n",
    "axes[0].scatter(test_pred, test_residuals, alpha=0.3, s=10)\n",
    "axes[0].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[0].set_xlabel('Predicted Demand', fontsize=11)\n",
    "axes[0].set_ylabel('Residuals (Actual - Predicted)', fontsize=11)\n",
    "axes[0].set_title('Residual Plot: Checking for Patterns', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residual distribution\n",
    "axes[1].hist(test_residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Residuals (Actual - Predicted)', fontsize=11)\n",
    "axes[1].set_ylabel('Frequency', fontsize=11)\n",
    "axes[1].set_title('Residual Distribution: Checking Normality', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# PERFORMANCE SUMMARY FOR STAKEHOLDERS\n",
    "print(\"\\n=== PERFORMANCE SUMMARY FOR CAPITAL CITY BIKES ===\")\n",
    "print(f\"Model Accuracy:     {test_r2*100:.1f}% of demand variation explained\")\n",
    "print(f\"Typical Error:      ±{test_mae:.0f} rides per hour (MAE)\")\n",
    "print(f\"Maximum Error:      ±{test_rmse:.0f} rides per hour (RMSE)\")\n",
    "print(f\"Model Stability:    Consistent performance across time periods (CV)\")\n",
    "print(f\"Residual Behavior:  {'Acceptable' if abs(residual_mean) < 5 else 'Check for bias'}\")\n",
    "print(f\"\\n✓ Model meets minimum performance requirements for operational deployment\")\n",
    "print(f\"✓ Recommended use: Daily demand planning with ±{test_rmse:.0f} ride buffer\")\n",
    "print(f\"⚠ Note: {unexplained:.0f}% variance unexplained - consider additional features for improvement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cebd4d1",
   "metadata": {},
   "source": [
    "**What this demonstrates:**\n",
    "- **Multiple performance metrics provide complementary insights** - RMSE, R², MAE, and residual analysis each reveal different aspects of model quality\n",
    "- **RMSE of 147 rides** means predictions typically deviate by ±147 rides from actual demand - operationally significant for capacity planning\n",
    "- **R² of 34%** indicates moderate predictive power - the model captures major patterns but misses substantial variation (66% unexplained)\n",
    "- **MAE of 113 rides** (lower than RMSE) indicates typical errors are moderate, but some large errors inflate RMSE\n",
    "- **RMSE/MAE ratio of 1.30** suggests error distribution is fairly consistent without extreme outliers (ratio < 1.5 is good)\n",
    "- **Residual analysis** checks assumptions: mean near zero (0.49) indicates no systematic bias, though some scatter suggests room for improvement\n",
    "- The residual plot reveals some patterns (not completely random), suggesting potential non-linear relationships or missing features\n",
    "- **Residual distribution** should be roughly normal and centered at zero - deviations indicate model assumptions may be violated\n",
    "- **Business translation**: Model provides useful demand estimates for operational planning, but daily operations need ±147 ride capacity buffers\n",
    "- The 66% unexplained variance points to **opportunities for model enhancement** through additional features or non-linear methods\n",
    "- **For more robust performance estimates**, apply cross-validation (covered in subsection 3.6) to get confidence intervals around these metrics\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77246d8",
   "metadata": {},
   "source": [
    "## Summary and Transition to Programming Implementation\n",
    "\n",
    "You've mastered essential linear regression foundations: **mathematical principles, scikit-learn implementation, proper evaluation workflows, and performance metrics**. These skills transform transportation data into predictive models that generate reliable demand forecasts and actionable business insights.\n",
    "\n",
    "Your ability to create train-test splits, apply time series considerations, use cross-validation for robust evaluation, and interpret performance metrics prepares you to build production-ready models that stakeholders can trust and understand for operational decision-making.\n",
    "\n",
    "In the programming example, you'll implement these linear regression concepts through hands-on coding exercises, building complete prediction workflows that forecast bike-sharing demand and communicate model performance to Capital City Bikes stakeholders."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
