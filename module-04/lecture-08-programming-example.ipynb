{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c6bf21c",
   "metadata": {},
   "source": [
    "# Lecture 8: Programming Example - Linear Models for Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed6f255",
   "metadata": {},
   "source": [
    "## Introduction: Building Your First Predictive Model for Transportation Demand\n",
    "\n",
    "Welcome back, junior data consultant! Your Capital Bikes Washington D.C. client has been impressed with your statistical analysis and visualization work. Now they're ready for the defining moment: building a predictive model that forecasts hourly bike demand. The CEO has just approved your recommendation to develop a demand forecasting system that will transform their operations from reactive to predictive.\n",
    "\n",
    "Think of predictive modeling as the ultimate consulting deliverable. While statistics reveal patterns and visualizations communicate insights, predictive models operationalize intelligence - they generate actionable forecasts that drive real-time business decisions. You'll master linear regression, the foundational algorithm that quantifies relationships between variables and produces interpretable predictions executives can trust and act upon.\n",
    "\n",
    "Your task: build a linear regression model that predicts hourly bike demand using weather conditions and temporal patterns. You'll learn to calculate correlation coefficients that measure relationship strength, engineer time-based features that capture demand cycles, implement scikit-learn models with proper train-test splits, and apply cross-validation for robust performance estimates. Every technique ensures your forecasts are accurate, reliable, and operationally actionable.\n",
    "\n",
    "> **ðŸš€ Interactive Learning Alert**\n",
    ">\n",
    "> This is a hands-on predictive modeling tutorial with real forecasting challenges. For the best experience:\n",
    ">\n",
    "> - **Click \"Open in Colab\"** at the bottom to run code interactively\n",
    "> - **Execute each code cell** by pressing **Shift + Enter**\n",
    "> - **Complete the challenges** to practice your predictive modeling skills\n",
    "> - **Think like a consultant** - model accuracy directly impacts operational planning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8623427",
   "metadata": {},
   "source": [
    "## Step 1: Calculate Correlations and Visualize Relationships\n",
    "\n",
    "Let's analyze which variables have linear relationships with bike demand by calculating correlations and creating visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df56b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\")\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "# Calculate correlation and regression for temperature vs demand (using raw data)\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(df['temp'], df['count'])\n",
    "\n",
    "# Chart 1: Raw scatter plot (all individual observations)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['temp'], df['count'], alpha=0.3, s=10, label='Raw Data Points')\n",
    "\n",
    "# Add regression line (calculated from raw data)\n",
    "temp_range = np.array([df['temp'].min(), df['temp'].max()])\n",
    "demand_trend = slope * temp_range + intercept\n",
    "plt.plot(temp_range, demand_trend, 'r-', linewidth=3, label=f'Regression Line (r={r_value:.3f})')\n",
    "\n",
    "plt.xlabel('Temperature (Â°C)', fontsize=12)\n",
    "plt.ylabel('Hourly Bike Rentals', fontsize=12)\n",
    "plt.title('Raw Data: Temperature vs Demand (All Observations)', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=== VISUALIZATION NOTE ===\")\n",
    "print(\"The scatter plot above shows all 10,886 hourly observations.\")\n",
    "print(\"While this represents the complete data, the density makes it hard to see\")\n",
    "print(\"the underlying pattern clearly. Let's create a cleaner visualization...\\n\")\n",
    "\n",
    "# Chart 2: Binned visualization (for pedagogical clarity)\n",
    "# Create temperature bins and calculate mean demand for each bin\n",
    "df['temp_bin'] = pd.cut(df['temp'], bins=20)\n",
    "binned = df.groupby('temp_bin', observed=True)['count'].agg(['mean', 'std', 'count'])\n",
    "binned['se'] = binned['std'] / np.sqrt(binned['count'])\n",
    "binned['temp_center'] = binned.index.map(lambda x: x.mid)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(binned['temp_center'], binned['mean'],\n",
    "            yerr=binned['se']*1.96,  # 95% confidence interval\n",
    "            fmt='o', markersize=8, capsize=5, capthick=2,\n",
    "            color='#2ECC71', ecolor='#2ECC71', alpha=0.7,\n",
    "            label='Binned Mean Demand (95% CI)')\n",
    "\n",
    "# Add the SAME regression line (still from raw data)\n",
    "plt.plot(temp_range, demand_trend, 'r-', linewidth=3, label=f'Regression Line (r={r_value:.3f})')\n",
    "\n",
    "plt.xlabel('Temperature (Â°C)', fontsize=12)\n",
    "plt.ylabel('Hourly Bike Rentals', fontsize=12)\n",
    "plt.title('Clean Visualization: Temperature vs Demand (Binned Averages)', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print correlation statistics\n",
    "print(\"=== CORRELATION ANALYSIS ===\")\n",
    "print(f\"Correlation (r): {r_value:.3f}\")\n",
    "print(f\"R-squared: {r_value**2:.3f} ({(r_value**2)*100:.1f}% variance explained)\")\n",
    "print(f\"Slope: {slope:.2f} bikes per Â°C\")\n",
    "\n",
    "print(\"\\n=== IMPORTANT CLARIFICATION ===\")\n",
    "print(\"Both charts show the SAME regression line calculated from the raw data.\")\n",
    "print(\"The binned visualization groups observations into temperature ranges and shows\")\n",
    "print(\"their average demand - this is purely for pedagogical purposes to help you see\")\n",
    "print(\"the pattern more clearly. However, the regression analysis always uses the\")\n",
    "print(\"complete raw data (all 10,886 observations), not the binned averages.\")\n",
    "print(\"This is the correct approach: bin for visualization clarity, but analyze raw data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f556e04",
   "metadata": {},
   "source": [
    "**What this does:**\n",
    "\n",
    "- Loads data and calculates correlation between temperature and demand using raw data\n",
    "- Shows two visualizations: raw scatter (complete but messy) and binned averages (clean and clear)\n",
    "- Both charts display the same regression line calculated from the raw data\n",
    "- Clarifies that binning is a visualization technique, not part of the statistical analysis\n",
    "- Prints correlation statistics (r â‰ˆ 0.39 shows moderate positive relationship)\n",
    "- RÂ² of ~15% means temperature alone explains only 15% of demand variation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812250f6",
   "metadata": {},
   "source": [
    "### Challenge 1: Explore Humidity-Demand Relationship\n",
    "\n",
    "Your client asks: \"Does humidity affect bike demand like temperature does?\" Investigate the humidity-demand relationship and compare it to the temperature relationship.\n",
    "\n",
    "**Your Task:** Calculate correlation statistics and create a clean binned visualization for humidity vs demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a63849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - analyze humidity-demand relationship\n",
    "\n",
    "# Step 1: Calculate regression statistics for humidity (using raw data)\n",
    "slope_hum, intercept_hum, r_value_hum, _, _ = stats.linregress(df['_____'], df['_____'])\n",
    "\n",
    "# Step 2: Create binned visualization for clarity\n",
    "df['humidity_bin'] = pd.cut(df['humidity'], bins=20)\n",
    "humidity_binned = df.groupby('humidity_bin', observed=True)['count'].agg(['mean', 'std', 'count'])\n",
    "humidity_binned['se'] = humidity_binned['std'] / np.sqrt(humidity_binned['count'])\n",
    "humidity_binned['humidity_center'] = humidity_binned.index.map(lambda x: x.mid)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(humidity_binned['_____'], humidity_binned['_____'],\n",
    "            yerr=humidity_binned['se']*1.96,\n",
    "            fmt='o', markersize=8, capsize=5, capthick=2,\n",
    "            color='#E74C3C', ecolor='#E74C3C', alpha=0.7,\n",
    "            label='Binned Mean Demand (95% CI)')\n",
    "\n",
    "# Add regression line (from raw data)\n",
    "humidity_range = np.array([df['humidity'].min(), df['humidity'].max()])\n",
    "demand_trend_hum = _____ * humidity_range + _____\n",
    "plt.plot(humidity_range, demand_trend_hum, 'b-', linewidth=3, label=f'Regression Line (r={r_value_hum:.3f})')\n",
    "\n",
    "plt.xlabel('Humidity (%)', fontsize=12)\n",
    "plt.ylabel('Hourly Bike Rentals', fontsize=12)\n",
    "plt.title('Humidity-Demand Relationship Analysis', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Step 3: Compare correlations\n",
    "print(\"=== CORRELATION COMPARISON ===\")\n",
    "print(f\"Temperature correlation: r = {r_value:.3f} (explains {(r_value**2)*100:.1f}%)\")\n",
    "print(f\"Humidity correlation: r = {r_value_hum:.3f} (explains {(r_value_hum**2)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20b4ac8",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ <strong>Tip</strong> (click to expand)</summary>\n",
    "\n",
    "\n",
    "Calculate regression using raw data: `stats.linregress(df['humidity'], df['count'])` extracts all 5 return values (use underscore _ for unused values like `slope_hum, intercept_hum, r_value_hum, _, _ = ...`). For the binned visualization, use `humidity_binned['humidity_center']` and `humidity_binned['mean']` in the errorbar plot. The regression line uses slope_hum and intercept_hum: demand = slope_hum Ã— humidity + intercept_hum. Remember: regression is calculated from raw data, binning is just for visualization clarity. Humidity typically shows a negative correlation (higher humidity = uncomfortable = fewer riders), so compare the RÂ² values to determine which weather factor matters more. Negative correlation means humid days reduce demand (discomfort effect), and this insight helps Capital City Bikes plan for different weather conditions.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5c2fa7",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ¤« <strong>Solution</strong> (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "# Step 1: Calculate regression statistics for humidity (using raw data)\n",
    "slope_hum, intercept_hum, r_value_hum, _, _ = stats.linregress(df['humidity'], df['count'])\n",
    "\n",
    "# Step 2: Create binned visualization for clarity\n",
    "df['humidity_bin'] = pd.cut(df['humidity'], bins=20)\n",
    "humidity_binned = df.groupby('humidity_bin', observed=True)['count'].agg(['mean', 'std', 'count'])\n",
    "humidity_binned['se'] = humidity_binned['std'] / np.sqrt(humidity_binned['count'])\n",
    "humidity_binned['humidity_center'] = humidity_binned.index.map(lambda x: x.mid)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(humidity_binned['humidity_center'], humidity_binned['mean'],\n",
    "            yerr=humidity_binned['se']*1.96,\n",
    "            fmt='o', markersize=8, capsize=5, capthick=2,\n",
    "            color='#E74C3C', ecolor='#E74C3C', alpha=0.7,\n",
    "            label='Binned Mean Demand (95% CI)')\n",
    "\n",
    "# Add regression line (from raw data)\n",
    "humidity_range = np.array([df['humidity'].min(), df['humidity'].max()])\n",
    "demand_trend_hum = slope_hum * humidity_range + intercept_hum\n",
    "plt.plot(humidity_range, demand_trend_hum, 'b-', linewidth=3, label=f'Regression Line (r={r_value_hum:.3f})')\n",
    "\n",
    "plt.xlabel('Humidity (%)', fontsize=12)\n",
    "plt.ylabel('Hourly Bike Rentals', fontsize=12)\n",
    "plt.title('Humidity-Demand Relationship Analysis', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Step 3: Compare correlations\n",
    "print(\"=== CORRELATION COMPARISON ===\")\n",
    "print(f\"Temperature correlation: r = {r_value:.3f} (explains {(r_value**2)*100:.1f}%)\")\n",
    "print(f\"Humidity correlation: r = {r_value_hum:.3f} (explains {(r_value_hum**2)*100:.1f}%)\")\n",
    "print(f\"\\nInsight: {'Temperature' if abs(r_value) > abs(r_value_hum) else 'Humidity'} shows stronger correlation with demand\")\n",
    "print(f\"Remember: Regression calculated from raw data, binning used only for clear visualization\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d212150",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def19bc3",
   "metadata": {},
   "source": [
    "## Step 2: Engineer Time-Based Features\n",
    "\n",
    "Weather alone can't capture temporal patterns like rush hours and daily cycles. Let's create time-based features that capture these patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474887f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and load data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\")\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "# Sort by datetime for proper temporal ordering\n",
    "df = df.sort_values('datetime').reset_index(drop=True)\n",
    "\n",
    "# Extract hour and create cyclical encoding\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "\n",
    "# Create lag features (demand from previous hours)\n",
    "df['demand_lag_1h'] = df['count'].shift(1)\n",
    "df['demand_lag_24h'] = df['count'].shift(24)\n",
    "\n",
    "# Extract day of week and month\n",
    "df['dayofweek'] = df['datetime'].dt.dayofweek\n",
    "df['month'] = df['datetime'].dt.month\n",
    "\n",
    "# Remove rows with NaN values from lag features\n",
    "df_clean = df.dropna(subset=['demand_lag_1h', 'demand_lag_24h'])\n",
    "\n",
    "# Visualize hourly pattern\n",
    "hourly_avg = df_clean.groupby('hour')['count'].mean()\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(hourly_avg.index, hourly_avg.values, marker='o', linewidth=2)\n",
    "plt.xlabel('Hour of Day', fontsize=12)\n",
    "plt.ylabel('Average Bike Rentals', fontsize=12)\n",
    "plt.title('Daily Demand Pattern Shows Rush Hours', fontsize=14, fontweight='bold')\n",
    "plt.xticks(range(0, 24, 2))\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=== ENGINEERED FEATURES ===\")\n",
    "print(f\"Total features created: hour, hour_sin, hour_cos, lag_1h, lag_24h, dayofweek, month\")\n",
    "print(f\"Dataset size after cleaning: {len(df_clean):,} observations\")\n",
    "print(f\"Peak hours: {hourly_avg.nlargest(3).index.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88cd68d",
   "metadata": {},
   "source": [
    "**What this does:**\n",
    "- Extracts hour from datetime and creates cyclical sin/cos encoding\n",
    "- Creates lag features (demand from 1 hour and 24 hours ago)\n",
    "- Extracts day of week and month for weekly/seasonal patterns\n",
    "- Removes rows with NaN values from lag features\n",
    "- Visualizes daily demand pattern showing morning and evening peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5051dad9",
   "metadata": {},
   "source": [
    "### Challenge 2: Visualize Weekly Demand Patterns\n",
    "\n",
    "Your client asks: \"Do we see different demand patterns on different days of the week?\" Analyze and visualize weekly patterns.\n",
    "\n",
    "**Your Task:** Calculate and visualize average demand by day of week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f6a4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - analyze weekly patterns\n",
    "\n",
    "# Calculate average demand by day of week\n",
    "day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "weekly_pattern = df_clean.groupby(_____)['_____']._____()\n",
    "\n",
    "# Visualize weekly pattern\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(_____, weekly_pattern.values, color='teal', edgecolor='black')\n",
    "plt.xlabel('Day of Week', fontsize=12)\n",
    "plt.ylabel('Average Hourly Demand', fontsize=12)\n",
    "plt.title('Weekly Demand Pattern', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print insights\n",
    "print(\"=== WEEKLY DEMAND PATTERN ===\")\n",
    "for i, day in enumerate(day_names):\n",
    "    print(f\"{day}: {weekly_pattern.iloc[i]:.1f} bikes per hour\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf697c3",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ <strong>Tip</strong> (click to expand)</summary>\n",
    "\n",
    "\n",
    "The dayofweek feature was already created in Step 2 (0=Monday through 6=Sunday). Use `.groupby('dayofweek')['count'].mean()` to calculate average demand for each day. Pass day_names to plt.bar() along with the values to get readable labels. Weekdays show relatively stable commuting patterns, Saturday typically peaks with leisure riding, while Sunday often drops (rest/preparation day).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b62a17",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ¤« <strong>Solution</strong> (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "# Calculate average demand by day of week\n",
    "day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "weekly_pattern = df_clean.groupby('dayofweek')['count'].mean()\n",
    "\n",
    "# Visualize weekly pattern\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(day_names, weekly_pattern.values, color='teal', edgecolor='black')\n",
    "plt.xlabel('Day of Week', fontsize=12)\n",
    "plt.ylabel('Average Hourly Demand', fontsize=12)\n",
    "plt.title('Weekly Demand Pattern', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print insights\n",
    "print(\"=== WEEKLY DEMAND PATTERN ===\")\n",
    "for i, day in enumerate(day_names):\n",
    "    print(f\"{day}: {weekly_pattern.iloc[i]:.1f} bikes per hour\")\n",
    "print(f\"\\nBest day: {day_names[weekly_pattern.idxmax()]} ({weekly_pattern.max():.1f} bikes)\")\n",
    "print(f\"Weakest day: {day_names[weekly_pattern.idxmin()]} ({weekly_pattern.min():.1f} bikes)\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa92e38",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfe8fcf",
   "metadata": {},
   "source": [
    "## Step 3: Train a Linear Regression Model\n",
    "\n",
    "Let's train a linear regression model using scikit-learn to predict bike demand from our engineered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed04dd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and load/prepare data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Load and prepare data (same as Step 2)\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\")\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df = df.sort_values('datetime').reset_index(drop=True)\n",
    "\n",
    "# Create features\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "df['demand_lag_1h'] = df['count'].shift(1)\n",
    "df['demand_lag_24h'] = df['count'].shift(24)\n",
    "df['dayofweek'] = df['datetime'].dt.dayofweek\n",
    "df['month'] = df['datetime'].dt.month\n",
    "df_clean = df.dropna(subset=['demand_lag_1h', 'demand_lag_24h'])\n",
    "\n",
    "# Prepare feature matrix and target\n",
    "feature_columns = ['temp', 'humidity', 'windspeed', 'hour_sin', 'hour_cos',\n",
    "                   'demand_lag_1h', 'demand_lag_24h', 'dayofweek', 'month']\n",
    "X = df_clean[feature_columns]\n",
    "y = df_clean['count']\n",
    "\n",
    "# Create and train model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "y_pred = model.predict(X)\n",
    "r2 = r2_score(y, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "mae = mean_absolute_error(y, y_pred)\n",
    "\n",
    "print(\"=== MODEL TRAINING COMPLETE ===\")\n",
    "print(f\"Features used: {len(feature_columns)}\")\n",
    "print(f\"\\nModel Coefficients:\")\n",
    "for feature, coef in zip(feature_columns, model.coef_):\n",
    "    print(f\"  {feature:20s}: {coef:+8.2f}\")\n",
    "\n",
    "print(f\"\\n=== PERFORMANCE (all data) ===\")\n",
    "print(f\"RÂ²:   {r2:.4f} ({r2*100:.1f}% variance explained)\")\n",
    "print(f\"RMSE: {rmse:.2f} bikes per hour\")\n",
    "print(f\"MAE:  {mae:.2f} bikes per hour\")\n",
    "\n",
    "print(\"\\nNote: These metrics are optimistic - we need train-test split!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220956dc",
   "metadata": {},
   "source": [
    "**What this does:**\n",
    "- Prepares feature matrix X (9 features) and target y (bike demand)\n",
    "- Trains LinearRegression model with .fit(X, y)\n",
    "- Makes predictions with .predict(X) and calculates performance metrics\n",
    "- Shows coefficients revealing which features increase/decrease demand\n",
    "- RÂ² of ~81% means model explains 81% of demand variation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e6bdac",
   "metadata": {},
   "source": [
    "### Challenge 3: Train Model with Different Feature Combinations\n",
    "\n",
    "Your client asks: \"Which features matter most? Can we get similar performance with fewer features?\" Compare models using different feature sets.\n",
    "\n",
    "**Your Task:** Train three models with different feature combinations and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40fddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - compare models with different feature sets\n",
    "\n",
    "# Model 1: Weather-only features\n",
    "weather_features = ['temp', 'humidity', 'windspeed']\n",
    "X_weather = df_clean[_____]\n",
    "model_weather = LinearRegression()\n",
    "model_weather.fit(X_weather, y)\n",
    "y_pred_weather = model_weather.predict(X_weather)\n",
    "r2_weather = r2_score(y, y_pred_weather)\n",
    "\n",
    "# Model 2: Time-only features\n",
    "time_features = ['hour_sin', 'hour_cos', 'dayofweek', 'month']\n",
    "X_time = df_clean[_____]\n",
    "model_time = LinearRegression()\n",
    "model_time.fit(X_time, y)\n",
    "y_pred_time = model_time.predict(X_time)\n",
    "r2_time = r2_score(y, y_pred_time)\n",
    "\n",
    "# Model 3: Complete model (weather + time + lags)\n",
    "complete_features = feature_columns  # Already defined above\n",
    "X_complete = df_clean[complete_features]\n",
    "model_complete = LinearRegression()\n",
    "model_complete.fit(X_complete, y)\n",
    "y_pred_complete = model_complete.predict(X_complete)\n",
    "r2_complete = r2_score(y, y_pred_complete)\n",
    "\n",
    "# Compare results\n",
    "print(\"=== MODEL COMPARISON ===\")\n",
    "print(f\"Weather-only model:  RÂ² = {r2_weather:.4f} ({_____} features)\")\n",
    "print(f\"Time-only model:     RÂ² = {r2_time:.4f} ({_____} features)\")\n",
    "print(f\"Complete model:      RÂ² = {r2_complete:.4f} ({_____} features)\")\n",
    "print(f\"\\nBest model: {'Weather' if r2_weather > max(r2_time, r2_complete) else 'Time' if r2_time > r2_complete else 'Complete'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7381df60",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ <strong>Tip</strong> (click to expand)</summary>\n",
    "\n",
    "\n",
    "Follow the same pattern for each model: create X subset using `df_clean[feature_list]` â†’ fit model with `LinearRegression().fit(X, y)` â†’ predict with `model.predict(X)` â†’ calculate RÂ² with `r2_score(y_true, y_pred)`. Weather features (temp, humidity, windspeed) capture environmental conditions, while time features (hour_sin, hour_cos, dayofweek, month) capture temporal patterns, and the complete model uses all 9 features including lag variables. Don't forget to fit each model separately (don't reuse the previous model object), and use `len(feature_list)` to count features correctly. The weather-only model is useful when you have weather forecasts but no historical demand, the time-only model works for seasonal planning independent of weather, and the complete model delivers best performance by combining all signals - this comparison shows which features contribute most value to your client's forecasting accuracy.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66108c7b",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ¤« <strong>Solution</strong> (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "# Model 1: Weather-only features\n",
    "weather_features = ['temp', 'humidity', 'windspeed']\n",
    "X_weather = df_clean[weather_features]\n",
    "model_weather = LinearRegression()\n",
    "model_weather.fit(X_weather, y)\n",
    "y_pred_weather = model_weather.predict(X_weather)\n",
    "r2_weather = r2_score(y, y_pred_weather)\n",
    "\n",
    "# Model 2: Time-only features\n",
    "time_features = ['hour_sin', 'hour_cos', 'dayofweek', 'month']\n",
    "X_time = df_clean[time_features]\n",
    "model_time = LinearRegression()\n",
    "model_time.fit(X_time, y)\n",
    "y_pred_time = model_time.predict(X_time)\n",
    "r2_time = r2_score(y, y_pred_time)\n",
    "\n",
    "# Model 3: Complete model (weather + time + lags)\n",
    "complete_features = feature_columns\n",
    "X_complete = df_clean[complete_features]\n",
    "model_complete = LinearRegression()\n",
    "model_complete.fit(X_complete, y)\n",
    "y_pred_complete = model_complete.predict(X_complete)\n",
    "r2_complete = r2_score(y, y_pred_complete)\n",
    "\n",
    "# Compare results\n",
    "print(\"=== MODEL COMPARISON ===\")\n",
    "print(f\"Weather-only model:  RÂ² = {r2_weather:.4f} ({len(weather_features)} features)\")\n",
    "print(f\"Time-only model:     RÂ² = {r2_time:.4f} ({len(time_features)} features)\")\n",
    "print(f\"Complete model:      RÂ² = {r2_complete:.4f} ({len(complete_features)} features)\")\n",
    "\n",
    "print(f\"\\nInsights:\")\n",
    "print(f\"- Time features alone explain {r2_time*100:.1f}% of variation (temporal patterns dominate)\")\n",
    "print(f\"- Weather + lag features add {(r2_complete-r2_time)*100:.1f}% additional explanation\")\n",
    "print(f\"- Lag features are crucial: they capture sequential dependencies\")\n",
    "print(f\"\\nRecommendation: Use complete model for best forecasting accuracy\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef0fca",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ecf4e2",
   "metadata": {},
   "source": [
    "## Step 4: Create Chronological Train-Test Split\n",
    "\n",
    "Let's evaluate model performance honestly by splitting data chronologically: train on past, test on future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bad7d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and prepare data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Load and prepare data (same as previous steps)\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\")\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df = df.sort_values('datetime').reset_index(drop=True)\n",
    "\n",
    "# Create features\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "df['demand_lag_1h'] = df['count'].shift(1)\n",
    "df['demand_lag_24h'] = df['count'].shift(24)\n",
    "df['dayofweek'] = df['datetime'].dt.dayofweek\n",
    "df['month'] = df['datetime'].dt.month\n",
    "df_clean = df.dropna(subset=['demand_lag_1h', 'demand_lag_24h'])\n",
    "\n",
    "feature_columns = ['temp', 'humidity', 'windspeed', 'hour_sin', 'hour_cos',\n",
    "                   'demand_lag_1h', 'demand_lag_24h', 'dayofweek', 'month']\n",
    "X = df_clean[feature_columns]\n",
    "y = df_clean['count']\n",
    "\n",
    "# Create chronological split: 80% train, 20% test\n",
    "split_index = int(len(df_clean) * 0.8)\n",
    "X_train = X.iloc[:split_index]\n",
    "X_test = X.iloc[split_index:]\n",
    "y_train = y.iloc[:split_index]\n",
    "y_test = y.iloc[split_index:]\n",
    "\n",
    "# Train model on training data only\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on both training and testing sets\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "print(\"=== CHRONOLOGICAL TRAIN-TEST SPLIT ===\")\n",
    "print(f\"Training set: {len(X_train)} obs ({len(X_train)/len(df_clean)*100:.1f}%)\")\n",
    "print(f\"Testing set:  {len(X_test)} obs ({len(X_test)/len(df_clean)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n=== PERFORMANCE COMPARISON ===\")\n",
    "print(f\"Training:  RÂ² = {train_r2:.4f}, RMSE = {train_rmse:.2f} bikes\")\n",
    "print(f\"Testing:   RÂ² = {test_r2:.4f}, RMSE = {test_rmse:.2f} bikes\")\n",
    "print(f\"Gap:       {train_r2 - test_r2:.4f}\")\n",
    "\n",
    "if (train_r2 - test_r2) < 0.05:\n",
    "    print(\"\\nâœ“ Small gap - model generalizes well to future data\")\n",
    "else:\n",
    "    print(\"\\nâš  Moderate gap - some overfitting present\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f499d89",
   "metadata": {},
   "source": [
    "**What this does:**\n",
    "- Splits data chronologically using .iloc[] (first 80% = train, last 20% = test)\n",
    "- Trains model only on historical data (training set)\n",
    "- Evaluates on both sets to compare training vs testing performance\n",
    "- Performance gap shows how well model generalizes to unseen future data\n",
    "- Test RÂ² and RMSE reveal realistic forecasting accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad320819",
   "metadata": {},
   "source": [
    "### Challenge 4: Experiment with Different Split Ratios\n",
    "\n",
    "Your client asks: \"What if we used more data for training? Would that improve test performance?\" Compare 70/30, 80/20, and 90/10 splits.\n",
    "\n",
    "**Your Task:** Train models with three different split ratios and compare their test performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c94e63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - compare different split ratios\n",
    "\n",
    "split_ratios = [0.7, 0.8, 0.9]\n",
    "results = []\n",
    "\n",
    "for ratio in split_ratios:\n",
    "    # Calculate split index for this ratio\n",
    "    split_idx = int(len(df_clean) * _____)\n",
    "\n",
    "    # Split data chronologically\n",
    "    X_train_temp = X.iloc[:_____]\n",
    "    X_test_temp = X.iloc[_____:]\n",
    "    y_train_temp = y.iloc[:_____]\n",
    "    y_test_temp = y.iloc[_____:]\n",
    "\n",
    "    # Train model\n",
    "    model_temp = LinearRegression()\n",
    "    model_temp.fit(X_train_temp, y_train_temp)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    y_test_pred_temp = model_temp.predict(X_test_temp)\n",
    "    test_r2_temp = r2_score(y_test_temp, y_test_pred_temp)\n",
    "    test_rmse_temp = np.sqrt(mean_squared_error(y_test_temp, y_test_pred_temp))\n",
    "\n",
    "    # Store results\n",
    "    results.append({\n",
    "        'ratio': f\"{int(ratio*100)}/{int((1-ratio)*100)}\",\n",
    "        'train_size': len(X_train_temp),\n",
    "        'test_size': len(X_test_temp),\n",
    "        'test_r2': test_r2_temp,\n",
    "        'test_rmse': test_rmse_temp\n",
    "    })\n",
    "\n",
    "# Display comparison\n",
    "print(\"=== SPLIT RATIO COMPARISON ===\")\n",
    "for result in results:\n",
    "    print(f\"{result['ratio']} split:\")\n",
    "    print(f\"  Training: {result['train_size']} obs  |  Testing: {result['test_size']} obs\")\n",
    "    print(f\"  Test RÂ²: {result['test_r2']:.4f}  |  Test RMSE: {result['test_rmse']:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87414fa2",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ <strong>Tip</strong> (click to expand)</summary>\n",
    "\n",
    "\n",
    "Loop through each ratio (0.7, 0.8, 0.9) and calculate `split_index = int(len(df_clean) * ratio)` for each iteration - don't reuse the same split_idx for all ratios. For each ratio, use `.iloc[:split_idx]` for training and `.iloc[split_idx:]` for testing, making sure to use chronological splitting (not random sampling). Train a fresh LinearRegression model for each split to ensure fair comparison, then store test RÂ² and RMSE in the results list. Larger training sets (90/10) provide more data for learning patterns but leave less data for reliable testing evaluation, while larger test sets (70/30) give more reliable performance estimates but less training data. The 70/30 split maximizes test data reliability, 80/20 provides standard balance, and 90/10 maximizes learning capacity - your client should understand this trade-off between learning and evaluation reliability when choosing their validation strategy.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bffda32",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ¤« <strong>Solution</strong> (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "split_ratios = [0.7, 0.8, 0.9]\n",
    "results = []\n",
    "\n",
    "for ratio in split_ratios:\n",
    "    # Calculate split index for this ratio\n",
    "    split_idx = int(len(df_clean) * ratio)\n",
    "\n",
    "    # Split data chronologically\n",
    "    X_train_temp = X.iloc[:split_idx]\n",
    "    X_test_temp = X.iloc[split_idx:]\n",
    "    y_train_temp = y.iloc[:split_idx]\n",
    "    y_test_temp = y.iloc[split_idx:]\n",
    "\n",
    "    # Train model\n",
    "    model_temp = LinearRegression()\n",
    "    model_temp.fit(X_train_temp, y_train_temp)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    y_test_pred_temp = model_temp.predict(X_test_temp)\n",
    "    test_r2_temp = r2_score(y_test_temp, y_test_pred_temp)\n",
    "    test_rmse_temp = np.sqrt(mean_squared_error(y_test_temp, y_test_pred_temp))\n",
    "\n",
    "    # Store results\n",
    "    results.append({\n",
    "        'ratio': f\"{int(ratio*100)}/{int((1-ratio)*100)}\",\n",
    "        'train_size': len(X_train_temp),\n",
    "        'test_size': len(X_test_temp),\n",
    "        'test_r2': test_r2_temp,\n",
    "        'test_rmse': test_rmse_temp\n",
    "    })\n",
    "\n",
    "# Display comparison\n",
    "print(\"=== SPLIT RATIO COMPARISON ===\")\n",
    "for result in results:\n",
    "    print(f\"{result['ratio']} split:\")\n",
    "    print(f\"  Training: {result['train_size']} obs  |  Testing: {result['test_size']} obs\")\n",
    "    print(f\"  Test RÂ²: {result['test_r2']:.4f}  |  Test RMSE: {result['test_rmse']:.2f}\")\n",
    "    print()\n",
    "\n",
    "print(\"Insight: Larger training sets provide more learning data, but smaller test sets reduce evaluation reliability.\")\n",
    "print(\"Recommendation: 80/20 split balances learning and evaluation needs for most applications.\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13a2ac4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51705c6b",
   "metadata": {},
   "source": [
    "## Step 5: Apply Cross-Validation for Robust Evaluation\n",
    "\n",
    "A single train-test split gives one performance estimate. Cross-validation provides multiple estimates for more reliable evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e6eb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and prepare data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Load and prepare data\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\")\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df = df.sort_values('datetime').reset_index(drop=True)\n",
    "\n",
    "# Create features\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "df['demand_lag_1h'] = df['count'].shift(1)\n",
    "df['demand_lag_24h'] = df['count'].shift(24)\n",
    "df['dayofweek'] = df['datetime'].dt.dayofweek\n",
    "df['month'] = df['datetime'].dt.month\n",
    "df_clean = df.dropna(subset=['demand_lag_1h', 'demand_lag_24h'])\n",
    "\n",
    "feature_columns = ['temp', 'humidity', 'windspeed', 'hour_sin', 'hour_cos',\n",
    "                   'demand_lag_1h', 'demand_lag_24h', 'dayofweek', 'month']\n",
    "X = df_clean[feature_columns]\n",
    "y = df_clean['count']\n",
    "\n",
    "# Create TimeSeriesSplit with 5 folds\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Show fold structure\n",
    "print(\"=== TIME SERIES CROSS-VALIDATION ===\")\n",
    "print(\"Fold structure (expanding window):\")\n",
    "for fold_num, (train_idx, test_idx) in enumerate(tscv.split(X), 1):\n",
    "    train_dates = df_clean.iloc[train_idx]['datetime']\n",
    "    test_dates = df_clean.iloc[test_idx]['datetime']\n",
    "    print(f\"Fold {fold_num}: Train {len(train_idx):,} obs, Test {len(test_idx):,} obs\")\n",
    "\n",
    "# Run cross-validation\n",
    "cv_scores = cross_val_score(LinearRegression(), X, y, cv=tscv, scoring='r2')\n",
    "\n",
    "# Show results\n",
    "print(f\"\\n=== CROSS-VALIDATION RESULTS ===\")\n",
    "for i, score in enumerate(cv_scores, 1):\n",
    "    print(f\"Fold {i}: RÂ² = {score:.4f}\")\n",
    "\n",
    "cv_mean = cv_scores.mean()\n",
    "cv_std = cv_scores.std()\n",
    "\n",
    "print(f\"\\nMean RÂ²: {cv_mean:.4f} (Â±{cv_std:.4f})\")\n",
    "print(f\"Range:   {cv_scores.min():.4f} to {cv_scores.max():.4f}\")\n",
    "\n",
    "if cv_std < 0.03:\n",
    "    print(\"\\nâœ“ Low variability - consistent performance across time\")\n",
    "elif cv_std < 0.06:\n",
    "    print(\"\\nâš  Moderate variability - performance varies somewhat\")\n",
    "else:\n",
    "    print(\"\\nâœ— High variability - unstable across time periods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9448d6",
   "metadata": {},
   "source": [
    "**What this does:**\n",
    "- Creates TimeSeriesSplit with 5 folds (expanding window approach)\n",
    "- Each fold trains on past data and tests on next time period\n",
    "- cross_val_score() automates training and evaluation across all folds\n",
    "- Mean RÂ² provides robust performance estimate\n",
    "- Standard deviation shows performance consistency across time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfc720c",
   "metadata": {},
   "source": [
    "### Challenge 5: Analyze Performance Variability Across Folds\n",
    "\n",
    "Your client asks: \"Which time periods are hardest to predict? Why does performance vary?\" Investigate which folds show best and worst performance.\n",
    "\n",
    "**Your Task:** Calculate detailed metrics for each fold and analyze what makes some periods easier to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eef3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - detailed fold analysis\n",
    "\n",
    "fold_results = []\n",
    "\n",
    "for fold_num, (train_idx, test_idx) in enumerate(tscv.split(X), 1):\n",
    "    # Split data for this fold\n",
    "    X_train_fold = X.iloc[_____]\n",
    "    X_test_fold = X.iloc[_____]\n",
    "    y_train_fold = y.iloc[_____]\n",
    "    y_test_fold = y.iloc[_____]\n",
    "\n",
    "    # Train model on this fold\n",
    "    model_fold = LinearRegression()\n",
    "    model_fold.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    y_pred_fold = model_fold.predict(X_test_fold)\n",
    "    r2_fold = r2_score(y_test_fold, y_pred_fold)\n",
    "    rmse_fold = np.sqrt(mean_squared_error(y_test_fold, y_pred_fold))\n",
    "\n",
    "    # Get test period dates and characteristics\n",
    "    test_dates_fold = df_clean.iloc[test_idx]['datetime']\n",
    "    avg_temp = df_clean.iloc[test_idx]['temp'].mean()\n",
    "    avg_demand = df_clean.iloc[test_idx]['count'].mean()\n",
    "\n",
    "    # Store results\n",
    "    fold_results.append({\n",
    "        'fold': fold_num,\n",
    "        'test_start': test_dates_fold.min().date(),\n",
    "        'test_end': test_dates_fold.max().date(),\n",
    "        'r2': r2_fold,\n",
    "        'rmse': rmse_fold,\n",
    "        'avg_temp': avg_temp,\n",
    "        'avg_demand': avg_demand\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "print(\"=== FOLD-BY-FOLD ANALYSIS ===\")\n",
    "for result in fold_results:\n",
    "    print(f\"\\nFold {result['fold']}: {result['test_start']} to {result['test_end']}\")\n",
    "    print(f\"  RÂ²: {result['r2']:.4f}  |  RMSE: {result['rmse']:.2f}\")\n",
    "    print(f\"  Avg temp: {result['avg_temp']:.1f}Â°C  |  Avg demand: {result['avg_demand']:.0f} bikes\")\n",
    "\n",
    "# Identify best and worst folds\n",
    "best_fold = max(fold_results, key=lambda x: x['r2'])\n",
    "worst_fold = min(fold_results, key=lambda x: x['r2'])\n",
    "\n",
    "print(f\"\\n=== PERFORMANCE INSIGHTS ===\")\n",
    "print(f\"Best fold: Fold {best_fold['fold']} (RÂ² = {best_fold['r2']:.4f})\")\n",
    "print(f\"  Period: {best_fold['test_start']} to {best_fold['test_end']}\")\n",
    "print(f\"Worst fold: Fold {worst_fold['fold']} (RÂ² = {worst_fold['r2']:.4f})\")\n",
    "print(f\"  Period: {worst_fold['test_start']} to {worst_fold['test_end']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410252d3",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ <strong>Tip</strong> (click to expand)</summary>\n",
    "\n",
    "\n",
    "Loop through `enumerate(tscv.split(X), 1)` to iterate through folds with numbering, using train_idx and test_idx with `.iloc[]` to split features and target for each fold. Don't reuse the same model across folds - create a fresh LinearRegression instance each time to ensure fair comparison. Use train_idx for training and test_idx for testing (remember to calculate metrics on the test set using y_test_fold vs y_pred_fold). Calculate both RÂ² and RMSE to understand performance completely, and extract test period characteristics like dates (using `df_clean.iloc[test_idx]['datetime']`), average temperature, and average demand. Use `max(list, key=lambda x: x['r2'])` to find the fold with best performance. Seasonal effects mean winter folds might show different patterns than summer, growth trends mean later folds include newer data with potentially different patterns, and holidays or special events in the test period affect predictability - understanding this variation helps your client identify when models need updating.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c3e4b1",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ¤« <strong>Solution</strong> (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "fold_results = []\n",
    "\n",
    "for fold_num, (train_idx, test_idx) in enumerate(tscv.split(X), 1):\n",
    "    # Split data for this fold\n",
    "    X_train_fold = X.iloc[train_idx]\n",
    "    X_test_fold = X.iloc[test_idx]\n",
    "    y_train_fold = y.iloc[train_idx]\n",
    "    y_test_fold = y.iloc[test_idx]\n",
    "\n",
    "    # Train model on this fold\n",
    "    model_fold = LinearRegression()\n",
    "    model_fold.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    y_pred_fold = model_fold.predict(X_test_fold)\n",
    "    r2_fold = r2_score(y_test_fold, y_pred_fold)\n",
    "    rmse_fold = np.sqrt(mean_squared_error(y_test_fold, y_pred_fold))\n",
    "\n",
    "    # Get test period dates and characteristics\n",
    "    test_dates_fold = df_clean.iloc[test_idx]['datetime']\n",
    "    avg_temp = df_clean.iloc[test_idx]['temp'].mean()\n",
    "    avg_demand = df_clean.iloc[test_idx]['count'].mean()\n",
    "\n",
    "    # Store results\n",
    "    fold_results.append({\n",
    "        'fold': fold_num,\n",
    "        'test_start': test_dates_fold.min().date(),\n",
    "        'test_end': test_dates_fold.max().date(),\n",
    "        'r2': r2_fold,\n",
    "        'rmse': rmse_fold,\n",
    "        'avg_temp': avg_temp,\n",
    "        'avg_demand': avg_demand\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "print(\"=== FOLD-BY-FOLD ANALYSIS ===\")\n",
    "for result in fold_results:\n",
    "    print(f\"\\nFold {result['fold']}: {result['test_start']} to {result['test_end']}\")\n",
    "    print(f\"  RÂ²: {result['r2']:.4f}  |  RMSE: {result['rmse']:.2f}\")\n",
    "    print(f\"  Avg temp: {result['avg_temp']:.1f}Â°C  |  Avg demand: {result['avg_demand']:.0f} bikes\")\n",
    "\n",
    "# Identify best and worst folds\n",
    "best_fold = max(fold_results, key=lambda x: x['r2'])\n",
    "worst_fold = min(fold_results, key=lambda x: x['r2'])\n",
    "\n",
    "print(f\"\\n=== PERFORMANCE INSIGHTS ===\")\n",
    "print(f\"Best fold: Fold {best_fold['fold']} (RÂ² = {best_fold['r2']:.4f})\")\n",
    "print(f\"  Period: {best_fold['test_start']} to {best_fold['test_end']}\")\n",
    "print(f\"  Characteristics: {best_fold['avg_temp']:.1f}Â°C avg, {best_fold['avg_demand']:.0f} bikes avg\")\n",
    "print(f\"\\nWorst fold: Fold {worst_fold['fold']} (RÂ² = {worst_fold['r2']:.4f})\")\n",
    "print(f\"  Period: {worst_fold['test_start']} to {worst_fold['test_end']}\")\n",
    "print(f\"  Characteristics: {worst_fold['avg_temp']:.1f}Â°C avg, {worst_fold['avg_demand']:.0f} bikes avg\")\n",
    "print(f\"\\nNote: Differences in performance across folds may indicate changes in demand patterns\")\n",
    "print(f\"over time (business growth, operational changes) or varying seasonal predictability.\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee52c0cc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318ce325",
   "metadata": {},
   "source": [
    "## Summary: Professional Linear Regression Modeling for Transportation Demand\n",
    "\n",
    "**What We've Accomplished:**\n",
    "- **Analyzed linear relationships** using correlation analysis and scipy.stats.linregress to understand which variables drive bike demand\n",
    "- **Engineered time-based features** including cyclical encoding (hour_sin/cos), lag features (demand history), and temporal components (day, month)\n",
    "- **Trained predictive models** using scikit-learn's LinearRegression with multiple feature combinations to forecast hourly bike demand\n",
    "- **Implemented chronological train-test splits** that preserve time series integrity and provide honest performance estimates for forecasting\n",
    "- **Applied cross-validation** with TimeSeriesSplit to obtain robust performance estimates across multiple temporal windows\n",
    "\n",
    "**Key Technical Skills Mastered:**\n",
    "- **Correlation analysis**: scipy.stats.linregress for calculating slopes, intercepts, and RÂ² values\n",
    "- **Feature engineering**: cyclical encoding with sin/cos, lag features with .shift(), datetime extraction with .dt accessor\n",
    "- **Model training**: LinearRegression.fit() for learning patterns, .predict() for generating forecasts\n",
    "- **Time series splitting**: chronological .iloc[] indexing, TimeSeriesSplit for expanding window cross-validation\n",
    "- **Performance evaluation**: r2_score, mean_squared_error, cross_val_score for comprehensive model assessment\n",
    "- **Result interpretation**: translating RÂ² and RMSE into business-relevant forecasting accuracy estimates\n",
    "\n",
    "**Next Steps:**\n",
    "In the next module, you'll advance to model evaluation techniques, learning to assess prediction quality, diagnose model limitations, and communicate performance to stakeholders. You'll also explore non-linear models that can capture more complex patterns beyond linear relationships.\n",
    "\n",
    "Your forecasting model transforms Capital City Bikes from reactive operations to predictive planning, enabling data-driven decisions about bike distribution, staffing levels, and capacity management. You've demonstrated the systematic machine learning workflow that professional data scientists use to deliver reliable predictions for real-world business applications!"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
