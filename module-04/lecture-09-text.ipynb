{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04b46f40",
   "metadata": {},
   "source": [
    "# Lecture 9: Tree-Based Models - Mastering Complex Pattern Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85586dcd",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this lecture, you will be able to:\n",
    "- Understand decision tree fundamentals and their hierarchical decision-making process\n",
    "- Explain how decision trees differ from linear regression in modeling approach\n",
    "- Recognize the overfitting problem and why individual decision trees have limitations\n",
    "- Understand ensemble learning concepts and the \"wisdom of crowds\" principle\n",
    "- Implement Random Forest models and explain their architecture\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ab59d9",
   "metadata": {},
   "source": [
    "## 1. Your Next Modeling Challenge: Capturing Non-Linear Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378becdd",
   "metadata": {},
   "source": [
    "### The Consultant's Evolution: When Linear Models Meet Their Limits\n",
    "\n",
    "Eight months into your consulting engagement with Capital City Bikes, your linear regression models have proven their worth. The board appreciates the interpretable coefficients, operations managers use temperature-based forecasts for daily planning, and the Series A funding round succeeded partly due to your data-driven demand predictions. But success has attracted attention—and revealed limitations.\n",
    "\n",
    "The CEO approaches you with new competitive intelligence: \"Three established bike-sharing companies have entered our market with sophisticated ML systems that outperform our predictions during complex scenarios. Last Tuesday's afternoon thunderstorm perfectly illustrates the problem. Our linear model predicted moderate demand based on average temperature relationships. Meanwhile, competitors somehow positioned their fleets perfectly for the post-storm surge when commuters abandoned transit for bikes. We lost significant market share in a single afternoon.\"\n",
    "\n",
    "She continues: \"Our investors want to know why we're using 'simple statistics' while competitors deploy 'advanced AI.' I trust our analytical foundation, but can we build more sophisticated prediction systems without losing the transparency that got us this far?\"\n",
    "\n",
    "This is the moment every data consultant anticipates—**when proven approaches meet their limitations and advanced capabilities become business necessities rather than technical luxuries**. Just as a bridge engineer progresses from understanding basic materials to designing complex structures, you're transitioning from foundational models to sophisticated algorithms that capture real-world complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b6b769",
   "metadata": {},
   "source": [
    "### Why Tree-Based Models: Intelligence Through Conditional Logic\n",
    "\n",
    "Think of decision trees as algorithms that mirror human decision-making processes. When experienced transportation managers predict demand, they don't apply simple mathematical formulas—they consider **conditional relationships**: \"If it's a weekend AND the weather is nice AND it's not a holiday, expect high recreational demand. But if it's a weekday AND it's raining AND it's rush hour, expect moderate commuter demand because people still need transportation.\"\n",
    "\n",
    "This conditional thinking is exactly what decision trees accomplish algorithmically. While linear regression assumes temperature always affects demand the same way regardless of context, **decision trees recognize that temperature's impact depends on other factors**—time of day, season, humidity, user types, and recent weather patterns.\n",
    "\n",
    "Decision trees serve multiple crucial purposes in your consulting toolkit. First, they **capture non-linear relationships** that linear models cannot represent with single equations. Second, they provide intuitive interpretability through human-readable decision rules. Third, they enable rapid pattern discovery for complex conditional relationships, particularly valuable when business logic suggests strong interaction effects between features.\n",
    "\n",
    "These qualities become your competitive advantage as the market becomes more sophisticated. While linear regression provided foundational insights and stakeholder confidence, **tree-based models provide pattern recognition that captures real-world complexity** while maintaining business interpretability through clear decision rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f156e3b4",
   "metadata": {},
   "source": [
    "## 2. Decision Tree Fundamentals\n",
    "\n",
    "This section establishes the theoretical foundation of decision tree algorithms before exploring their implementation complexities. We'll start by defining decision trees from first principles, examine how they differ fundamentally from the linear regression models we mastered in Lecture 8, understand the hierarchical decision-making process that drives tree construction, and explore the advantages and limitations that make trees particularly suitable for certain prediction scenarios in urban mobility contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a99f4c",
   "metadata": {},
   "source": [
    "### 2.1. What is a Decision Tree\n",
    "\n",
    "A decision tree is a machine learning algorithm that **makes predictions by learning a series of if-then decision rules from training data**. Unlike linear regression, which assumes relationships can be expressed as mathematical equations with fixed coefficients, decision trees create hierarchical rules that partition the data into distinct regions, each with its own prediction value.\n",
    "\n",
    "The fundamental difference lies in how these algorithms model relationships. Linear regression finds coefficients that define a single equation applying uniformly to all observations: `y = b₀ + b₁x₁ + b₂x₂ + ...`. Decision trees, in contrast, create different prediction rules for different data subsets: \"If x₁ > 20 AND x₂ ≤ 50, then predict y = 100; else if x₁ > 20 AND x₂ > 50, then predict y = 150.\" This enables them to **capture complex, non-linear patterns impossible to represent with single linear equations**.\n",
    "\n",
    "**At its core, a decision tree resembles human decision-making logic**. Each internal node represents a question about a specific feature (such as \"Is temperature > 20°C?\"), each branch represents an answer to that question, and each leaf node provides a final prediction. This hierarchical structure enables modeling conditional relationships where one feature's importance depends on other features' values.\n",
    "\n",
    "In the context of predictive modeling, decision trees serve as both practical prediction tools and interpretable business logic. Their transparency makes them particularly valuable when stakeholders need to understand not just what the model predicts, but **why it makes specific predictions for specific scenarios**.\n",
    "\n",
    "Let's see how a simple decision tree captures temperature and humidity relationships in bike-sharing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7f7d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "\n",
    "# Load the bike-sharing dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\")\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "print(\"=== DECISION TREE FUNDAMENTALS ===\\n\")\n",
    "\n",
    "# Create a simple 2-feature model for visualization\n",
    "X_simple = df[['temp', 'humidity']]\n",
    "y = df['count']\n",
    "\n",
    "# Train a shallow tree (max_depth=3) for interpretability\n",
    "# max_depth controls tree depth - shallow trees are easier to interpret\n",
    "# random_state ensures reproducible results across runs\n",
    "tree = DecisionTreeRegressor(max_depth=3, random_state=42)\n",
    "# .fit() trains the model on the provided features (X) and target (y)\n",
    "tree.fit(X_simple, y)\n",
    "\n",
    "print(\"--- Simple Decision Tree Structure ---\")\n",
    "# .get_depth() returns the maximum depth of the tree (root to furthest leaf)\n",
    "print(f\"Tree depth: {tree.get_depth()}\")\n",
    "# .get_n_leaves() returns the number of leaf nodes (final prediction regions)\n",
    "print(f\"Number of leaf nodes: {tree.get_n_leaves()}\")\n",
    "print(f\"Number of decision nodes: {tree.tree_.node_count - tree.get_n_leaves()}\")\n",
    "\n",
    "# Visualize the tree structure\n",
    "plt.figure(figsize=(16, 8))\n",
    "# plot_tree() creates a visual diagram of the tree's decision rules\n",
    "# filled=True colors nodes by predicted value, rounded=True smooths box corners\n",
    "plot_tree(tree, feature_names=['temp', 'humidity'], filled=True, rounded=True,\n",
    "          fontsize=10, proportion=True)\n",
    "plt.title('Decision Tree Structure: Temperature & Humidity → Bike Demand',\n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show example predictions following decision paths\n",
    "print(\"\\n--- Example Prediction Paths ---\")\n",
    "examples = [\n",
    "    {'temp': 25, 'humidity': 40, 'scenario': 'Warm & Dry'},\n",
    "    {'temp': 15, 'humidity': 70, 'scenario': 'Cool & Humid'},\n",
    "    {'temp': 10, 'humidity': 50, 'scenario': 'Cold & Moderate'}\n",
    "]\n",
    "\n",
    "for ex in examples:\n",
    "    X_example = pd.DataFrame({'temp': [ex['temp']], 'humidity': [ex['humidity']]})\n",
    "    # .predict() generates predictions for new observations\n",
    "    prediction = tree.predict(X_example)[0]\n",
    "\n",
    "    # Show the decision path\n",
    "    # .decision_path() returns which nodes were traversed for this prediction\n",
    "    decision_path = tree.decision_path(X_example)\n",
    "    node_indicator = decision_path.toarray()[0]\n",
    "\n",
    "    print(f\"\\n{ex['scenario']}: temp={ex['temp']}°C, humidity={ex['humidity']}%\")\n",
    "    print(f\"  Predicted demand: {prediction:.0f} bikes/hour\")\n",
    "    print(f\"  Decision path: {np.sum(node_indicator)} nodes traversed\")\n",
    "\n",
    "print(\"\\n--- Key Characteristics ---\")\n",
    "print(\"✓ Hierarchical structure: Root → Internal Nodes → Leaf Nodes\")\n",
    "print(\"✓ Each split creates subsets with more similar target values\")\n",
    "print(\"✓ Predictions are constant within each leaf region\")\n",
    "print(\"✓ No assumption about linear relationships required\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a1d7d3",
   "metadata": {},
   "source": [
    "**What this demonstrates:**\n",
    "- **Hierarchical decision logic** creates intuitive rules: \"If temp > 22.6°C, then check humidity...\"\n",
    "- **Different predictions for different regions** - the model learns that 25°C with low humidity generates different demand than 25°C with high humidity\n",
    "- **Automatic interaction discovery** - the tree finds that temperature and humidity jointly affect demand without manual feature engineering\n",
    "- **Human-readable structure** - stakeholders can follow the exact logic the model uses for predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f574a84d",
   "metadata": {},
   "source": [
    "### 2.2. How Trees Learn from Data\n",
    "\n",
    "Now that we understand what decision trees are, let's explore how they actually learn from data. The tree construction process transforms raw observations into hierarchical decision structures through **a systematic algorithm that recursively partitions data into increasingly homogeneous subsets**.\n",
    "\n",
    "The construction begins with the root node containing the entire training dataset. The algorithm evaluates every possible split for every feature, calculating how well each potential split separates the data according to the target variable. **For numerical features like temperature**, this involves testing thresholds: \"temperature ≤ 15°C or > 15°C\", \"temperature ≤ 20°C or > 20°C\", and so on for every unique value. **For categorical features like season**, this involves testing groupings: \"season in {winter, spring} or {summer, fall}\".\n",
    "\n",
    "The quality of each potential split is measured using **mathematical criteria that quantify uncertainty reduction**. For regression problems like demand prediction, the common criterion is mean squared error (MSE) reduction. A good split should create subsets where target values are more similar within each subset than in the original dataset. High MSE reduction indicates the split successfully separates observations with different target values.\n",
    "\n",
    "The concept underlying split selection is straightforward: **minimize within-group variance while maximizing between-group variance**. If we split temperature at 20°C, observations below 20°C should have similar demand values to each other (low within-group variance), and their average demand should differ substantially from observations above 20°C (high between-group variance). This creates more accurate predictions within each resulting subset.\n",
    "\n",
    "**The recursive splitting process** continues for each newly created subset, treating each as a smaller dataset that can be further divided. This creates the tree's hierarchical structure, where early splits near the root handle the most important distinctions (temperature might be the first split if it's most predictive), while later splits near the leaves handle more nuanced patterns affecting smaller subsets.\n",
    "\n",
    "**Stopping criteria** prevent infinite growth and control overfitting. Common conditions include:\n",
    "- Minimum samples per split (e.g., don't split nodes with fewer than 20 observations)\n",
    "- Maximum tree depth (e.g., stop after 10 levels of splits)\n",
    "- Minimum impurity decrease (e.g., only split if MSE improves by at least 0.01)\n",
    "\n",
    "The final tree structure represents a complete set of decision rules. To predict for a new observation, it follows the decision path from root to leaf, with each internal node directing the path based on feature values.\n",
    "\n",
    "Let's watch a decision tree learn splits step by step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa821b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the bike-sharing dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\")\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "print(\"=== HOW TREES LEARN: RECURSIVE SPLITTING ===\\n\")\n",
    "\n",
    "# Use simple features for clear demonstration\n",
    "X = df[['temp', 'humidity']]\n",
    "y = df['count']\n",
    "\n",
    "# Train trees with increasing depth to show progressive learning\n",
    "depths = [1, 2, 3, 5]\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, depth in enumerate(depths):\n",
    "    tree = DecisionTreeRegressor(max_depth=depth, random_state=42)\n",
    "    tree.fit(X, y)\n",
    "\n",
    "    predictions = tree.predict(X)\n",
    "    # mean_squared_error() calculates average squared difference between actual and predicted\n",
    "    mse = mean_squared_error(y, predictions)\n",
    "    # .score() returns R² (coefficient of determination): 1.0 = perfect, 0.0 = no better than mean\n",
    "    r2 = tree.score(X, y)\n",
    "\n",
    "    # Create decision boundary visualization\n",
    "    x_min, x_max = X['temp'].min() - 1, X['temp'].max() + 1\n",
    "    y_min, y_max = X['humidity'].min() - 1, X['humidity'].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "\n",
    "    # Convert meshgrid to DataFrame with proper feature names\n",
    "    mesh_data = pd.DataFrame(np.c_[xx.ravel(), yy.ravel()], columns=['temp', 'humidity'])\n",
    "    Z = tree.predict(mesh_data)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    axes[idx].contourf(xx, yy, Z, alpha=0.4, levels=15, cmap='RdYlGn')\n",
    "    scatter = axes[idx].scatter(X['temp'], X['humidity'], c=y, cmap='RdYlGn',\n",
    "                                 s=1, alpha=0.3, edgecolors='none')\n",
    "\n",
    "    axes[idx].set_xlabel('Temperature (°C)', fontsize=10)\n",
    "    axes[idx].set_ylabel('Humidity (%)', fontsize=10)\n",
    "    axes[idx].set_title(f'Depth={depth}: {tree.get_n_leaves()} regions, R²={r2:.3f}',\n",
    "                         fontsize=11, fontweight='bold')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Tree Learning Process: How Additional Splits Capture Complexity',\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.colorbar(scatter, ax=axes, label='Demand (bikes/hour)', shrink=0.8)\n",
    "plt.show()\n",
    "\n",
    "# Show numerical progression\n",
    "print(\"--- Learning Progression with Increasing Depth ---\\n\")\n",
    "for depth in [1, 2, 3, 5, 10, None]:\n",
    "    tree = DecisionTreeRegressor(max_depth=depth, random_state=42)\n",
    "    tree.fit(X, y)\n",
    "\n",
    "    train_r2 = tree.score(X, y)\n",
    "    train_mse = mean_squared_error(y, tree.predict(X))\n",
    "\n",
    "    depth_str = \"Unlimited\" if depth is None else str(depth)\n",
    "    print(f\"Max Depth = {depth_str:>9s}: \"\n",
    "          f\"{tree.get_n_leaves():>4d} leaves, \"\n",
    "          f\"R² = {train_r2:.4f}, \"\n",
    "          f\"MSE = {train_mse:>9.1f}\")\n",
    "\n",
    "print(\"\\n--- Key Observations ---\")\n",
    "print(\"✓ Depth 1: Single split creates 2 regions (most important distinction)\")\n",
    "print(\"✓ Each additional level doubles potential regions (2^depth)\")\n",
    "print(\"✓ Deeper trees fit training data better (higher R², lower MSE)\")\n",
    "print(\"✓ With only 2 features, even unlimited depth cannot achieve perfect fit\")\n",
    "print(\"⚠ This demonstrates feature limitation: model capacity constrained by inputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a49d802",
   "metadata": {},
   "source": [
    "**What this demonstrates:**\n",
    "- **Progressive learning** - depth 1 makes one split (likely on temperature), creating 2 prediction regions\n",
    "- **Exponential complexity** - each depth level can double the number of regions (2¹, 2², 2³, etc.)\n",
    "- **Training performance improvement** - R² increases from ~0.10 (depth 1) to ~0.36 (unlimited depth)\n",
    "- **Limited by features** - even unlimited depth with 974 leaves achieves only R² = 0.36 because only 2 features cannot fully capture demand patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef392bc8",
   "metadata": {},
   "source": [
    "### 2.3. Decision Trees vs. Linear Regression\n",
    "\n",
    "Now that we understand how decision trees work, let's examine how they compare to the linear regression models we mastered in Lecture 8. This comparison reveals fundamental differences in modeling philosophy that determine when each approach provides optimal value.\n",
    "\n",
    "**Linear regression** assumes relationships can be expressed through mathematical equations with fixed coefficients applied uniformly: `Demand = 15 + 9.2×temp - 2.2×humidity`. The coefficient 9.2 means every 1°C temperature increase adds 9.2 bikes/hour on average, **regardless of humidity, season, or any other context**. This simplicity enables clear interpretation but limits representational flexibility.\n",
    "\n",
    "**Decision trees** make no such uniformity assumption. Instead, they create **conditional rules where one feature's effect depends on other features' values**: \"If temp > 22°C AND humidity < 50%, predict 200 bikes. But if temp > 22°C AND humidity > 70%, predict 120 bikes.\" Temperature's effect changes based on humidity context—exactly how real-world patterns often behave.\n",
    "\n",
    "The comparison spans multiple critical dimensions:\n",
    "\n",
    "- **Interpretability**: Linear regression provides global interpretability through coefficient values: \"temperature increases demand by 9.2 bikes/°C everywhere.\" Decision trees provide local interpretability through scenario-specific rules: \"in this specific context (warm + dry), expect high demand.\"\n",
    "- **Non-linear relationships**: Linear regression assumes straight-line relationships—temperature's effect is constant. Decision trees capture non-linearity naturally: they might learn that demand increases rapidly from 10-20°C, plateaus from 20-25°C, then decreases above 30°C, all without explicit non-linear terms.\n",
    "- **Robustness**: Linear regression is sensitive to outliers that can substantially affect fitted coefficients. Decision trees are outlier-resistant because they split on thresholds rather than fitting equations: one extreme value can't distort the entire model structure.\n",
    "- **Data requirements**: Linear regression works with small datasets (100s of observations) when relationships are approximately linear. Decision trees require larger datasets (1000s of observations) to reliably learn hierarchical patterns without overfitting.\n",
    "\n",
    "Let's visualize these fundamental modeling differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da2a183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Load the bike-sharing dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\")\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "print(\"=== LINEAR REGRESSION VS DECISION TREES ===\\n\")\n",
    "\n",
    "# Use temperature as single predictor for clear comparison\n",
    "X = df[['temp']]\n",
    "y = df['count']\n",
    "\n",
    "# Train both models\n",
    "# LinearRegression() creates a model that fits: y = coefficient×x + intercept\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X, y)\n",
    "\n",
    "# min_samples_leaf=100 prevents leaf nodes with fewer than 100 observations\n",
    "tree_model = DecisionTreeRegressor(max_depth=5, min_samples_leaf=100, random_state=42)\n",
    "tree_model.fit(X, y)\n",
    "\n",
    "# Generate predictions\n",
    "temp_range = np.linspace(X['temp'].min(), X['temp'].max(), 300)\n",
    "temp_range_df = pd.DataFrame({'temp': temp_range})\n",
    "lr_predictions = lr_model.predict(temp_range_df)\n",
    "tree_predictions = tree_model.predict(temp_range_df)\n",
    "\n",
    "# Visualize differences\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Linear Regression\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X, y, alpha=0.1, s=5, color='gray', label='Actual Data')\n",
    "plt.plot(temp_range_df['temp'], lr_predictions, 'r-', linewidth=3, label='Linear Regression')\n",
    "plt.xlabel('Temperature (°C)', fontsize=11)\n",
    "plt.ylabel('Hourly Bike Rentals', fontsize=11)\n",
    "plt.title('Linear Regression: Single Global Relationship', fontsize=12, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add equation\n",
    "# .coef_ contains the fitted coefficient(s), .intercept_ contains the y-intercept\n",
    "equation = f'y = {lr_model.coef_[0]:.2f}×temp + {lr_model.intercept_:.2f}'\n",
    "plt.text(0.05, 0.95, equation, transform=plt.gca().transAxes,\n",
    "         fontsize=10, verticalalignment='top',\n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "# Decision Tree\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X, y, alpha=0.1, s=5, color='gray', label='Actual Data')\n",
    "plt.plot(temp_range_df['temp'], tree_predictions, 'b-', linewidth=3, label='Decision Tree')\n",
    "plt.xlabel('Temperature (°C)', fontsize=11)\n",
    "plt.ylabel('Hourly Bike Rentals', fontsize=11)\n",
    "plt.title('Decision Tree: Multiple Local Relationships', fontsize=12, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add tree info\n",
    "tree_info = f'{tree_model.get_n_leaves()} regions (leaves)\\nDepth = {tree_model.get_depth()}'\n",
    "plt.text(0.05, 0.95, tree_info, transform=plt.gca().transAxes,\n",
    "         fontsize=10, verticalalignment='top',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"--- Model Comparison ---\\n\")\n",
    "\n",
    "# Linear Regression characteristics\n",
    "print(\"Linear Regression:\")\n",
    "print(f\"  Equation: Demand = {lr_model.coef_[0]:.2f} × temp + {lr_model.intercept_:.2f}\")\n",
    "print(f\"  Prediction type: Smooth, continuous line\")\n",
    "print(f\"  Interpretation: 'Each 1°C increase adds {lr_model.coef_[0]:.1f} bikes everywhere'\")\n",
    "print(f\"  Training R²: {lr_model.score(X, y):.4f}\")\n",
    "\n",
    "print(\"\\nDecision Tree:\")\n",
    "print(f\"  Structure: {tree_model.get_n_leaves()} leaf regions, depth {tree_model.get_depth()}\")\n",
    "print(f\"  Prediction type: Step function (constant within regions)\")\n",
    "print(f\"  Interpretation: 'Different temp ranges have different demand levels'\")\n",
    "print(f\"  Training R²: {tree_model.score(X, y):.4f}\")\n",
    "\n",
    "print(\"\\n--- Key Differences for Capital City Bikes ---\")\n",
    "print(\"\\nWhen to use Linear Regression:\")\n",
    "print(\"  ✓ Relationships are approximately linear\")\n",
    "print(\"  ✓ Need simple, global coefficient interpretation\")\n",
    "print(\"  ✓ Communicating average effects to stakeholders\")\n",
    "print(\"  ✓ Small datasets (hundreds of observations)\")\n",
    "\n",
    "print(\"\\nWhen to use Decision Trees:\")\n",
    "print(\"  ✓ Relationships are non-linear or conditional\")\n",
    "print(\"  ✓ Need scenario-specific predictions (different contexts)\")\n",
    "print(\"  ✓ Automatic feature interaction discovery important\")\n",
    "print(\"  ✓ Large datasets (thousands of observations)\")\n",
    "\n",
    "print(\"\\nComplementary strengths:\")\n",
    "print(\"  → Start with linear regression for baseline + interpretability\")\n",
    "print(\"  → Add decision trees when complexity justifies additional sophistication\")\n",
    "print(\"  → Use both to understand simple trends vs. complex patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655326c1",
   "metadata": {},
   "source": [
    "**What this demonstrates:**\n",
    "- **Linear regression** creates a single straight line: the same temperature effect applies everywhere\n",
    "- **Decision trees** create step functions: different temperature ranges have different predicted demands\n",
    "- **Training performance** - trees fit training data better (higher R²) due to additional flexibility\n",
    "- **Interpretability trade-off** - linear regression provides one clear rule; trees provide multiple conditional rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17567828",
   "metadata": {},
   "source": [
    "## 3. From Single Trees to Forest Intelligence\n",
    "\n",
    "This section transitions from individual decision trees to powerful ensemble methods that address fundamental tree limitations. We'll build your understanding systematically: first examining why individual trees fail in practice through the overfitting problem, then understanding how combining multiple models creates collective intelligence, mastering Random Forest architecture that systematically combines trees, and finally implementing these concepts with scikit-learn to create production-ready prediction systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f968cf",
   "metadata": {},
   "source": [
    "### 3.1. Why Individual Trees Fail: The Overfitting Problem\n",
    "\n",
    "Decision trees can achieve near-perfect accuracy on training data by growing deep enough to memorize every observation. This sounds impressive—until we test on new data and performance collapses. This is **the overfitting problem**, the most critical limitation preventing individual decision trees from reliable deployment.\n",
    "\n",
    "Overfitting occurs when a model **learns training data too specifically, capturing noise and random variations rather than underlying patterns** that generalize to new observations. Decision trees are particularly vulnerable because their flexible structure allows them to create increasingly specific rules: \"If temp = 22.3°C AND humidity = 67% AND hour = 8 AND weekday = Tuesday, then predict 43 bikes.\" This rule perfectly predicts one training observation but fails completely for similar conditions like 22.5°C or Wednesday.\n",
    "\n",
    "The problem manifests through **high variance**—small changes in training data produce dramatically different tree structures with inconsistent predictions. Train on January-October data versus February-November data, and you might get entirely different splitting patterns and prediction rules. This instability makes individual trees unreliable for business applications requiring consistent forecasting.\n",
    "\n",
    "The bias-variance tradeoff provides the mathematical framework. **Bias** refers to errors from oversimplifying relationships (underfitting). **Variance** refers to errors from excessive sensitivity to training data specifics (overfitting). Individual decision trees exhibit low bias (they can represent complex patterns) but extremely high variance (they're unstable across training samples). This high variance is their Achilles heel.\n",
    "\n",
    "For Capital City Bikes, an overfit tree might memorize that \"May 15th at 5pm had 47 rentals\" rather than learning \"spring weekday evenings average 45 rentals.\" The memorized fact is useless for predicting May 16th or any future date. What we need is **learned patterns, not memorized history**.\n",
    "\n",
    "**The severity of overfitting depends critically on feature set complexity**. With many features—especially engineered features like interactions (`temp × humidity`), polynomials (`temp²`), and temporal bins—trees have more opportunities to memorize noise through highly specific rule combinations. With limited features, overfitting still occurs but is constrained by the smaller decision space available for memorization.\n",
    "\n",
    "Let's see overfitting in action by comparing training and testing performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460df14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the bike-sharing dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\")\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "# Extract temporal features\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['month'] = df['datetime'].dt.month\n",
    "df['weekday'] = df['datetime'].dt.weekday\n",
    "\n",
    "print(\"=== THE OVERFITTING PROBLEM ===\\n\")\n",
    "\n",
    "# Prepare features\n",
    "feature_cols = ['temp', 'humidity', 'windspeed', 'hour', 'weekday', 'month']\n",
    "X = df[feature_cols]\n",
    "y = df['count']\n",
    "\n",
    "# Chronological split for time series\n",
    "# Use first 80% for training, last 20% for testing (preserves temporal order)\n",
    "split_idx = int(len(df) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "print(f\"Training: {len(X_train):,} observations\")\n",
    "print(f\"Testing:  {len(X_test):,} observations\\n\")\n",
    "\n",
    "# Train trees with increasing complexity\n",
    "# None means unlimited depth - tree grows until all leaves are pure or hit other limits\n",
    "depths = [2, 5, 10, 20, None]\n",
    "results = []\n",
    "\n",
    "for depth in depths:\n",
    "    tree = DecisionTreeRegressor(max_depth=depth, random_state=42)\n",
    "    tree.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate on both sets\n",
    "    # R² measures proportion of variance explained (higher is better, max 1.0)\n",
    "    train_r2 = tree.score(X_train, y_train)\n",
    "    test_r2 = tree.score(X_test, y_test)\n",
    "\n",
    "    # RMSE measures average prediction error in original units (lower is better)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, tree.predict(X_train)))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, tree.predict(X_test)))\n",
    "\n",
    "    results.append({\n",
    "        'depth': 'Unlimited' if depth is None else depth,\n",
    "        'leaves': tree.get_n_leaves(),\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'gap': train_r2 - test_r2\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "print(\"--- Overfitting Progression ---\")\n",
    "print(f\"{'Depth':<10} {'Leaves':<8} {'Train R²':<10} {'Test R²':<10} {'Gap':<10} {'Status'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for r in results:\n",
    "    status = \"✓ Good\" if r['gap'] < 0.10 else \"⚠ Moderate\" if r['gap'] < 0.30 else \"✗ Severe Overfit\"\n",
    "    print(f\"{str(r['depth']):<10} {r['leaves']:<8} {r['train_r2']:<10.4f} \"\n",
    "          f\"{r['test_r2']:<10.4f} {r['gap']:<10.4f} {status}\")\n",
    "\n",
    "# Visualize overfitting\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# R² comparison\n",
    "depths_plot = [str(r['depth']) for r in results]\n",
    "axes[0].plot(depths_plot, [r['train_r2'] for r in results], 'o-', linewidth=2,\n",
    "             markersize=8, label='Training R²', color='#2ECC71')\n",
    "axes[0].plot(depths_plot, [r['test_r2'] for r in results], 's-', linewidth=2,\n",
    "             markersize=8, label='Testing R²', color='#E74C3C')\n",
    "axes[0].set_xlabel('Maximum Tree Depth', fontsize=11)\n",
    "axes[0].set_ylabel('R² Score', fontsize=11)\n",
    "axes[0].set_title('Overfitting: Training vs Testing Performance', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Performance gap\n",
    "axes[1].bar(depths_plot, [r['gap'] for r in results], color=['#2ECC71' if r['gap'] < 0.10\n",
    "            else '#F39C12' if r['gap'] < 0.30 else '#E74C3C' for r in results], alpha=0.7)\n",
    "axes[1].axhline(y=0.10, color='orange', linestyle='--', linewidth=2, label='Moderate Overfit Threshold')\n",
    "axes[1].axhline(y=0.30, color='red', linestyle='--', linewidth=2, label='Severe Overfit Threshold')\n",
    "axes[1].set_xlabel('Maximum Tree Depth', fontsize=11)\n",
    "axes[1].set_ylabel('Performance Gap (Train R² - Test R²)', fontsize=11)\n",
    "axes[1].set_title('Overfitting Severity', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Key Insights ---\")\n",
    "print(f\"Shallow trees (depth 2-5): Limited train R² → High bias (underfitting)\")\n",
    "print(f\"Deep trees (depth 10-20): Improved train R² but test R² begins declining → Overfitting emerges\")\n",
    "print(f\"Unlimited depth: Near-perfect train R² ({results[-1]['train_r2']:.2%}) but test R² drops to ({results[-1]['test_r2']:.2%})\")\n",
    "print(f\"Performance gap: {results[-1]['train_r2'] - results[-1]['test_r2']:.2f} ({(results[-1]['train_r2'] - results[-1]['test_r2']) * 100:.0f} percentage points)\")\n",
    "print(f\"\\nConclusion with 6 basic features:\")\n",
    "print(f\"  • Overfitting is present (39-point train-test gap) but moderate\")\n",
    "print(f\"  • Limited feature space constrains how much noise can be memorized\")\n",
    "print(f\"  • With richer features (15-20 including interactions), gaps would be far more severe\")\n",
    "print(f\"  • Individual trees still face the bias-variance dilemma with no optimal depth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72f20fb",
   "metadata": {},
   "source": [
    "**What this demonstrates:**\n",
    "- **Training performance improves monotonically** - deeper trees always fit training data better (R² increases from ~0.40 to ~0.99)\n",
    "- **Testing performance peaks then declines** - test R² peaks around depth 10-20, then slightly degrades as depth increases further\n",
    "- **Moderate overfitting with 6 features** - unlimited depth achieves near-perfect training R² (99%) but testing R² of ~61%, a 39-percentage-point gap\n",
    "- **Feature set constraints** - with only 6 basic features, even unlimited-depth trees are somewhat constrained in how much noise they can memorize\n",
    "- **The bias-variance dilemma persists** - no single tree depth achieves both excellent training fit and strong generalization\n",
    "- **Preview of richer features** - in the programming example with 15-20 engineered features, you'll see far more dramatic overfitting (training R² 99%, testing R² 45-50%) and Random Forest's correspondingly stronger benefits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1c926d",
   "metadata": {},
   "source": [
    "### 3.2. Random Forest: The Complete Ensemble Solution\n",
    "\n",
    "Now that we understand why individual trees fail, let's explore an algorithm that solves this problem: **Random Forest**. The fundamental insight is surprisingly simple—combining multiple imperfect models often produces results superior to any individual model, similar to how diverse teams make better decisions than individual experts.\n",
    "\n",
    "The **\"wisdom of crowds\" principle** explains this phenomenon. When multiple models make independent predictions, their individual errors tend to be partially uncorrelated. Model A might overpredict scenario X while underpredicting scenario Y. Model B makes different mistakes in different scenarios. **Averaging their predictions cancels out individual errors**, capturing collective knowledge while reducing mistake impacts.\n",
    "\n",
    "Random Forest implements this principle through **two complementary mechanisms** that create diverse trees:\n",
    "\n",
    "1. **Bootstrap sampling**: Each tree trains on a different random sample drawn with replacement from the original dataset. This means some observations appear multiple times in a bootstrap sample while others don't appear at all. Each tree learns from a slightly different data perspective, leading to different tree structures and prediction patterns.\n",
    "2. **Feature randomness**: At each node in each tree, instead of considering all features when selecting the best split, the algorithm randomly selects a subset of features and chooses the best split only from this subset. This process repeats at every internal node in every tree, ensuring extensive diversity in how different trees partition the feature space.\n",
    "\n",
    "The mathematical beauty of Random Forest lies in **variance reduction without bias increase**. Individual decision trees have high variance (unstable across training samples) but low bias (can capture complex patterns). Averaging many high-variance models—each trained on different bootstrap samples and using different feature subsets—dramatically reduces variance while maintaining pattern-capture ability. **This is the theoretical solution to the bias-variance dilemma that trapped individual trees**.\n",
    "\n",
    "**Ensemble diversity is crucial**. If all models make identical predictions, the ensemble provides no advantage. Random Forest's two diversity mechanisms work together: bootstrap sampling ensures each tree sees different observations, while feature randomness ensures each tree partitions the space differently. This combination creates powerful, uncorrelated predictions that average beautifully.\n",
    "\n",
    "**The magnitude of Random Forest's benefits depends on the overfitting severity**. When individual trees severely overfit (high variance), ensemble averaging produces dramatic improvements in testing performance. When individual trees exhibit moderate overfitting—as can occur with limited feature sets—Random Forest still provides crucial benefits through **prediction stability and reduced variance**, even if absolute accuracy gains are modest. In both cases, the theoretical principle holds: averaging diverse high-variance models reduces overall variance.\n",
    "\n",
    "Let's see Random Forest in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a5c295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the bike-sharing dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\")\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "# Extract temporal features\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['month'] = df['datetime'].dt.month\n",
    "df['weekday'] = df['datetime'].dt.weekday\n",
    "\n",
    "print(\"=== RANDOM FOREST: FROM SINGLE TREE TO ENSEMBLE ===\\n\")\n",
    "\n",
    "# Prepare features\n",
    "feature_cols = ['temp', 'humidity', 'windspeed', 'hour', 'weekday', 'month']\n",
    "X = df[feature_cols]\n",
    "y = df['count']\n",
    "\n",
    "print(f\"Total features: {len(feature_cols)}\")\n",
    "print(f\"Random Forest max_features: √{len(feature_cols)} ≈ {int(np.sqrt(len(feature_cols)))} features per split\\n\")\n",
    "\n",
    "# Chronological split\n",
    "split_idx = int(len(df) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "print(f\"Training: {len(X_train):,} observations\")\n",
    "print(f\"Testing:  {len(X_test):,} observations\\n\")\n",
    "\n",
    "# Single tree baseline (the problem)\n",
    "print(\"--- Single Decision Tree (Baseline) ---\")\n",
    "# No max_depth specified = unlimited growth, demonstrating the overfitting problem\n",
    "single_tree = DecisionTreeRegressor(random_state=42)\n",
    "single_tree.fit(X_train, y_train)\n",
    "\n",
    "single_train_r2 = single_tree.score(X_train, y_train)\n",
    "single_test_r2 = single_tree.score(X_test, y_test)\n",
    "\n",
    "print(f\"Training R²: {single_train_r2:.4f}\")\n",
    "print(f\"Testing R²:  {single_test_r2:.4f}\")\n",
    "print(f\"Overfit gap: {single_train_r2 - single_test_r2:.4f}\")\n",
    "print(f\"Leaves: {single_tree.get_n_leaves()}\")\n",
    "\n",
    "# Compare Random Forest ensemble sizes\n",
    "print(\"\\n--- Random Forest: Impact of Ensemble Size ---\")\n",
    "ensemble_sizes = [10, 25, 50, 100]\n",
    "results = [{'n_trees': 1, 'train_r2': single_train_r2, 'test_r2': single_test_r2, \n",
    "            'gap': single_train_r2 - single_test_r2}]\n",
    "\n",
    "for n_trees in ensemble_sizes:\n",
    "    # RandomForestRegressor() creates an ensemble of decision trees\n",
    "    # n_estimators: number of trees to build\n",
    "    # max_features='sqrt': at each split, consider √n features for diversity\n",
    "    # n_jobs=-1: use all CPU cores for parallel training\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=n_trees,\n",
    "        max_features='sqrt',  # √6 ≈ 2-3 features per split\n",
    "        random_state=42,\n",
    "        n_jobs=-1  # Use all CPU cores\n",
    "    )\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    train_r2 = rf.score(X_train, y_train)\n",
    "    test_r2 = rf.score(X_test, y_test)\n",
    "    gap = train_r2 - test_r2\n",
    "\n",
    "    results.append({\n",
    "        'n_trees': n_trees,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'gap': gap\n",
    "    })\n",
    "    \n",
    "    print(f\"Trees: {n_trees:>3}  Train R²: {train_r2:.4f}  Test R²: {test_r2:.4f}  Gap: {gap:.4f}\")\n",
    "\n",
    "# Visualize ensemble growth\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Panel 1: Performance vs ensemble size\n",
    "n_trees_plot = [r['n_trees'] for r in results]\n",
    "axes[0].plot(n_trees_plot, [r['train_r2'] for r in results], 'o-', linewidth=2,\n",
    "             markersize=8, label='Training R²', color='#2ECC71')\n",
    "axes[0].plot(n_trees_plot, [r['test_r2'] for r in results], 's-', linewidth=2,\n",
    "             markersize=8, label='Testing R²', color='#3498DB')\n",
    "axes[0].set_xlabel('Number of Trees in Ensemble', fontsize=11)\n",
    "axes[0].set_ylabel('R² Score', fontsize=11)\n",
    "axes[0].set_title('Ensemble Size vs Performance', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xscale('log')\n",
    "\n",
    "# Panel 2: Overfitting gap reduction\n",
    "axes[1].plot(n_trees_plot, [r['gap'] for r in results], 'o-', linewidth=2,\n",
    "             markersize=8, color='#E74C3C')\n",
    "axes[1].axhline(y=results[0]['gap'], color='gray', linestyle='--', linewidth=2,\n",
    "                label=f'Single Tree Gap ({results[0][\"gap\"]:.3f})')\n",
    "axes[1].set_xlabel('Number of Trees in Ensemble', fontsize=11)\n",
    "axes[1].set_ylabel('Overfit Gap (Train R² - Test R²)', fontsize=11)\n",
    "axes[1].set_title('Variance Reduction Through Averaging', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Key Insights ---\")\n",
    "print(f\"✓ Single tree overfitting: Gap = {results[0]['gap']:.3f} (train R² {results[0]['train_r2']:.3f}, test R² {results[0]['test_r2']:.3f})\")\n",
    "print(f\"✓ Random Forest (100 trees): Gap = {results[-1]['gap']:.3f} (train R² {results[-1]['train_r2']:.3f}, test R² {results[-1]['test_r2']:.3f})\")\n",
    "print(f\"✓ Gap reduction: {results[0]['gap']:.3f} → {results[-1]['gap']:.3f} ({results[0]['gap'] - results[-1]['gap']:+.3f} improvement)\")\n",
    "print(f\"✓ Test R² change: {results[0]['test_r2']:.3f} → {results[-1]['test_r2']:.3f} ({results[-1]['test_r2'] - results[0]['test_r2']:+.3f})\")\n",
    "print(f\"\\nWith 6 basic features:\")\n",
    "print(f\"  • Random Forest reduces variance and provides prediction stability\")\n",
    "print(f\"  • Test accuracy improvement is modest due to moderate baseline overfitting\")\n",
    "print(f\"  • Main benefit: consistent, reliable predictions rather than dramatic accuracy gains\")\n",
    "print(f\"\\nIn programming example with 15-20 engineered features:\")\n",
    "print(f\"  • Individual trees will show severe overfitting (test R² ~0.45-0.50)\")\n",
    "print(f\"  • Random Forest will demonstrate dramatic improvement (test R² ~0.72-0.75)\")\n",
    "print(f\"  • This validates the theoretical principle: ensemble averaging shines when variance is high\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2713b9",
   "metadata": {},
   "source": [
    "**Understanding the RandomForestRegressor parameters:**\n",
    "\n",
    "The code above demonstrates Random Forest's core components through scikit-learn's `RandomForestRegressor` class. Let's examine each parameter and design decision:\n",
    "\n",
    "- **`n_estimators`**: The number of trees in the forest. We test [10, 25, 50, 100] to show how ensemble size affects performance. More trees generally improve predictions by averaging more diverse estimates, though with diminishing returns. In practice, 100-500 trees work well for most problems.\n",
    "\n",
    "- **`max_features='sqrt'`**: At each split, the algorithm randomly selects √n features to consider (√6 ≈ 2-3 features in our case). This feature randomness forces different trees to use different features, creating ensemble diversity that reduces correlation between tree predictions. For regression problems, `'sqrt'` is the recommended default.\n",
    "\n",
    "- **`random_state=42`**: Sets the random seed for reproducible results. Random Forest involves randomness in both bootstrap sampling and feature selection, so fixing the seed ensures consistent outputs across runs—essential for debugging and fair model comparison.\n",
    "\n",
    "- **`n_jobs=-1`**: Uses all available CPU cores for parallel tree training. Since trees train independently, Random Forest naturally parallelizes, making it efficient on modern multi-core machines. The value -1 tells scikit-learn to use all processors.\n",
    "\n",
    "The loop structure compares ensemble sizes systematically. For each size, we:\n",
    "1. **Initialize** a Random Forest with specific parameters\n",
    "2. **Train** using `.fit(X_train, y_train)` which builds all trees on bootstrap samples\n",
    "3. **Evaluate** on both training and testing sets using `.score()` (returns R²)\n",
    "4. **Calculate the gap** between training and testing R² to quantify overfitting\n",
    "5. **Store results** for visualization and comparison\n",
    "\n",
    "The visualization creates two panels that tell the complete story: Panel 1 shows how training R² remains high (~0.95-0.98) while test R² stays relatively stable (~0.61) as we add trees, while Panel 2 demonstrates modest gap reduction as ensemble size increases. This reveals an important lesson about Random Forest's dual nature: **it always reduces variance through ensemble averaging, but the magnitude of improvement depends on the severity of baseline overfitting**.\n",
    "\n",
    "**What this demonstrates:**\n",
    "- **Theoretical principle confirmed** - Random Forest reduces the train-test gap through ensemble averaging of diverse high-variance trees\n",
    "- **Moderate baseline overfitting** - with 6 basic features, the single tree's gap (~0.39) is already somewhat constrained\n",
    "- **Variance reduction observed** - ensemble averaging provides gap reduction and prediction stability, the core theoretical benefit\n",
    "- **Context-dependent gains** - when baseline overfitting is moderate, Random Forest's main value is stability rather than dramatic accuracy improvement\n",
    "- **Feature engineering matters** - the programming example will demonstrate Random Forest's full potential with richer features (15-20) where baseline overfitting is severe and ensemble benefits are dramatic\n",
    "- **Business value** - even with modest accuracy gains, Random Forest provides crucial prediction consistency that enables reliable operational planning for Capital City Bikes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7a499c",
   "metadata": {},
   "source": [
    "### 3.3. Understanding Feature Importance in Random Forests\n",
    "\n",
    "While Random Forest provides strong predictive performance, business stakeholders rarely settle for black-box predictions. They need answers to strategic questions: \"Which factors actually drive bike demand?\" \"Should we invest in weather-responsive strategies or focus on temporal patterns?\" \"Which features justify data collection and maintenance costs?\"\n",
    "\n",
    "Random Forest provides a natural mechanism for answering these questions: **feature importance scores**. The algorithm uses a concept called **mean decrease in impurity** (MDI). Recall that when building each tree, the algorithm splits nodes to reduce uncertainty. Features that consistently produce large uncertainty reductions are more important for predictions.\n",
    "\n",
    "Here's the calculation process:\n",
    "1. For each tree, track which features are used for splitting and how much each split reduces MSE\n",
    "2. Sum the total MSE reduction attributable to each feature across all splits in that tree\n",
    "3. Average these totals across all trees in the forest\n",
    "4. Normalize to percentages (importance scores sum to 1.0)\n",
    "\n",
    "**Features with high importance scores** appear frequently in trees and produce large MSE reductions when used for splitting. These are the predictors that most effectively separate high-demand from low-demand scenarios.\n",
    "\n",
    "**Important interpretation guidelines:**\n",
    "\n",
    "- **Importance is relative, not absolute**: A score of 0.60 means \"this feature accounts for 60% of total predictive contribution relative to other features in the model,\" not \"this feature explains 60% of variance in the target.\"\n",
    "\n",
    "- **Correlated features share importance**: If two features are highly correlated (like `temp` and `season`), Random Forest may distribute importance between them unpredictably. The pair's combined importance is meaningful, but individual scores are less stable.\n",
    "\n",
    "- **High importance ≠ causation**: Feature importance identifies predictive relationships, not causal mechanisms. High `hour` importance means time-of-day predicts demand well, but doesn't prove that time causes demand (though in this case the causal link is clear).\n",
    "\n",
    "- **Low importance has multiple meanings**: A feature might score low because it's genuinely unimportant, because it's redundant with other features, or because it affects only a small subset of observations.\n",
    "\n",
    "Let's extract and interpret feature importance for bike demand prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b0a766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Load the bike-sharing dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\")\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "# Extract temporal features\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['month'] = df['datetime'].dt.month\n",
    "df['weekday'] = df['datetime'].dt.weekday\n",
    "\n",
    "print(\"=== RANDOM FOREST FEATURE IMPORTANCE ===\\n\")\n",
    "\n",
    "# Prepare features\n",
    "feature_cols = ['temp', 'humidity', 'windspeed', 'hour', 'weekday', 'month']\n",
    "X = df[feature_cols]\n",
    "y = df['count']\n",
    "\n",
    "# Chronological split\n",
    "split_idx = int(len(df) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "print(f\"Training on {len(X_train):,} observations\")\n",
    "print(f\"Features: {', '.join(feature_cols)}\\n\")\n",
    "\n",
    "# Train Random Forest\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_features='sqrt',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Extract feature importance\n",
    "# .feature_importances_ returns the relative importance of each feature (sum = 1.0)\n",
    "# Based on mean decrease in impurity (MSE reduction) across all trees\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Display rankings\n",
    "print(\"--- Feature Importance Rankings ---\\n\")\n",
    "print(f\"{'Rank':<6} {'Feature':<12} {'Importance':<12} {'Visual'}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for rank, (idx, row) in enumerate(feature_importance_df.iterrows(), 1):\n",
    "    bar = '█' * int(row['importance'] * 100)\n",
    "    percentage = row['importance'] * 100\n",
    "    print(f\"{rank:<6} {row['feature']:<12} {percentage:>6.2f}%        {bar}\")\n",
    "\n",
    "# Calculate cumulative importance\n",
    "feature_importance_df['cumulative'] = feature_importance_df['importance'].cumsum()\n",
    "\n",
    "print(\"\\n--- Cumulative Importance Analysis ---\")\n",
    "print(f\"Top feature alone: {feature_importance_df.iloc[0]['importance']:.1%} of predictive power\")\n",
    "print(f\"Top 2 features: {feature_importance_df.iloc[1]['cumulative']:.1%} of predictive power\")\n",
    "print(f\"Top 3 features: {feature_importance_df.iloc[2]['cumulative']:.1%} of predictive power\")\n",
    "\n",
    "# Visualize feature importance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Panel 1: Horizontal bar chart\n",
    "colors = ['#E74C3C' if i == 0 else '#F39C12' if i == 1 else '#3498DB' \n",
    "          for i in range(len(feature_importance_df))]\n",
    "axes[0].barh(feature_importance_df['feature'], feature_importance_df['importance'],\n",
    "             color=colors, alpha=0.7)\n",
    "axes[0].set_xlabel('Importance Score', fontsize=11)\n",
    "axes[0].set_title('Feature Importance Rankings', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Panel 2: Cumulative importance\n",
    "axes[1].plot(range(1, len(feature_importance_df) + 1), \n",
    "             feature_importance_df['cumulative'], 'o-', linewidth=2,\n",
    "             markersize=8, color='#9B59B6')\n",
    "axes[1].axhline(y=0.80, color='gray', linestyle='--', linewidth=2, \n",
    "                label='80% threshold')\n",
    "axes[1].set_xlabel('Number of Top Features', fontsize=11)\n",
    "axes[1].set_ylabel('Cumulative Importance', fontsize=11)\n",
    "axes[1].set_title('Cumulative Feature Contribution', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xticks(range(1, len(feature_importance_df) + 1))\n",
    "axes[1].set_xticklabels(range(1, len(feature_importance_df) + 1))\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare model with all features vs top features only\n",
    "print(\"\\n--- Predictive Value Test: All Features vs Top 3 ---\")\n",
    "\n",
    "# Model with top 3 features\n",
    "top_features = feature_importance_df.head(3)['feature'].tolist()\n",
    "rf_reduced = RandomForestRegressor(n_estimators=100, max_features='sqrt',\n",
    "                                   random_state=42, n_jobs=-1)\n",
    "rf_reduced.fit(X_train[top_features], y_train)\n",
    "\n",
    "all_features_r2 = rf.score(X_test, y_test)\n",
    "top_features_r2 = rf_reduced.score(X_test[top_features], y_test)\n",
    "\n",
    "print(f\"All 6 features:  Test R² = {all_features_r2:.4f}\")\n",
    "print(f\"Top 3 features:  Test R² = {top_features_r2:.4f}\")\n",
    "print(f\"Performance retained: {(top_features_r2 / all_features_r2) * 100:.1f}%\")\n",
    "\n",
    "print(\"\\n--- Business Insights for Capital City Bikes ---\")\n",
    "top_feature_name = feature_importance_df.iloc[0]['feature']\n",
    "top_feature_importance = feature_importance_df.iloc[0]['importance']\n",
    "\n",
    "print(f\"✓ '{top_feature_name}' dominates with {top_feature_importance:.1%} importance\")\n",
    "print(f\"  → Implication: Temporal patterns drive demand far more than weather\")\n",
    "print(f\"  → Strategy: Optimize fleet positioning by time-of-day schedules\")\n",
    "\n",
    "second_feature = feature_importance_df.iloc[1]['feature']\n",
    "second_importance = feature_importance_df.iloc[1]['importance']\n",
    "print(f\"\\n✓ '{second_feature}' ranks second with {second_importance:.1%} importance\")\n",
    "print(f\"  → Implication: Weather affects demand but is secondary to timing\")\n",
    "\n",
    "low_importance_features = feature_importance_df.tail(2)['feature'].tolist()\n",
    "print(f\"\\n✓ Low importance: {', '.join(low_importance_features)}\")\n",
    "print(f\"  → Implication: These features add marginal value\")\n",
    "print(f\"  → Decision: Keep for small gains, but don't prioritize data quality\")\n",
    "\n",
    "print(f\"\\n✓ Top 3 features retain {(top_features_r2 / all_features_r2) * 100:.1f}% of performance\")\n",
    "print(f\"  → Implication: Dropping 3 features costs ~26% of R² performance\")\n",
    "print(f\"  → Decision: All 6 features provide meaningful predictive value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9021fc8",
   "metadata": {},
   "source": [
    "**What this demonstrates:**\n",
    "- **Feature importance extraction** - accessed via the `.feature_importances_` attribute after training\n",
    "- **Hour dominance** - `hour` accounts for ~49% importance, showing time-of-day is the primary demand driver\n",
    "- **Cumulative analysis** - the top 3 features capture ~79% of predictive power\n",
    "- **Business translation** - importance scores directly inform strategy: Capital City Bikes should prioritize temporal optimization (rush hour fleet positioning) over weather-reactive strategies\n",
    "- **Model simplification tradeoff** - using only the top 3 features retains ~74% of test performance (R² drops from 0.61 to 0.45), a significant loss suggesting all 6 features provide value\n",
    "\n",
    "This feature importance capability makes Random Forest particularly valuable for consulting applications. You're not just predicting demand—you're identifying which factors actually matter, enabling data-driven resource allocation and strategic planning that stakeholders can understand and act upon.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8295df98",
   "metadata": {},
   "source": [
    "## Summary and Transition to Programming Implementation\n",
    "\n",
    "You've mastered essential tree-based modeling foundations: decision tree mechanics, overfitting challenges, ensemble learning principles, and Random Forest architecture. These skills **transform transportation data into sophisticated prediction systems that capture non-linear relationships and conditional patterns** linear models cannot represent.\n",
    "\n",
    "Crucially, you've learned to **balance predictive power with interpretability while recognizing model limitations**. Your Random Forest models achieved ~61% R² on bike demand prediction with 6 basic features—a substantial improvement over linear regression's ~16%—while maintaining business interpretability through feature importance scores. This combination enables competitive advantage without sacrificing stakeholder confidence:\n",
    "- Demonstrated why individual decision trees overfit (training R² 100%, testing R² 61%)\n",
    "- Explained ensemble averaging reduces variance and provides prediction stability\n",
    "- Identified that temporal features (hour) dominate with 60% importance, guiding strategy\n",
    "- Recognized feature limitations: model performance ceiling indicates need for richer predictors (holidays, events, weather patterns)\n",
    "\n",
    "Your ability to build Random Forest models, evaluate performance through proper train-test splitting, interpret feature importance, and compare results with linear baselines prepares you to deploy increasingly sophisticated algorithms. More importantly, you understand that **professional ML development requires strategic algorithm selection**—knowing when linear simplicity suffices and when tree complexity justifies additional sophistication.\n",
    "\n",
    "In the programming example, you'll implement these tree-based concepts through hands-on coding exercises, building complete Random Forest workflows that forecast bike-sharing demand and communicate both model capabilities and business insights to Capital City Bikes stakeholders."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
