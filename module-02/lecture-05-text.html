<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Lecture 05: Text - Predictive Modeling MOOC</title>

    <style>
      body {
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Roboto",
          "Helvetica", "Arial", sans-serif;
        line-height: 1.6;
        max-width: 1200px;
        margin: 0 auto;
        padding: 20px;
        background-color: #f8f9fa;
      }

      .container {
        background-color: white;
        border-radius: 8px;
        padding: 30px;
        box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
      }

      .nav-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 20px 30px;
        border-radius: 8px 8px 0 0;
        margin: -30px -30px 30px -30px;
        display: flex;
        justify-content: space-between;
        align-items: center;
        flex-wrap: wrap;
        gap: 15px;
      }

      .nav-header h1 {
        margin: 0;
        font-size: 1.5rem;
        font-weight: 600;
      }

      .nav-buttons {
        display: flex;
        gap: 10px;
        flex-wrap: wrap;
      }

      .btn {
        display: inline-block;
        padding: 10px 20px;
        text-decoration: none;
        font-weight: 600;
        border-radius: 6px;
        transition: all 0.3s ease;
        font-size: 0.9rem;
      }

      .btn-colab {
        background-color: #f9ab00;
        color: #1a1a1a;
        border: 2px solid #f9ab00;
      }

      .btn-colab:hover {
        background-color: #ff9900;
        border-color: #ff9900;
        transform: translateY(-2px);
        box-shadow: 0 4px 12px rgba(249, 171, 0, 0.3);
      }

      .btn-github {
        background-color: transparent;
        color: white;
        border: 2px solid white;
      }

      .btn-github:hover {
        background-color: white;
        color: #667eea;
        transform: translateY(-2px);
        box-shadow: 0 4px 12px rgba(255, 255, 255, 0.3);
      }

      .jupyter-content {
        margin-top: 20px;
      }

      /* Notebook cell styling */
      div.cell {
        margin-bottom: 1.5rem;
      }

      div.input_area {
        border-left: 3px solid #667eea;
        background-color: #f8f9fa;
        padding: 10px;
        border-radius: 4px;
      }

      div.output_area {
        padding: 10px;
        border-left: 3px solid #28a745;
        background-color: #f8f9fa;
        border-radius: 4px;
        margin-top: 10px;
      }

      /* Code highlighting */
      .highlight {
        background-color: #f8f9fa;
        border-radius: 4px;
      }

      /* Responsive design */
      @media (max-width: 768px) {
        body {
          padding: 10px;
        }

        .container {
          padding: 15px;
        }

        .nav-header {
          padding: 15px;
          margin: -15px -15px 15px -15px;
        }

        .nav-header h1 {
          font-size: 1.2rem;
        }

        .nav-buttons {
          width: 100%;
          justify-content: center;
        }
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="nav-header">
        <h1>Lecture 05: Text</h1>
        <div class="nav-buttons">
          <a
            href="https://colab.research.google.com/github/pmarcelino/predictive-modeling/blob/main/module-02/lecture-05-text.ipynb"
            target="_blank"
            rel="noopener"
            class="btn btn-colab"
          >
            ‚ñ∂ Open in Colab
          </a>
          <a
            href="https://github.com/pmarcelino/predictive-modeling/blob/main/module-02/lecture-05-text.ipynb"
            target="_blank"
            rel="noopener"
            class="btn btn-github"
          >
            üìÇ View on GitHub
          </a>
        </div>
      </div>
      <div class="jupyter-content">
        <main>
          <div class="border-box-sizing" id="notebook" tabindex="-1">
            <div class="container" id="notebook-container">
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=0d00a0ee"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h1
                      id="Lecture-5:-Advanced-Preprocessing-&amp;-Feature-Engineering---Optimizing-Data-for-Machine-Learning"
                      >Lecture 5: Advanced Preprocessing &amp; Feature
                      Engineering - Optimizing Data for Machine Learning<a
                        class="anchor-link"
                        href="#Lecture-5:-Advanced-Preprocessing-&amp;-Feature-Engineering---Optimizing-Data-for-Machine-Learning"
                      ></a
                    ></h1>
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=315bf85b"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h2 id="Learning-Objectives"
                      >Learning Objectives<a
                        class="anchor-link"
                        href="#Learning-Objectives"
                      ></a></h2
                    ><p>By the end of this lecture, you will be able to:</p>
                    <ul>
                      <li
                        >Apply categorical encoding techniques for machine
                        learning compatibility</li
                      >
                      <li
                        >Implement data scaling and normalization strategies for
                        optimal model performance</li
                      >
                      <li
                        >Design and implement time-based features for
                        transportation demand prediction</li
                      >
                    </ul>
                    <hr />
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=b18c57a6"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h2
                      id="1.-From-Clean-Data-to-Machine-Learning-Ready-Features"
                      >1. From Clean Data to Machine Learning Ready Features<a
                        class="anchor-link"
                        href="#1.-From-Clean-Data-to-Machine-Learning-Ready-Features"
                      ></a
                    ></h2>
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=ff3e97b9"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h3 id="1.1.-The-Bridge-Between-Data-and-Models"
                      >1.1. The Bridge Between Data and Models<a
                        class="anchor-link"
                        href="#1.1.-The-Bridge-Between-Data-and-Models"
                      ></a></h3
                    ><p
                      >You now have <strong>clean, reliable data</strong> from
                      your bike-sharing client - but
                      <strong
                        >clean data isn't the same as machine learning ready
                        data</strong
                      >. Raw data variables often need
                      <strong
                        >transformation, combination, and optimization</strong
                      >
                      before machine learning algorithms can use them
                      effectively.</p
                    >
                    <p
                      >In the previous lecture, you learned to identify and
                      handle data quality issues - missing values, outliers, and
                      inconsistencies. That cleaning work ensures you're
                      starting with reliable, trustworthy data. Now, we move to
                      the next essential step:
                      <strong
                        >transforming that clean data into optimized features
                        for machine learning</strong
                      >.</p
                    >
                    <p
                      >Think of this stage like preparing ingredients for a
                      sophisticated recipe. Having fresh, quality ingredients
                      (clean data) is essential, but you still need to chop,
                      season, and combine them in specific ways to create the
                      final dish (predictive model). This preparation involves
                      two complementary activities:</p
                    >
                    <p
                      ><strong>Data preprocessing</strong> adjusts existing
                      variables to formats and scales that algorithms can
                      process efficiently - ensuring numerical features exist on
                      compatible scales so algorithm mechanics don't introduce
                      artificial biases.
                      <strong>Feature engineering</strong> creates new variables
                      that capture patterns and relationships hidden in raw data
                      - transforming simple measurements into sophisticated
                      representations that expose the underlying structure
                      driving predictions.</p
                    >
                    <p
                      >The boundaries between <strong>data cleaning</strong>,
                      <strong>preprocessing</strong>, and
                      <strong>feature engineering</strong> are not always
                      clear-cut. In practice, these steps often overlap, and the
                      exact labels matter less than the outcome:
                      <strong
                        >data that is trustworthy, ready for algorithms, and
                        enriched with predictive signal</strong
                      >.</p
                    >
                    <p
                      >In this lecture, we will focus on three essential
                      transformation techniques:</p
                    >
                    <ol>
                      <li
                        ><strong>Categorical encoding</strong>, which converts
                        qualitative categories into numerical
                        representations,</li
                      >
                      <li
                        ><strong>Numerical scaling and normalization</strong>,
                        which adjust numerical values either to standardized
                        distributions or to a fixed range,</li
                      >
                      <li
                        ><strong>Temporal feature creation</strong>, which
                        extracts meaningful time-based patterns such as
                        seasonality, trends, or hour-of-day effects.</li
                      >
                    </ol>
                    <p
                      >These methods form the backbone of most machine learning
                      workflows, especially in time-sensitive transportation
                      contexts like bike-sharing demand. While many other
                      transformation strategies exist in the broader ML toolkit,
                      mastering these three gives you a strong foundation for
                      preparing real-world datasets.</p
                    >
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=09c5368f"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h3
                      id="1.2.-Understanding-Data-Preprocessing:-Making-Data-Algorithm-Ready"
                      >1.2. Understanding Data Preprocessing: Making Data
                      Algorithm-Ready<a
                        class="anchor-link"
                        href="#1.2.-Understanding-Data-Preprocessing:-Making-Data-Algorithm-Ready"
                      ></a></h3
                    ><p
                      >Machine learning algorithms operate on numerical matrices
                      through mathematical operations like multiplication,
                      addition, and distance calculations. However,
                      transportation data arrives in formats that algorithms
                      cannot process directly - text labels like "Rainy" or
                      "Clear," and numerical values spanning wildly different
                      scales. Data preprocessing transforms this raw data into
                      algorithm-compatible formats while preserving the
                      information needed for accurate predictions.</p
                    >
                    <p
                      >Transportation datasets present specific preprocessing
                      challenges that require careful handling:</p
                    >
                    <ul>
                      <li
                        ><strong>Mixed Data Types</strong>: Transportation
                        systems generate both categorical variables (weather
                        conditions, day types, station locations) and continuous
                        numerical measurements (temperature, humidity, bike
                        counts). Each type requires different preprocessing
                        strategies to become machine learning compatible.</li
                      >
                      <li
                        ><strong>Categorical Relationships</strong>: Not all
                        categories are equal. Weather severity has natural
                        ordering (Clear &lt; Misty &lt; Light Rain &lt; Heavy
                        Rain), while seasons are cyclical labels without
                        inherent hierarchy. Choosing the wrong encoding strategy
                        can introduce false relationships or miss important
                        orderings.</li
                      >
                      <li
                        ><strong>Scale Disparities</strong>: Raw features exist
                        on incompatible numerical scales - temperature ranges
                        from -10¬∞C to 40¬∞C, humidity spans 0-100%, while hourly
                        bike rentals can vary from 1 to 1000+. Without
                        normalization, algorithms incorrectly prioritize
                        features with larger numerical ranges over potentially
                        more predictive features with smaller scales.</li
                      >
                    </ul>
                    <p
                      >Professional preprocessing ensures that algorithm
                      limitations don't distort the transportation patterns
                      you're trying to predict.</p
                    >
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=f35d60f7"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h3
                      id="1.3.-Understanding-Time-Based-Feature-Engineering-in-Transportation"
                      >1.3. Understanding Time-Based Feature Engineering in
                      Transportation<a
                        class="anchor-link"
                        href="#1.3.-Understanding-Time-Based-Feature-Engineering-in-Transportation"
                      ></a></h3
                    ><p
                      >Once data is preprocessed into algorithm-compatible
                      formats, feature engineering creates new variables that
                      capture hidden patterns and relationships in the raw data.
                      While preprocessing ensures algorithms can process your
                      data, feature engineering determines what insights they
                      can extract from it. For transportation demand prediction,
                      time is one of the most critical dimensions requiring
                      sophisticated feature engineering.</p
                    >
                    <p
                      >Transportation demand exhibits temporal complexities that
                      raw timestamps cannot capture directly:</p
                    >
                    <ul>
                      <li
                        ><strong>Cyclical Time Patterns</strong>: Transportation
                        operates on repeating cycles - hourly rush patterns,
                        daily commute rhythms, weekly work schedules, and
                        seasonal vacation periods. Raw timestamps treat these
                        cycles as linear sequences, failing to recognize that 11
                        PM and midnight are adjacent hours, or that December and
                        January are consecutive months. Machine learning
                        algorithms need explicit encoding to understand temporal
                        continuity.</li
                      >
                      <li
                        ><strong>Multi-Scale Temporal Dependencies</strong>:
                        Demand at any moment depends on patterns at multiple
                        time scales simultaneously. Current hour demand relates
                        to yesterday's same hour (daily cycle), last week's same
                        hour (weekly cycle), and recent hours (momentum). Simple
                        timestamp features cannot represent these layered
                        temporal relationships that drive transportation
                        behavior.</li
                      >
                      <li
                        ><strong>Temporal Context and Transitions</strong>: The
                        meaning of any time point depends on its position within
                        larger temporal structures. Friday evening demand
                        differs fundamentally from Monday evening demand despite
                        identical clock times. Similarly, demand evolves
                        systematically as the work week progresses, requiring
                        features that capture position within weekly and
                        seasonal cycles.</li
                      >
                      <li
                        ><strong>Sequential Momentum Effects</strong>:
                        Transportation demand exhibits inertia - high demand
                        periods tend to persist, and transitions between demand
                        states follow predictable patterns. Raw timestamps
                        provide no information about recent demand history or
                        emerging trends that influence near-future
                        predictions.</li
                      >
                    </ul>
                    <p
                      >Professional time-based feature engineering transforms
                      simple timestamps into sophisticated temporal features
                      that capture these cyclical, multi-scale, contextual, and
                      sequential patterns essential for accurate transportation
                      demand forecasting.</p
                    >
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=6cc2ded2"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h2 id="2.-Categorical-Encoding-Strategies"
                      >2. Categorical Encoding Strategies<a
                        class="anchor-link"
                        href="#2.-Categorical-Encoding-Strategies"
                      ></a
                    ></h2>
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=4c6d2041"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h3
                      id="2.1.-Understanding-Categorical-Variables-in-Transportation"
                      >2.1. Understanding Categorical Variables in
                      Transportation<a
                        class="anchor-link"
                        href="#2.1.-Understanding-Categorical-Variables-in-Transportation"
                      ></a></h3
                    ><p
                      >We will start by exploring why categorical variables
                      matter in transportation datasets and how to transform
                      them into useful numerical representations.</p
                    >
                    <p
                      >Most machine learning algorithms rely on numerical
                      operations such as addition, multiplication, and
                      comparison. But transportation data often contains
                      text-based categories like <em>‚ÄúRainy‚Äù</em> or
                      <em>‚ÄúClear.‚Äù</em> Algorithms cannot process these directly
                      ‚Äî we need to
                      <strong>convert them into numbers</strong> while
                      preserving their meaning for prediction.</p
                    >
                    <p
                      >The challenge lies in doing this transformation in a way
                      that <strong>respects the type of category</strong> and
                      the relationships it carries. Not all categorical
                      variables behave the same way: some have natural
                      orderings, while others are just labels.</p
                    >
                    <p
                      ><strong
                        >Examples of Categorical Variables in
                        Bike-Sharing:</strong
                      ></p
                    >
                    <ul>
                      <li
                        ><strong>Weather Conditions</strong>: Clear, misty,
                        light rain, heavy rain</li
                      >
                      <li
                        ><strong>Day Types</strong>: Weekday, weekend,
                        holiday</li
                      >
                      <li
                        ><strong>Seasons</strong>: Spring, summer, fall,
                        winter</li
                      >
                      <li
                        ><strong>Time Periods</strong>: Rush hour, off-peak,
                        late night</li
                      >
                      <li
                        ><strong>Events</strong>: Normal, special event,
                        maintenance period</li
                      >
                    </ul>
                    <p
                      >Each requires a different encoding strategy depending on
                      whether categories are <em>unordered labels</em>,
                      <em>ranked scales</em>, or
                      <em>business-specific conditions</em>.</p
                    >
                    <p
                      >Here we introduce three widely used approaches to
                      categorical encoding: one-hot encoding, ordinal encoding,
                      and binary encoding. Each method fits different types of
                      variables.</p
                    >
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=a410eed6"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h3 id="2.2.-One-Hot-Encoding-for-Nominal-Categories**"
                      >2.2. One-Hot Encoding for Nominal Categories**<a
                        class="anchor-link"
                        href="#2.2.-One-Hot-Encoding-for-Nominal-Categories**"
                      ></a></h3
                    ><p
                      ><strong>Definition</strong>: One-hot encoding creates
                      binary columns for each category, with values 1 (present)
                      or 0 (absent).</p
                    >
                    <p
                      ><strong>When to use</strong>: For categories with
                      <strong>no inherent order</strong> where all options
                      should be treated equally.</p
                    >
                    <p><strong>Python Example:</strong></p>
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing code_cell rendered"
                id="cell-id=d248d0d9"
              >
                <div class="input">
                  <div class="prompt input_prompt">In¬†[46]:</div>
                  <div class="inner_cell">
                    <div class="input_area">
                      <div class="highlight hl-python">
                        <pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="n">data_path</span> <span class="o">=</span> <span class="s2">"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv"</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="p">[</span><span class="s1">'datetime'</span><span class="p">])</span>

<span class="c1"># Ensure 'weather_condition' exists; derive it from numeric 'weather' if needed</span>
<span class="k">if</span> <span class="s1">'weather_condition'</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
    <span class="k">if</span> <span class="s1">'weather'</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
        <span class="n">_weather_map</span> <span class="o">=</span> <span class="p">{</span><span class="mi">1</span><span class="p">:</span> <span class="s1">'Clear'</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="s1">'Misty'</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span> <span class="s1">'Light Rain'</span><span class="p">,</span> <span class="mi">4</span><span class="p">:</span> <span class="s1">'Heavy Rain'</span><span class="p">}</span>
        <span class="n">df</span><span class="p">[</span><span class="s1">'weather_condition'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'weather'</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">_weather_map</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">'category'</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">"Neither 'weather_condition' nor 'weather' columns are present in df."</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Original weather column (as categories):"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">'weather_condition'</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">())</span>

<span class="c1"># One-hot encode weather conditions</span>
<span class="n">weather_encoded</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">'weather_condition'</span><span class="p">],</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">'weather'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">One-hot encoded columns:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">weather_encoded</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Original 1 column ‚Üí </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">weather_encoded</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span><span class="si">}</span><span class="s2"> binary columns"</span><span class="p">)</span>
</pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div class="output_wrapper">
                  <div class="output">
                    <div class="output_area">
                      <div class="prompt"></div>
                      <div
                        class="output_subarea output_stream output_stdout output_text"
                      >
                        <pre>
Original weather column (as categories):
weather_condition
Clear         7192
Misty         2834
Light Rain     859
Heavy Rain       1
Name: count, dtype: int64

One-hot encoded columns:
   weather_Clear  weather_Heavy Rain  weather_Light Rain  weather_Misty
0           True               False               False          False
1           True               False               False          False
2           True               False               False          False
3           True               False               False          False
4           True               False               False          False

Original 1 column ‚Üí 4 binary columns
</pre
                        >
                      </div>
                    </div>
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=81ef3571"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <p
                      >This transformation creates separate binary columns for
                      each weather condition. Where we previously had one column
                      with text values ('Clear', 'Cloudy', 'Light Rain', 'Heavy
                      Rain'), we now have four binary columns (weather_Clear,
                      weather_Cloudy, weather_Light Rain, weather_Heavy Rain)
                      that machine learning algorithms can process directly.</p
                    >
                    <p
                      >Each row has exactly one '1' (<code>True</code>) and the
                      rest '0s' (<code>False</code>), preserving the original
                      information in a numerical format. For bike-sharing demand
                      prediction, this allows the model to learn different
                      demand patterns for each weather type - for example, Clear
                      days might show high recreational usage while Light Rain
                      days might see reduced casual ridership but maintained
                      commuter patterns.</p
                    >
                    <p
                      >The key advantage: "Clear" and "Cloudy" are treated as
                      equally valid categories without implying any ordering or
                      hierarchy between them.</p
                    >
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=5f8dc71e"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h3 id="2.3.-Ordinal-Encoding-for-Ordered-Categories"
                      >2.3. Ordinal Encoding for Ordered Categories<a
                        class="anchor-link"
                        href="#2.3.-Ordinal-Encoding-for-Ordered-Categories"
                      ></a></h3
                    ><p
                      ><strong>Definition</strong>: Ordinal encoding maps
                      categories to numbers that reflect their natural order.</p
                    >
                    <p
                      ><strong>When to use</strong>: For categories where
                      <strong>order matters</strong>, such as severity or
                      ranking.</p
                    >
                    <p><strong>Python Example:</strong></p>
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing code_cell rendered"
                id="cell-id=d068b29e"
              >
                <div class="input">
                  <div class="prompt input_prompt">In¬†[47]:</div>
                  <div class="inner_cell">
                    <div class="input_area">
                      <div class="highlight hl-python">
                        <pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="n">data_path</span> <span class="o">=</span> <span class="s2">"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv"</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="p">[</span><span class="s1">'datetime'</span><span class="p">])</span>

<span class="c1"># Ensure 'weather_condition' exists; derive it from numeric 'weather' if needed</span>
<span class="k">if</span> <span class="s1">'weather_condition'</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
    <span class="k">if</span> <span class="s1">'weather'</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
        <span class="n">_weather_map</span> <span class="o">=</span> <span class="p">{</span><span class="mi">1</span><span class="p">:</span> <span class="s1">'Clear'</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="s1">'Misty'</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span> <span class="s1">'Light Rain'</span><span class="p">,</span> <span class="mi">4</span><span class="p">:</span> <span class="s1">'Heavy Rain'</span><span class="p">}</span>
        <span class="n">df</span><span class="p">[</span><span class="s1">'weather_condition'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'weather'</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">_weather_map</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">'category'</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">"Neither 'weather_condition' nor 'weather' columns are present in df."</span><span class="p">)</span>

<span class="c1"># Encoding weather severity by order</span>
<span class="n">weather_severity_map</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'Clear'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">'Misty'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">'Light Rain'</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s1">'Heavy Rain'</span><span class="p">:</span> <span class="mi">4</span><span class="p">}</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'weather_severity'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'weather_condition'</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">weather_severity_map</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Weather severity column (as ordered numbers):"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">'weather_severity'</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">())</span>
</pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div class="output_wrapper">
                  <div class="output">
                    <div class="output_area">
                      <div class="prompt"></div>
                      <div
                        class="output_subarea output_stream output_stdout output_text"
                      >
                        <pre>
Weather severity column (as ordered numbers):
weather_severity
1    7192
2    2834
3     859
4       1
Name: count, dtype: int64
</pre
                        >
                      </div>
                    </div>
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=4e6b5a58"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <p
                      >This transformation converts text weather conditions into
                      ordered numerical values that preserve their severity
                      relationship. Where we previously had one column with text
                      values ('Clear', 'Misty', 'Light Rain', 'Heavy Rain'), we
                      now have a single numerical column with ordered values (1,
                      2, 3, 4) that machine learning algorithms can process
                      directly while understanding the inherent ordering.</p
                    >
                    <p
                      >Each weather condition maps to a specific number
                      representing its severity level: Clear conditions (most
                      favorable, severity 1) appear 7,192 times in the dataset,
                      Misty conditions (severity 2) appear 2,834 times, Light
                      Rain (severity 3) occurs 859 times, and Heavy Rain (most
                      severe, severity 4) is rare with only 1 occurrence. For
                      bike-sharing demand prediction, this allows the model to
                      understand that deteriorating weather conditions
                      progressively reduce ridership - a unit increase in
                      weather severity (e.g., from Clear to Misty) represents a
                      consistent step toward worse conditions.</p
                    >
                    <p
                      >The key advantage: the model learns that Heavy Rain &gt;
                      Light Rain &gt; Misty &gt; Clear in terms of severity,
                      capturing the natural ordering that affects transportation
                      behavior, unlike one-hot encoding which would treat these
                      conditions as unrelated categories.</p
                    >
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=087ba423"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h3
                      id="2.4.-Binary-Encoding-for-Business-Specific-Conditions"
                      >2.4. Binary Encoding for Business-Specific Conditions<a
                        class="anchor-link"
                        href="#2.4.-Binary-Encoding-for-Business-Specific-Conditions"
                      ></a></h3
                    ><p
                      ><strong>Definition</strong>: Creates simple 0/1 features
                      for key conditions.</p
                    >
                    <p
                      ><strong>When to use</strong>: When certain conditions are
                      <strong>especially important for the business</strong>.</p
                    >
                    <p>Examples in bike-sharing:</p>
                    <ul>
                      <li><code>is_holiday</code>: 1 if holiday, else 0</li>
                      <li
                        ><code>is_weekend</code>: 1 if Saturday/Sunday, else
                        0</li
                      >
                      <li
                        ><code>is_rush_hour</code>: 1 if within rush hour, else
                        0</li
                      >
                      <li
                        ><code>is_good_weather</code>: 1 if clear or mild
                        conditions, else 0</li
                      >
                    </ul>
                    <p><strong>Python Example:</strong></p>
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing code_cell rendered"
                id="cell-id=b0863cb9"
              >
                <div class="input">
                  <div class="prompt input_prompt">In¬†[48]:</div>
                  <div class="inner_cell">
                    <div class="input_area">
                      <div class="highlight hl-python">
                        <pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">data_path</span> <span class="o">=</span> <span class="s2">"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv"</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="p">[</span><span class="s1">'datetime'</span><span class="p">])</span>

<span class="c1"># Binary indicators for business-relevant conditions</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'is_holiday'</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">'holiday'</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="c1"># Weekend: Saturday=5, Sunday=6</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'is_weekend'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'datetime'</span><span class="p">]</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">dayofweek</span><span class="o">.</span><span class="n">isin</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="c1"># Rush hour windows (commuting peaks)</span>
<span class="n">rush_hours</span> <span class="o">=</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">19</span><span class="p">]</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'is_rush_hour'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'datetime'</span><span class="p">]</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">hour</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">rush_hours</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="c1"># Good weather: Clear/Misty considered favorable; support either numeric or text weather columns</span>
<span class="k">if</span> <span class="s1">'weather'</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
    <span class="n">df</span><span class="p">[</span><span class="s1">'is_good_weather'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'weather'</span><span class="p">]</span><span class="o">.</span><span class="n">isin</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="k">elif</span> <span class="s1">'weather_condition'</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
    <span class="n">df</span><span class="p">[</span><span class="s1">'is_good_weather'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'weather_condition'</span><span class="p">]</span><span class="o">.</span><span class="n">isin</span><span class="p">([</span><span class="s1">'Clear'</span><span class="p">,</span> <span class="s1">'Misty'</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">df</span><span class="p">[</span><span class="s1">'is_good_weather'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># fallback if weather info is unavailable</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Sample binary business-condition features (first 10 rows):"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="s1">'datetime'</span><span class="p">,</span> <span class="s1">'is_holiday'</span><span class="p">,</span> <span class="s1">'is_weekend'</span><span class="p">,</span> <span class="s1">'is_rush_hour'</span><span class="p">,</span> <span class="s1">'is_good_weather'</span><span class="p">]]</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">to_string</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
</pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div class="output_wrapper">
                  <div class="output">
                    <div class="output_area">
                      <div class="prompt"></div>
                      <div
                        class="output_subarea output_stream output_stdout output_text"
                      >
                        <pre>
Sample binary business-condition features (first 10 rows):
           datetime  is_holiday  is_weekend  is_rush_hour  is_good_weather
2011-01-01 00:00:00           0           1             0                1
2011-01-01 01:00:00           0           1             0                1
2011-01-01 02:00:00           0           1             0                1
2011-01-01 03:00:00           0           1             0                1
2011-01-01 04:00:00           0           1             0                1
2011-01-01 05:00:00           0           1             0                1
2011-01-01 06:00:00           0           1             0                1
2011-01-01 07:00:00           0           1             1                1
2011-01-01 08:00:00           0           1             1                1
2011-01-01 09:00:00           0           1             1                1
</pre
                        >
                      </div>
                    </div>
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=25871ffd"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <p
                      >This transformation creates focused binary indicators
                      that flag business-critical conditions. Where we
                      previously needed to check multiple columns and apply
                      business logic (e.g., "Is this Saturday or Sunday?" or
                      "Does the hour fall in 7-9 or 17-19?"), we now have simple
                      0/1 flags (is_weekend, is_rush_hour, is_good_weather,
                      is_holiday) that machine learning algorithms can process
                      directly.</p
                    >
                    <p
                      >Examining January 1, 2011 (the dataset start): this
                      Saturday is correctly flagged as weekend (is_weekend=1)
                      throughout the day, not a holiday (is_holiday=0), and
                      shows good weather conditions (is_good_weather=1). The
                      rush hour indicator activates appropriately at 7 AM, 8 AM,
                      and 9 AM (is_rush_hour=1), even on this weekend day -
                      capturing that some commuters work weekends and morning
                      activity patterns persist. For bike-sharing demand
                      prediction, this allows the model to learn distinct
                      patterns for each condition combination - for example,
                      weekend mornings with good weather drive recreational
                      usage, while weekday rush hours with poor weather
                      concentrate demand around transit stations.</p
                    >
                    <p
                      >The key advantage: binary flags make complex business
                      rules immediately accessible to the model without
                      requiring it to rediscover these domain-specific condition
                      definitions, allowing faster learning of operationally
                      relevant patterns.</p
                    >
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=17927771"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h2
                      id="3.-Scaling-and-Normalization-for-Optimal-Performance"
                      >3. Scaling and Normalization for Optimal Performance<a
                        class="anchor-link"
                        href="#3.-Scaling-and-Normalization-for-Optimal-Performance"
                      ></a></h2
                    ><p
                      >You may have sophisticated engineered features from your
                      bike-sharing data - but having great features isn't enough
                      if they can't work together effectively. Raw feature
                      values often exist on completely different scales,
                      creating a hidden problem that can sabotage your machine
                      learning models.</p
                    >
                    <p
                      >Consider the magnitude differences in your Washington
                      D.C. bike-sharing dataset:</p
                    >
                    <ul>
                      <li
                        ><strong>Temperature</strong>: -10 to 40¬∞C (range of 50
                        units)</li
                      >
                      <li
                        ><strong>Humidity</strong>: 0 to 100% (range of 100
                        units)</li
                      >
                      <li
                        ><strong>Bike demand</strong>: 1 to 1000+ rentals per
                        hour (range of 1000+ units)</li
                      >
                      <li
                        ><strong>Hour of day</strong>: 0 to 23 (range of 24
                        units)</li
                      >
                    </ul>
                    <p
                      >Without proper scaling, machine learning algorithms
                      naturally give more weight to features with larger
                      numerical ranges. In this case, bike count values in the
                      hundreds will overshadow temperature changes in the tens -
                      even when temperature might be more predictive of future
                      demand.</p
                    >
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=930a4165"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h3 id="3.1.-Scaling-and-Normalization-Fundamentals"
                      >3.1. Scaling and Normalization Fundamentals<a
                        class="anchor-link"
                        href="#3.1.-Scaling-and-Normalization-Fundamentals"
                      ></a></h3
                    ><p
                      >Before exploring specific techniques, let's establish the
                      main definitions:</p
                    >
                    <ul>
                      <li
                        ><strong>Normalization</strong> reshapes distributions
                        to fit standardized ranges or statistical properties</li
                      >
                      <li
                        ><strong>Scaling</strong> adjusts feature value ranges
                        to enable fair comparison across variables</li
                      >
                    </ul>
                    <p
                      >A key principle here is that
                      <strong
                        >these transformations preserve underlying data patterns
                        while standardizing numerical representation</strong
                      >.</p
                    >
                    <p
                      >Professional scaling ensures features contribute based on
                      <em>predictive importance</em> rather than arbitrary
                      numerical scale, keeping your models aligned with real
                      transportation dynamics. Consider a Tuesday 3 PM demand
                      prediction request from your client. Without proper
                      scaling, your model might overweight historical bike
                      counts (values in the hundreds) while underweighting a
                      forecasted 15-degree temperature drop (numerically smaller
                      change). The result: inflated demand predictions and
                      misallocated bike fleet resources.</p
                    >
                    <p
                      >Now that you know <em>why</em> scaling matters, let's
                      explore the two core methods every transportation
                      consultant should master: StandardScaler and MinMaxScaler.
                      Each one transforms the data differently and is best
                      suited for particular feature types.</p
                    >
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=3ca94a33"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h3 id="3.2.-StandardScaler:-Statistical-Normalization"
                      >3.2. StandardScaler: Statistical Normalization<a
                        class="anchor-link"
                        href="#3.2.-StandardScaler:-Statistical-Normalization"
                      ></a></h3
                    ><p
                      ><strong>Definition:</strong> StandardScaler (Z-score
                      normalization) standardizes features so they have mean 0
                      and standard deviation 1. This preserves the distribution
                      shape but makes features comparable on a common
                      statistical scale.</p
                    >
                    <p
                      ><strong>Formula:</strong> $$scaled\_value =
                      \frac{original\_value - mean}{standard\_deviation}$$</p
                    >
                    <p><strong>Python Example:</strong></p>
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing code_cell rendered"
                id="cell-id=1aeb1c43"
              >
                <div class="input">
                  <div class="prompt input_prompt">In¬†[49]:</div>
                  <div class="inner_cell">
                    <div class="input_area">
                      <div class="highlight hl-python">
                        <pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="n">data_path</span> <span class="o">=</span> <span class="s2">"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv"</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="p">[</span><span class="s1">'datetime'</span><span class="p">])</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">numerical_columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'temp'</span><span class="p">,</span> <span class="s1">'atemp'</span><span class="p">,</span> <span class="s1">'humidity'</span><span class="p">,</span> <span class="s1">'windspeed'</span><span class="p">,</span> <span class="s1">'casual'</span><span class="p">,</span> <span class="s1">'registered'</span><span class="p">]</span>

<span class="c1"># Show before/after comparison</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Original values (first 5 rows):"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">numerical_columns</span><span class="p">]</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>

<span class="n">df_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">numerical_columns</span><span class="p">])</span>
<span class="n">df_scaled_display</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">df_scaled</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">numerical_columns</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Scaled values (first 5 rows):"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df_scaled_display</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Scaled statistics:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Mean: </span><span class="si">{</span><span class="n">df_scaled</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>  <span class="c1"># Should be ~0</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Std: </span><span class="si">{</span><span class="n">df_scaled</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>     <span class="c1"># Should be ~1</span>
</pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div class="output_wrapper">
                  <div class="output">
                    <div class="output_area">
                      <div class="prompt"></div>
                      <div
                        class="output_subarea output_stream output_stdout output_text"
                      >
                        <pre>
Original values (first 5 rows):
   temp   atemp  humidity  windspeed  casual  registered
0  9.84  14.395        81        0.0       3          13
1  9.02  13.635        80        0.0       8          32
2  9.02  13.635        80        0.0       5          27
3  9.84  14.395        75        0.0       3          10
4  9.84  14.395        75        0.0       0           1

Scaled values (first 5 rows):
       temp     atemp  humidity  windspeed    casual  registered
0 -1.333661 -1.092737  0.993213  -1.567754 -0.660992   -0.943854
1 -1.438907 -1.182421  0.941249  -1.567754 -0.560908   -0.818052
2 -1.438907 -1.182421  0.941249  -1.567754 -0.620958   -0.851158
3 -1.333661 -1.092737  0.681430  -1.567754 -0.660992   -0.963717
4 -1.333661 -1.092737  0.681430  -1.567754 -0.721042   -1.023307

Scaled statistics:
Mean: [-0.  0.  0. -0.  0. -0.]
Std: [1. 1. 1. 1. 1. 1.]
</pre
                        >
                      </div>
                    </div>
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=360da4fc"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <p
                      >Notice how StandardScaler transforms the features to have
                      mean ‚âà 0 and standard deviation = 1. Temperature values
                      that ranged from 0-40¬∞C now center around 0, with most
                      values falling between -2 and +2. This puts temperature,
                      humidity, and bike counts on the same statistical scale,
                      ensuring the model treats them fairly rather than
                      over-weighting variables with larger raw values.</p
                    >
                    <p
                      >The transformation preserves relationships within each
                      feature while making them directly comparable. For
                      bike-sharing prediction, this ensures that a 10-point
                      change in humidity has a similar numerical weight as a
                      10-degree change in temperature, allowing the model to
                      learn which features are truly most predictive.</p
                    >
                    <p><strong>When to Use:</strong></p>
                    <ul>
                      <li>Weather variables with near-normal distributions</li>
                      <li>Linear regression or neural networks</li>
                      <li>Situations without extreme outliers</li>
                    </ul>
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=7e392475"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h3 id="3.3.-MinMaxScaler:-Bounded-Range-Normalization"
                      >3.3. MinMaxScaler: Bounded Range Normalization<a
                        class="anchor-link"
                        href="#3.3.-MinMaxScaler:-Bounded-Range-Normalization"
                      ></a></h3
                    ><p
                      ><strong>Definition:</strong> MinMaxScaler rescales values
                      into a defined range, usually 0‚Äì1. This ensures all values
                      fit within a predictable bound.</p
                    >
                    <p
                      ><strong>Formula:</strong> $$scaled\_value =
                      \frac{original\_value - min}{max - min}$$</p
                    >
                    <p><strong>Python Example:</strong></p>
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing code_cell rendered"
                id="cell-id=fa427c2e"
              >
                <div class="input">
                  <div class="prompt input_prompt">In¬†[50]:</div>
                  <div class="inner_cell">
                    <div class="input_area">
                      <div class="highlight hl-python">
                        <pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">MinMaxScaler</span>

<span class="n">data_path</span> <span class="o">=</span> <span class="s2">"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv"</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="p">[</span><span class="s1">'datetime'</span><span class="p">])</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">(</span><span class="n">feature_range</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">numerical_columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'temp'</span><span class="p">,</span> <span class="s1">'atemp'</span><span class="p">,</span> <span class="s1">'humidity'</span><span class="p">,</span> <span class="s1">'windspeed'</span><span class="p">,</span> <span class="s1">'casual'</span><span class="p">,</span> <span class="s1">'registered'</span><span class="p">]</span>

<span class="c1"># Show before</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Original min/max (per column):"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">numerical_columns</span><span class="p">]</span><span class="o">.</span><span class="n">agg</span><span class="p">([</span><span class="s1">'min'</span><span class="p">,</span><span class="s1">'max'</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

<span class="c1"># Fit &amp; transform</span>
<span class="n">mm_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">numerical_columns</span><span class="p">])</span>
<span class="n">mm_scaled_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">mm_scaled</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">numerical_columns</span><span class="p">)</span>

<span class="c1"># Show after</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Scaled min/max (should be ~0 and ~1):"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">mm_scaled_df</span><span class="o">.</span><span class="n">agg</span><span class="p">([</span><span class="s1">'min'</span><span class="p">,</span><span class="s1">'max'</span><span class="p">])</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

<span class="c1"># Optional: invert back to original scale (sanity check)</span>
<span class="n">mm_inverted</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">mm_scaled_df</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Inverse-transformed (first 5 rows) matches original scale:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">mm_inverted</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">numerical_columns</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
</pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div class="output_wrapper">
                  <div class="output">
                    <div class="output_area">
                      <div class="prompt"></div>
                      <div
                        class="output_subarea output_stream output_stdout output_text"
                      >
                        <pre>
Original min/max (per column):
             min       max
temp        0.82   41.0000
atemp       0.76   45.4550
humidity    0.00  100.0000
windspeed   0.00   56.9969
casual      0.00  367.0000
registered  0.00  886.0000

Scaled min/max (should be ~0 and ~1):
            min  max
temp        0.0  1.0
atemp       0.0  1.0
humidity    0.0  1.0
windspeed   0.0  1.0
casual      0.0  1.0
registered  0.0  1.0

Inverse-transformed (first 5 rows) matches original scale:
   temp   atemp  humidity  windspeed  casual  registered
0  9.84  14.395      81.0        0.0     3.0        13.0
1  9.02  13.635      80.0        0.0     8.0        32.0
2  9.02  13.635      80.0        0.0     5.0        27.0
3  9.84  14.395      75.0        0.0     3.0        10.0
4  9.84  14.395      75.0        0.0     0.0         1.0
</pre
                        >
                      </div>
                    </div>
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=9c26853a"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <p
                      >Notice how MinMaxScaler compresses all values into the
                      0‚Äì1 range while preserving the relative distances between
                      data points. Temperature values that originally ranged
                      from 0-40¬∞C are now squeezed between 0 and 1, with the
                      minimum temperature mapping to 0 and the maximum to 1.
                      This bounded transformation is particularly useful when
                      you need predictable value ranges.</p
                    >
                    <p
                      >The transformation maintains the original distribution
                      shape and outlier patterns, simply rescaling them to fit
                      the target range. For bike-sharing prediction, this
                      ensures that features with different natural scales‚Äîlike
                      temperature (0-40¬∞C) and humidity (0-100%)‚Äîare all mapped
                      to the same 0-1 range, preventing any single feature from
                      dominating due to its larger numerical scale. The
                      inverse_transform function allows you to convert
                      predictions back to the original scale for
                      interpretation.</p
                    >
                    <p><strong>When to Use:</strong></p>
                    <ul>
                      <li>Time-based features (e.g., hour of day)</li>
                      <li>Variables with natural bounds (e.g., percentages)</li>
                      <li>Algorithms sensitive to bounded inputs</li>
                    </ul>
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=10cbf3fe"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h2
                      id="4.-Time-Based-Feature-Engineering-for-Transportation"
                      >4. Time-Based Feature Engineering for Transportation<a
                        class="anchor-link"
                        href="#4.-Time-Based-Feature-Engineering-for-Transportation"
                      ></a
                    ></h2>
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=4a946e5b"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h3
                      id="4.1.-Extracting-Temporal-Intelligence-from-Timestamps"
                      >4.1. Extracting Temporal Intelligence from Timestamps<a
                        class="anchor-link"
                        href="#4.1.-Extracting-Temporal-Intelligence-from-Timestamps"
                      ></a></h3
                    ><p
                      >Time is the most important dimension in transportation
                      data, but raw timestamps contain hidden patterns that must
                      be extracted and transformed to be useful for machine
                      learning.</p
                    >
                    <p
                      >Think about how a clock works - after 11 PM comes
                      midnight (12 AM), but to a computer, these look like
                      completely different numbers (23 and 0). This creates a
                      problem: the computer thinks 11 PM and midnight are far
                      apart, when they're actually next to each other on the
                      clock. Similarly, transportation demand operates
                      simultaneously at multiple temporal scales - hourly rush
                      patterns, weekly commute cycles, and seasonal variations -
                      all requiring specialized feature engineering
                      approaches.</p
                    >
                    <p
                      >In this section, we explore four essential temporal
                      feature engineering techniques that transform raw
                      timestamps into predictive features for bike-sharing
                      demand forecasting:</p
                    >
                    <ol>
                      <li
                        ><strong
                          >Cyclical encoding for continuous time</strong
                        ></li
                      >
                      <li><strong>Time-since features</strong></li>
                      <li><strong>Temporal aggregation features</strong></li>
                      <li
                        ><strong
                          >Lag features for sequential patterns</strong
                        ></li
                      >
                    </ol>
                    <p
                      >Each technique captures different temporal patterns that
                      drive transportation demand.</p
                    >
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=ad9dd2d4"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h3 id="4.2.-Cyclical-Encoding-for-Continuous-Time"
                      >4.2. Cyclical Encoding for Continuous Time<a
                        class="anchor-link"
                        href="#4.2.-Cyclical-Encoding-for-Continuous-Time"
                      ></a></h3
                    ><p
                      ><strong>Definition</strong>: Cyclical encoding transforms
                      linear time values (hours, days, months) into circular
                      representations using sine and cosine functions, ensuring
                      that adjacent time points remain close in the feature
                      space.</p
                    >
                    <p
                      ><strong>Purpose</strong>: This technique prevents
                      artificial breaks in temporal data - hour 23 and hour 0
                      are neighbors on a clock, and models should treat them as
                      such.</p
                    >
                    <p><strong>Python Example</strong>:</p>
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing code_cell rendered"
                id="cell-id=d0bba649"
              >
                <div class="input">
                  <div class="prompt input_prompt">In¬†[51]:</div>
                  <div class="inner_cell">
                    <div class="input_area">
                      <div class="highlight hl-python">
                        <pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="n">data_path</span> <span class="o">=</span> <span class="s2">"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv"</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="p">[</span><span class="s1">'datetime'</span><span class="p">])</span>

<span class="c1"># Extract hour from timestamp</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'hour'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'datetime'</span><span class="p">]</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">hour</span>

<span class="c1"># Create cyclical encoding for 24-hour cycle</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'hour_sin'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">df</span><span class="p">[</span><span class="s1">'hour'</span><span class="p">]</span> <span class="o">/</span> <span class="mi">24</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'hour_cos'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">df</span><span class="p">[</span><span class="s1">'hour'</span><span class="p">]</span> <span class="o">/</span> <span class="mi">24</span><span class="p">)</span>

<span class="c1"># Compare hour 23 vs hour 0</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Hour 23 encoding:"</span><span class="p">,</span> <span class="sa">f</span><span class="s2">"sin=</span><span class="si">{</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">'hour'</span><span class="p">]</span><span class="o">==</span><span class="mi">23</span><span class="p">][</span><span class="s1">'hour_sin'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, cos=</span><span class="si">{</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">'hour'</span><span class="p">]</span><span class="o">==</span><span class="mi">23</span><span class="p">][</span><span class="s1">'hour_cos'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Hour 0 encoding: "</span><span class="p">,</span> <span class="sa">f</span><span class="s2">"sin=</span><span class="si">{</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">'hour'</span><span class="p">]</span><span class="o">==</span><span class="mi">0</span><span class="p">][</span><span class="s1">'hour_sin'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, cos=</span><span class="si">{</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">'hour'</span><span class="p">]</span><span class="o">==</span><span class="mi">0</span><span class="p">][</span><span class="s1">'hour_cos'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div class="output_wrapper">
                  <div class="output">
                    <div class="output_area">
                      <div class="prompt"></div>
                      <div
                        class="output_subarea output_stream output_stdout output_text"
                      >
                        <pre>
Hour 23 encoding: sin=-0.259, cos=0.966
Hour 0 encoding:  sin=0.000, cos=1.000
</pre
                        >
                      </div>
                    </div>
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=4c05ffe3"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <p
                      >The formula <code>2 * œÄ * hour / 24</code> maps the
                      24-hour cycle onto a circle. We use both sine and cosine
                      because together they uniquely identify any point on the
                      circle. Hour 0 maps to (sin=0, cos=1), hour 6 to (sin=1,
                      cos=0), hour 12 to (sin=0, cos=-1), and hour 18 to
                      (sin=-1, cos=0). Critically, hour 23 maps to approximately
                      (sin=-0.259, cos=0.966), very close to hour 0's
                      position.</p
                    >
                    <p
                      >For bike-sharing demand prediction, this encoding allows
                      the model to learn that late-night hours (22-23) and
                      early-morning hours (0-1) share similar low-demand
                      patterns. Without cyclical encoding, a model would
                      incorrectly assume hour 23 is as different from hour 0 as
                      hour 0 is from hour 23, missing the continuous flow of
                      nighttime demand patterns.</p
                    >
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=2cfc0ba6"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h3 id="4.3.-Time-Since-Features"
                      >4.3. Time-Since Features<a
                        class="anchor-link"
                        href="#4.3.-Time-Since-Features"
                      ></a></h3
                    ><p
                      ><strong>Definition</strong>: Time-since features measure
                      the elapsed time between the current observation and a
                      meaningful reference point, such as the last weekend,
                      holiday, or weather event.</p
                    >
                    <p
                      ><strong>Purpose</strong>: These features capture recovery
                      patterns and transition effects that influence
                      transportation demand after significant events.</p
                    >
                    <p><strong>Python Example</strong>:</p>
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing code_cell rendered"
                id="cell-id=4702e6a4"
              >
                <div class="input">
                  <div class="prompt input_prompt">In¬†[52]:</div>
                  <div class="inner_cell">
                    <div class="input_area">
                      <div class="highlight hl-python">
                        <pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="n">data_path</span> <span class="o">=</span> <span class="s2">"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv"</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="p">[</span><span class="s1">'datetime'</span><span class="p">])</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">'datetime'</span><span class="p">)</span>

<span class="c1"># Calculate days since last weekend (Monday=0, Sunday=6)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'day_of_week'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'datetime'</span><span class="p">]</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">dayofweek</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'is_weekend'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'day_of_week'</span><span class="p">]</span><span class="o">.</span><span class="n">isin</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="c1"># Simple calculation: Monday=1, Tuesday=2, ..., Friday=5, Sat/Sun=0</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'days_since_weekend'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'day_of_week'</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="mi">5</span> <span class="k">else</span> <span class="mi">0</span>
<span class="p">)</span>

<span class="c1"># Show demand pattern by days since weekend</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Average demand by days since weekend:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">'days_since_weekend'</span><span class="p">)[</span><span class="s1">'count'</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
</pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div class="output_wrapper">
                  <div class="output">
                    <div class="output_area">
                      <div class="prompt"></div>
                      <div
                        class="output_subarea output_stream output_stdout output_text"
                      >
                        <pre>
Average demand by days since weekend:
days_since_weekend
0    188.8
1    190.4
2    189.7
3    188.4
4    197.3
5    197.8
Name: count, dtype: float64
</pre
                        >
                      </div>
                    </div>
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=62e8f7a5"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <p
                      >We created a feature that tracks how many days have
                      passed since the last weekend ended. Monday gets value 1,
                      Tuesday gets 2, and so on through Friday (value 5), while
                      weekends themselves get 0. This captures the weekly rhythm
                      where demand patterns evolve as the work week
                      progresses.</p
                    >
                    <p
                      >Analysis reveals that bike-sharing demand builds
                      throughout the work week. Monday through Wednesday (1-3
                      days since weekend) show relatively stable baseline demand
                      around 189-190 bikes per hour. Demand increases notably on
                      Thursday and Friday (4-5 days), reaching nearly 198 bikes
                      per hour - about 5% higher than mid-week. This pattern
                      suggests people may make more trips toward the end of the
                      work week, possibly combining commutes with after-work
                      activities or weekend preparation. Operators should ensure
                      higher bike availability at stations on Thursday-Friday
                      afternoons.</p
                    >
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=1753717b"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h3 id="4.4.-Temporal-Aggregation-Features"
                      >4.4. Temporal Aggregation Features<a
                        class="anchor-link"
                        href="#4.4.-Temporal-Aggregation-Features"
                      ></a></h3
                    ><p
                      ><strong>Definition</strong>: Temporal aggregation
                      features summarize past values of a variable over a
                      defined time window, providing context about recent trends
                      and stability.</p
                    >
                    <p
                      ><strong>Purpose</strong>: They help models understand
                      whether demand has been rising or falling, and whether
                      conditions have been stable or volatile.</p
                    >
                    <p><strong>Python Example</strong>:</p>
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing code_cell rendered"
                id="cell-id=7c2080eb"
              >
                <div class="input">
                  <div class="prompt input_prompt">In¬†[53]:</div>
                  <div class="inner_cell">
                    <div class="input_area">
                      <div class="highlight hl-python">
                        <pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="n">data_path</span> <span class="o">=</span> <span class="s2">"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv"</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="p">[</span><span class="s1">'datetime'</span><span class="p">])</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">'datetime'</span><span class="p">)</span>

<span class="c1"># Create 3-hour rolling average demand</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'demand_3h_avg'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'count'</span><span class="p">]</span><span class="o">.</span><span class="n">rolling</span><span class="p">(</span><span class="n">window</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">min_periods</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="c1"># Create 24-hour rolling average demand</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'demand_24h_avg'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'count'</span><span class="p">]</span><span class="o">.</span><span class="n">rolling</span><span class="p">(</span><span class="n">window</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span> <span class="n">min_periods</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="c1"># Compare current demand to 24-hour average (momentum indicator)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'demand_momentum'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'count'</span><span class="p">]</span> <span class="o">-</span> <span class="n">df</span><span class="p">[</span><span class="s1">'demand_24h_avg'</span><span class="p">]</span>

<span class="c1"># Show example</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Sample temporal aggregation features:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="s1">'datetime'</span><span class="p">,</span> <span class="s1">'count'</span><span class="p">,</span> <span class="s1">'demand_3h_avg'</span><span class="p">,</span> <span class="s1">'demand_24h_avg'</span><span class="p">,</span> <span class="s1">'demand_momentum'</span><span class="p">]]</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">30</span><span class="p">)</span><span class="o">.</span><span class="n">to_string</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
</pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div class="output_wrapper">
                  <div class="output">
                    <div class="output_area">
                      <div class="prompt"></div>
                      <div
                        class="output_subarea output_stream output_stdout output_text"
                      >
                        <pre>
Sample temporal aggregation features:
           datetime  count  demand_3h_avg  demand_24h_avg  demand_momentum
2011-01-01 00:00:00     16      16.000000       16.000000         0.000000
2011-01-01 01:00:00     40      28.000000       28.000000        12.000000
2011-01-01 02:00:00     32      29.333333       29.333333         2.666667
2011-01-01 03:00:00     13      28.333333       25.250000       -12.250000
2011-01-01 04:00:00      1      15.333333       20.400000       -19.400000
2011-01-01 05:00:00      1       5.000000       17.166667       -16.166667
2011-01-01 06:00:00      2       1.333333       15.000000       -13.000000
2011-01-01 07:00:00      3       2.000000       13.500000       -10.500000
2011-01-01 08:00:00      8       4.333333       12.888889        -4.888889
2011-01-01 09:00:00     14       8.333333       13.000000         1.000000
2011-01-01 10:00:00     36      19.333333       15.090909        20.909091
2011-01-01 11:00:00     56      35.333333       18.500000        37.500000
2011-01-01 12:00:00     84      58.666667       23.538462        60.461538
2011-01-01 13:00:00     94      78.000000       28.571429        65.428571
2011-01-01 14:00:00    106      94.666667       33.733333        72.266667
2011-01-01 15:00:00    110     103.333333       38.500000        71.500000
2011-01-01 16:00:00     93     103.000000       41.705882        51.294118
2011-01-01 17:00:00     67      90.000000       43.111111        23.888889
2011-01-01 18:00:00     35      65.000000       42.684211        -7.684211
2011-01-01 19:00:00     37      46.333333       42.400000        -5.400000
2011-01-01 20:00:00     36      36.000000       42.095238        -6.095238
2011-01-01 21:00:00     34      35.666667       41.727273        -7.727273
2011-01-01 22:00:00     28      32.666667       41.130435       -13.130435
2011-01-01 23:00:00     39      33.666667       41.041667        -2.041667
2011-01-02 00:00:00     17      28.000000       41.083333       -24.083333
2011-01-02 01:00:00     17      24.333333       40.125000       -23.125000
2011-01-02 02:00:00      9      14.333333       39.166667       -30.166667
2011-01-02 03:00:00      6      10.666667       38.875000       -32.875000
2011-01-02 04:00:00      3       6.000000       38.958333       -35.958333
2011-01-02 06:00:00      2       3.666667       39.000000       -37.000000
</pre
                        >
                      </div>
                    </div>
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=e5bf104c"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <p
                      >We created rolling window features that summarize recent
                      demand history. The 3-hour average captures immediate
                      trends - if demand has been building over the past few
                      hours, this average will be rising. The 24-hour average
                      provides daily context, smoothing out hourly fluctuations.
                      The momentum indicator (current - 24h average) shows
                      whether current demand is above or below typical levels
                      for this time of day.</p
                    >
                    <p
                      >These aggregation features help distinguish between
                      sustained demand trends and temporary spikes. For example,
                      if current demand is 200 bikes/hour but the 3-hour average
                      is 150, this suggests demand is accelerating - perhaps
                      weather improved or an event started. Conversely, if
                      current demand is 100 but the 3-hour average is 150,
                      demand is declining and bike rebalancing can be delayed.
                      The 24-hour average is particularly valuable for detecting
                      anomalies: when current demand deviates significantly from
                      the 24-hour norm (momentum &gt; ¬±50), it signals special
                      conditions requiring operational attention.</p
                    >
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=2e5c5298"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h3 id="4.5.-Lag-Features-for-Sequential-Patterns"
                      >4.5. Lag Features for Sequential Patterns<a
                        class="anchor-link"
                        href="#4.5.-Lag-Features-for-Sequential-Patterns"
                      ></a></h3
                    ><p
                      ><strong>Definition</strong>: Lag features use values from
                      previous time steps as predictors for the current
                      observation, explicitly introducing historical patterns
                      into the model.</p
                    >
                    <p
                      ><strong>Purpose</strong>: They capture the sequential
                      dependencies and recurring cycles that characterize
                      transportation demand.</p
                    >
                    <p><strong>Python Example</strong>:</p>
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing code_cell rendered"
                id="cell-id=bb183b50"
              >
                <div class="input">
                  <div class="prompt input_prompt">In¬†[54]:</div>
                  <div class="inner_cell">
                    <div class="input_area">
                      <div class="highlight hl-python">
                        <pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="n">data_path</span> <span class="o">=</span> <span class="s2">"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv"</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="p">[</span><span class="s1">'datetime'</span><span class="p">])</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">'datetime'</span><span class="p">)</span>

<span class="c1"># Create lag features at different time scales</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'demand_lag_1h'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'count'</span><span class="p">]</span><span class="o">.</span><span class="n">shift</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>      <span class="c1"># 1 hour ago</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'demand_lag_24h'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'count'</span><span class="p">]</span><span class="o">.</span><span class="n">shift</span><span class="p">(</span><span class="mi">24</span><span class="p">)</span>    <span class="c1"># Same hour yesterday</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'demand_lag_7d'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'count'</span><span class="p">]</span><span class="o">.</span><span class="n">shift</span><span class="p">(</span><span class="mi">24</span> <span class="o">*</span> <span class="mi">7</span><span class="p">)</span> <span class="c1"># Same hour last week</span>

<span class="c1"># Calculate correlations to understand predictive power</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Lag feature correlations with current demand:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"1-hour lag:  </span><span class="si">{</span><span class="n">df</span><span class="p">[</span><span class="s1">'count'</span><span class="p">]</span><span class="o">.</span><span class="n">corr</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">'demand_lag_1h'</span><span class="p">])</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"24-hour lag: </span><span class="si">{</span><span class="n">df</span><span class="p">[</span><span class="s1">'count'</span><span class="p">]</span><span class="o">.</span><span class="n">corr</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">'demand_lag_24h'</span><span class="p">])</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"7-day lag:   </span><span class="si">{</span><span class="n">df</span><span class="p">[</span><span class="s1">'count'</span><span class="p">]</span><span class="o">.</span><span class="n">corr</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">'demand_lag_7d'</span><span class="p">])</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="c1"># Show example for Saturday morning</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Saturday 7 AM demand patterns:"</span><span class="p">)</span>
<span class="n">saturday_7am</span> <span class="o">=</span> <span class="n">df</span><span class="p">[(</span><span class="n">df</span><span class="p">[</span><span class="s1">'datetime'</span><span class="p">]</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">dayofweek</span> <span class="o">==</span> <span class="mi">5</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">'datetime'</span><span class="p">]</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">hour</span> <span class="o">==</span> <span class="mi">7</span><span class="p">)]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">saturday_7am</span><span class="p">[[</span><span class="s1">'datetime'</span><span class="p">,</span> <span class="s1">'count'</span><span class="p">,</span> <span class="s1">'demand_lag_1h'</span><span class="p">,</span> <span class="s1">'demand_lag_24h'</span><span class="p">,</span> <span class="s1">'demand_lag_7d'</span><span class="p">]]</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">to_string</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
</pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div class="output_wrapper">
                  <div class="output">
                    <div class="output_area">
                      <div class="prompt"></div>
                      <div
                        class="output_subarea output_stream output_stdout output_text"
                      >
                        <pre>
Lag feature correlations with current demand:
1-hour lag:  0.842
24-hour lag: 0.811
7-day lag:   0.786

Saturday 7 AM demand patterns:
           datetime  count  demand_lag_1h  demand_lag_24h  demand_lag_7d
2011-01-01 07:00:00      3            2.0             NaN            NaN
2011-01-08 07:00:00      9            2.0            84.0           16.0
2011-01-15 07:00:00     10            3.0            70.0           16.0
2011-02-05 07:00:00      4            4.0            87.0          113.0
2011-02-12 07:00:00     11            2.0            74.0           11.0
</pre
                        >
                      </div>
                    </div>
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=02cdee69"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <p
                      >We created three lag features at different time scales.
                      The 1-hour lag captures immediate momentum - high demand
                      often persists for several hours. The 24-hour lag captures
                      daily repetition - 8 AM today resembles 8 AM yesterday.
                      The 7-day lag captures weekly cycles - Monday patterns
                      repeat week after week. The <code>.shift()</code> function
                      moves values backward in time, making historical demand
                      available as features for prediction.</p
                    >
                    <p
                      >The correlation analysis reveals that 1-hour lags
                      (correlation ‚âà 0.84) are the strongest predictors for
                      bike-sharing demand, as demand in consecutive hours tends
                      to be very similar. The 24-hour lag (correlation ‚âà 0.81)
                      is nearly as strong, reflecting the dominant daily cycle
                      in urban transportation. The Saturday 7 AM example
                      demonstrates the value of the 7-day lag (correlation ‚âà
                      0.79): demand on one Saturday (9 bikes) is much closer to
                      the previous Saturday (16 bikes) than to Friday 7 AM (84
                      bikes), showing that weekly patterns are crucial for
                      weekend predictions.</p
                    >
                    <hr />
                  </div>
                </div>
              </div>
              <div
                class="cell border-box-sizing text_cell rendered"
                id="cell-id=0c9828f2"
                ><div class="prompt input_prompt"> </div
                ><div class="inner_cell">
                  <div class="text_cell_render border-box-sizing rendered_html">
                    <h2 id="Summary-and-Transition-to-Exploratory-Data-Analysis"
                      >Summary and Transition to Exploratory Data Analysis<a
                        class="anchor-link"
                        href="#Summary-and-Transition-to-Exploratory-Data-Analysis"
                      ></a></h2
                    ><p
                      >You've mastered advanced preprocessing and feature
                      engineering techniques: categorical encoding strategies,
                      scaling methods, cyclical time encoding, and lag features.
                      These skills transform clean transportation data into
                      machine learning-ready inputs.</p
                    >
                    <p
                      >Your ability to create new features from raw data
                      prepares you to work with complex transportation
                      prediction challenges while maintaining the data quality
                      essential for accurate models.</p
                    >
                    <p
                      >In the next module, you'll learn how to explore and
                      visualize these engineered features to generate business
                      insights and validate that your preprocessing pipeline
                      creates data that reflects real-world transportation
                      patterns.</p
                    >
                  </div>
                </div>
              </div>
            </div>
          </div>
        </main>
      </div>
    </div>
  </body>
</html>
