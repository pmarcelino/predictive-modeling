{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a18e5bf",
   "metadata": {},
   "source": [
    "# Lecture 5: Programming Example - Advanced Feature Engineering for Transportation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741b6a62",
   "metadata": {},
   "source": [
    "## Introduction: Transforming Clean Data into ML-Ready Features\n",
    "\n",
    "Today you'll learn about advanced preprocessing and feature engineering techniques that transform your clean bike-sharing data into optimized inputs for machine learning models. We'll apply categorical encoding first, then implement scaling strategies, and finally create time-based features that capture transportation demand patterns.\n",
    "\n",
    "Every feature we create serves a specific purpose: enabling machine learning algorithms to better understand and predict transportation demand patterns.\n",
    "\n",
    "> **ðŸš€ Interactive Learning Alert**\n",
    "> \n",
    "> This is a hands-on data cleaning tutorial with detective work and problem-solving. For the best experience:\n",
    ">\n",
    "> - **Click \"Open in Colab\"** at the bottom to run code interactively\n",
    "> - **Execute each code cell** by pressing **Shift + Enter**\n",
    "> - **Complete the challenges** to practice your data cleaning skills\n",
    "> - **Think like a consultant** - every decision impacts client trust\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971217e2",
   "metadata": {},
   "source": [
    "## Step 1: Categorical Feature Engineering with One-Hot Encoding\n",
    "\n",
    "Let's start by transforming categorical variables into machine learning-compatible formats, starting with the most fundamental technique: one-hot encoding for nominal categories.\n",
    "\n",
    "We'll use pandas' `get_dummies()` function to convert categorical weather conditions into binary columns that machine learning algorithms can process. This function creates a separate binary column for each unique category, where 1 indicates the presence of that category and 0 indicates its absence.\n",
    "\n",
    "Let's transform weather conditions into one-hot encoded features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4170ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\")\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "# Create weather_condition column from numeric weather codes\n",
    "if 'weather' in df.columns:\n",
    "    weather_map = {1: 'Clear', 2: 'Misty', 3: 'Light Rain', 4: 'Heavy Rain'}\n",
    "    df['weather_condition'] = df['weather'].map(weather_map)\n",
    "    print(\"Weather condition categories:\")\n",
    "    print(df['weather_condition'].value_counts())\n",
    "else:\n",
    "    print(\"Weather column not found - creating synthetic weather conditions\")\n",
    "    df['weather_condition'] = 'Clear'\n",
    "\n",
    "# Apply one-hot encoding\n",
    "weather_encoded = pd.get_dummies(df['weather_condition'], prefix='weather')\n",
    "print(\"\\nOne-hot encoded weather columns:\")\n",
    "print(weather_encoded.head())\n",
    "\n",
    "# Add encoded columns to main dataframe\n",
    "df = pd.concat([df, weather_encoded], axis=1)\n",
    "print(f\"\\nOriginal 1 column â†’ {len(weather_encoded.columns)} binary columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0356116",
   "metadata": {},
   "source": [
    "**What this accomplishes:**\n",
    "- Converts text weather labels into numerical format algorithms can process\n",
    "- Creates 4 binary columns (one per weather type)\n",
    "- Preserves all category information without implying false orderings\n",
    "\n",
    "**Business value:**\n",
    "Enables the model to learn that \"Clear\" weather drives high recreational demand while \"Light Rain\" reduces casual ridership but maintains commuter patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd503eeb",
   "metadata": {},
   "source": [
    "### Challenge 1: Create Day Type Categories with One-Hot Encoding\n",
    "Create a `day_type` feature that categorizes days as 'weekday', 'weekend', or 'holiday', then apply one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceea98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - create day_type categories and apply one-hot encoding\n",
    "\n",
    "# Step 1: Extract day of week from datetime (0=Monday, 6=Sunday)\n",
    "df['day_of_week'] = df['datetime'].dt._____  # Fill in: dayofweek\n",
    "\n",
    "# Step 2: Create day_type categories\n",
    "df['day_type'] = '_____'  # Fill in: 'weekday' as default\n",
    "df.loc[df['day_of_week'].isin([5, 6]), 'day_type'] = '_____'  # Fill in: 'weekend' for Sat/Sun\n",
    "df.loc[df['holiday'] == 1, 'day_type'] = '_____'  # Fill in: 'holiday'\n",
    "\n",
    "# Step 3: Apply one-hot encoding\n",
    "day_type_encoded = pd.get_dummies(df['_____'], prefix='daytype')  # Fill in: 'day_type'\n",
    "df = pd.concat([df, day_type_encoded], axis=1)\n",
    "\n",
    "print(\"Day type distribution:\")\n",
    "print(df['day_type'].value_counts())\n",
    "print(f\"\\nCreated {len(day_type_encoded.columns)} binary day type features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235bb586",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ <strong>Tip</strong> (click to expand)</summary>\n",
    "\n",
    "\n",
    "Think about this in three steps: First, extract day of week using `.dt.dayofweek` (returns 0-6 where 0=Monday, 6=Sunday). Second, create a default 'weekday' label, then override for weekends and holidays. Third, apply `pd.get_dummies()` just like we did for weather conditions. Watch out: `.dt.dayofweek` uses 0=Monday, so weekends are 5 and 6 (not 6 and 7). Also, order matters when setting categories - set holidays AFTER weekends if you want holidays to take priority. Remember `.isin([5, 6])` for checking multiple values - using `== 5 or == 6` won't work in pandas. Your client needs different strategies for weekdays (commute-focused) vs weekends (recreation) vs holidays (special events).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4c2baa",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ¤« <strong>Solution</strong> (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "# Step 1: Extract day of week from datetime (0=Monday, 6=Sunday)\n",
    "df['day_of_week'] = df['datetime'].dt.dayofweek\n",
    "\n",
    "# Step 2: Create day_type categories\n",
    "df['day_type'] = 'weekday'\n",
    "df.loc[df['day_of_week'].isin([5, 6]), 'day_type'] = 'weekend'\n",
    "df.loc[df['holiday'] == 1, 'day_type'] = 'holiday'\n",
    "\n",
    "# Step 3: Apply one-hot encoding\n",
    "day_type_encoded = pd.get_dummies(df['day_type'], prefix='daytype')\n",
    "df = pd.concat([df, day_type_encoded], axis=1)\n",
    "\n",
    "print(\"Day type distribution:\")\n",
    "print(df['day_type'].value_counts())\n",
    "print(f\"\\nCreated {len(day_type_encoded.columns)} binary day type features\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c4b36b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00402fac",
   "metadata": {},
   "source": [
    "## Step 2: Binary Encoding for Business-Specific Conditions\n",
    "\n",
    "Beyond standard categorical encoding, transportation consultants create focused binary indicators (0 or 1) that flag business-critical conditions. These features encode domain expertise directly into your data.\n",
    "\n",
    "We'll create boolean conditions using pandas' `.isin()` method, then convert True/False values to 1/0 integers with `.astype(int)` for machine learning compatibility.\n",
    "\n",
    "Let's build business-relevant binary features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38608b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\")\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "# Extract hour and day of week from datetime\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['day_of_week'] = df['datetime'].dt.dayofweek\n",
    "\n",
    "# Binary rush hour indicator (morning 7-9, evening 17-19)\n",
    "rush_hours = [7, 8, 9, 17, 18, 19]\n",
    "df['is_rush_hour'] = df['hour'].isin(rush_hours).astype(int)\n",
    "\n",
    "# Binary weekend indicator\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Binary good weather indicator (Clear or Misty conditions)\n",
    "if 'weather' in df.columns:\n",
    "    df['is_good_weather'] = df['weather'].isin([1, 2]).astype(int)\n",
    "else:\n",
    "    df['is_good_weather'] = 1  # Default to good weather if data unavailable\n",
    "\n",
    "print(\"Binary feature distributions:\")\n",
    "print(f\"Rush hour periods: {df['is_rush_hour'].mean()*100:.1f}% of hours\")\n",
    "print(f\"Weekend days: {df['is_weekend'].mean()*100:.1f}% of days\")\n",
    "print(f\"Good weather: {df['is_good_weather'].mean()*100:.1f}% of time\")\n",
    "\n",
    "print(\"\\nSample of binary features:\")\n",
    "print(df[['datetime', 'is_rush_hour', 'is_weekend', 'is_good_weather']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c8439d",
   "metadata": {},
   "source": [
    "**What this accomplishes:**\n",
    "- Creates focused indicators for operationally critical conditions\n",
    "- Encodes transportation domain expertise into features\n",
    "- Provides clear yes/no signals that models can easily learn from\n",
    "\n",
    "**Business value:**\n",
    "These flags help models learn distinct patterns like \"weekend + good weather = high recreational demand\" or \"rush hour + weekday = commuter surge.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5cc8e4",
   "metadata": {},
   "source": [
    "### Challenge 2: Create Season Binary Indicators\n",
    "Create binary indicators for each season to help the model learn seasonal demand patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bab491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - create binary indicators for each season\n",
    "\n",
    "# Step 1: Extract month from datetime\n",
    "df['month'] = df['datetime'].dt._____  # Fill in: month\n",
    "\n",
    "# Step 2: Create binary indicators for each season\n",
    "# Spring: March, April, May (months 3, 4, 5)\n",
    "df['is_spring'] = df['month'].isin([_____, _____, _____]).astype(int)  # Fill in: 3, 4, 5\n",
    "\n",
    "# Summer: June, July, August (months 6, 7, 8)\n",
    "df['is_summer'] = df['month'].isin([_____, _____, _____]).astype(int)  # Fill in: 6, 7, 8\n",
    "\n",
    "# Fall: September, October, November (months 9, 10, 11)\n",
    "df['is_fall'] = df['month'].isin([_____, _____, _____]).astype(int)  # Fill in: 9, 10, 11\n",
    "\n",
    "# Winter: December, January, February (months 12, 1, 2)\n",
    "df['is_winter'] = df['month'].isin([_____, _____, _____]).astype(int)  # Fill in: 12, 1, 2\n",
    "\n",
    "print(\"Seasonal distribution:\")\n",
    "for season in ['spring', 'summer', 'fall', 'winter']:\n",
    "    pct = df[f'is_{season}'].mean() * 100\n",
    "    print(f\"{season.capitalize()}: {pct:.1f}% of observations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58278553",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ <strong>Tip</strong> (click to expand)</summary>\n",
    "\n",
    "\n",
    "Extract month numbers using `.dt.month` (returns 1-12, not month names). For each season, use `.isin()` to check if the month is in that season's list, then `.astype(int)` to convert True/False to 1/0. Watch out: winter spans the year boundary, so you need `[12, 1, 2]` not consecutive numbers. Remember to use `.astype(int)` because boolean True/False won't work directly in ML algorithms. Bike-sharing demand changes dramatically by season - summer weekend afternoons need 3x more bikes than winter weekday evenings, so these season indicators help your model learn those patterns.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e6549c",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ¤« <strong>Solution</strong> (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "# Step 1: Extract month from datetime\n",
    "df['month'] = df['datetime'].dt.month\n",
    "\n",
    "# Step 2: Create binary indicators for each season\n",
    "# Spring: March, April, May (months 3, 4, 5)\n",
    "df['is_spring'] = df['month'].isin([3, 4, 5]).astype(int)\n",
    "\n",
    "# Summer: June, July, August (months 6, 7, 8)\n",
    "df['is_summer'] = df['month'].isin([6, 7, 8]).astype(int)\n",
    "\n",
    "# Fall: September, October, November (months 9, 10, 11)\n",
    "df['is_fall'] = df['month'].isin([9, 10, 11]).astype(int)\n",
    "\n",
    "# Winter: December, January, February (months 12, 1, 2)\n",
    "df['is_winter'] = df['month'].isin([12, 1, 2]).astype(int)\n",
    "\n",
    "print(\"Seasonal distribution:\")\n",
    "for season in ['spring', 'summer', 'fall', 'winter']:\n",
    "    pct = df[f'is_{season}'].mean() * 100\n",
    "    print(f\"{season.capitalize()}: {pct:.1f}% of observations\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d31f62",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b744917",
   "metadata": {},
   "source": [
    "## Step 3: Feature Scaling with StandardScaler\n",
    "\n",
    "Now that we've created categorical features in numerical form, we need to address a critical issue: our features exist on wildly different scales. Temperature ranges from 0-40Â°C, humidity spans 0-100%, and bike counts vary from 1 to 1000+. Without scaling, machine learning algorithms will incorrectly prioritize features with larger numbers.\n",
    "\n",
    "We'll apply `StandardScaler` to normalize our features - transforming them to have mean=0 and standard deviation=1. This Z-score normalization ensures all features contribute equally regardless of their original measurement scales.\n",
    "\n",
    "`StandardScaler` uses a consistent three-step workflow:\n",
    "\n",
    "1. **Create the scaler**: `scaler = StandardScaler()`\n",
    "2. **Fit to data**: `scaler.fit_transform(df[columns])` - calculates mean/std from data AND transforms it\n",
    "3. **Result**: All features now have mean â‰ˆ 0, std â‰ˆ 1\n",
    "\n",
    "Let's apply `StandardScaler` to weather variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d482b047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\")\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "# Identify weather features for scaling\n",
    "weather_features = ['temp', 'atemp', 'humidity', 'windspeed']\n",
    "\n",
    "# Check original scales\n",
    "print(\"Original feature scales:\")\n",
    "print(df[weather_features].describe().round(1))\n",
    "\n",
    "# Create and apply StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df[weather_features] = scaler.fit_transform(df[weather_features])\n",
    "\n",
    "# Check scaled results\n",
    "print(\"\\nScaled feature statistics:\")\n",
    "print(df[weather_features].describe().round(3))\n",
    "print(\"\\nNotice: mean â‰ˆ 0, std â‰ˆ 1 for all features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66643510",
   "metadata": {},
   "source": [
    "**What this accomplishes:**\n",
    "- All weather features now exist on the same statistical scale\n",
    "- Features with originally larger ranges (like humidity 0-100) no longer dominate\n",
    "- Algorithms can fairly compare the importance of temperature vs. humidity vs. windspeed\n",
    "\n",
    "**Business value:**\n",
    "Ensures your model doesn't overweight humidity simply because it's measured in larger numbers (0-100%) compared to temperature (0-40Â°C)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5831395",
   "metadata": {},
   "source": [
    "### Challenge 3: Verify Scaling Preserved Relationships\n",
    "Check that StandardScaler transformed the scales without breaking the relationships between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d56693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - verify scaling results\n",
    "\n",
    "# Check that correlations are preserved (scaling shouldn't change relationships)\n",
    "if 'temp' in df.columns and 'count' in df.columns:\n",
    "    correlation = df['temp'].corr(df['_____'])  # Fill in: 'count'\n",
    "    print(f\"Temperature-Count correlation after scaling: {correlation:.3f}\")\n",
    "    print(\"(This should be similar to original correlation)\")\n",
    "\n",
    "# Verify scaled features have mean â‰ˆ 0, std â‰ˆ 1\n",
    "print(\"\\nScaled feature validation:\")\n",
    "for feature in weather_features:\n",
    "    mean_val = df[feature]._____()  # Fill in: mean\n",
    "    std_val = df[feature]._____()   # Fill in: std\n",
    "    print(f\"{feature}: mean={mean_val:.3f}, std={std_val:.3f}\")\n",
    "\n",
    "    # Check if properly standardized (mean close to 0, std close to 1)\n",
    "    if abs(mean_val) < 0.01 and abs(std_val - 1) < 0.01:\n",
    "        print(f\"  âœ“ {feature} properly standardized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf2eea3",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ <strong>Tip</strong> (click to expand)</summary>\n",
    "\n",
    "\n",
    "Scaling validation has two key checks: First, verify relationships are preserved by calculating correlations - they should be identical to the original. Second, check that mean â‰ˆ 0 and std â‰ˆ 1 using `.mean()` and `.std()`. Don't expect exact 0.000 and 1.000 - slight floating-point variations are normal (check if abs(mean) < 0.01). Think of scaling like translating a book: the story (relationships) stays the same, only the language (scale) changes. If correlations change significantly, your scaling corrupted the data!\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297e0069",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ¤« <strong>Solution</strong> (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "# Check that correlations are preserved (scaling shouldn't change relationships)\n",
    "if 'temp' in df.columns and 'count' in df.columns:\n",
    "    correlation = df['temp'].corr(df['count'])\n",
    "    print(f\"Temperature-Count correlation after scaling: {correlation:.3f}\")\n",
    "    print(\"(This should be similar to original correlation)\")\n",
    "\n",
    "# Verify scaled features have mean â‰ˆ 0, std â‰ˆ 1\n",
    "print(\"\\nScaled feature validation:\")\n",
    "for feature in weather_features:\n",
    "    mean_val = df[feature].mean()\n",
    "    std_val = df[feature].std()\n",
    "    print(f\"{feature}: mean={mean_val:.3f}, std={std_val:.3f}\")\n",
    "\n",
    "    # Check if properly standardized (mean close to 0, std close to 1)\n",
    "    if abs(mean_val) < 0.01 and abs(std_val - 1) < 0.01:\n",
    "        print(f\"  âœ“ {feature} properly standardized\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ca1a99",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5a2e74",
   "metadata": {},
   "source": [
    "## Step 4: Range Normalization with MinMaxScaler\n",
    "\n",
    "`StandardScaler` is excellent for normally-distributed features, but some features naturally have fixed bounds. For these, `MinMaxScaler` provides an alternative scaling strategy that compresses values into a defined range, typically 0 to 1.\n",
    "\n",
    "`MinMaxScaler` transforms features using a linear mapping: the minimum value becomes 0, the maximum becomes 1, and all other values scale proportionally between these bounds. The process follows three steps:\n",
    "\n",
    "1. **Initialize**: `scaler = MinMaxScaler(feature_range=(0, 1))`\n",
    "2. **Transform**: `scaler.fit_transform(df[columns])` - calculates min/max values and applies the transformation\n",
    "3. **Output**: Features compressed to 0-1 range while preserving relative distances\n",
    "\n",
    "Let's apply `MinMaxScaler` to temporal features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21615c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\")\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "# Extract temporal features\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['day_of_week'] = df['datetime'].dt.dayofweek\n",
    "df['month'] = df['datetime'].dt.month\n",
    "\n",
    "# Identify temporal features for MinMax scaling\n",
    "temporal_features = ['hour', 'day_of_week', 'month']\n",
    "\n",
    "# Check original ranges\n",
    "print(\"Original temporal feature ranges:\")\n",
    "print(df[temporal_features].agg(['min', 'max']))\n",
    "\n",
    "# Create and apply MinMaxScaler\n",
    "minmax_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "df[temporal_features] = minmax_scaler.fit_transform(df[temporal_features])\n",
    "\n",
    "# Check scaled results\n",
    "print(\"\\nScaled temporal feature ranges:\")\n",
    "print(df[temporal_features].agg(['min', 'max']).round(4))\n",
    "print(\"\\nNotice: All features now range from 0 to 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b69e72",
   "metadata": {},
   "source": [
    "**What this accomplishes:**\n",
    "- Temporal features compressed to consistent 0-1 range\n",
    "- Boundary properties preserved (midnight still maps to 0, values stay bounded)\n",
    "- Compatible scale with other normalized features\n",
    "\n",
    "**Business value:**\n",
    "Ensures hour-of-day (0-23) contributes proportionally to demand predictions alongside day-of-week (0-6), preventing arbitrary scale differences from biasing the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eee351f",
   "metadata": {},
   "source": [
    "### Challenge 4: Compare Scaling Methods\n",
    "Create a comparison showing how StandardScaler and MinMaxScaler transform the same data differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964cb011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - compare scaling methods on temperature data\n",
    "\n",
    "# Get fresh temperature data (before scaling)\n",
    "df_fresh = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\")\n",
    "original_temp = df_fresh['temp'].head(10)\n",
    "\n",
    "# Apply StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "standard_scaler = StandardScaler()\n",
    "temp_standard = standard_scaler.fit_transform(original_temp.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Apply MinMaxScaler\n",
    "minmax_scaler = _____()  # Fill in: MinMaxScaler\n",
    "temp_minmax = minmax_scaler.fit_transform(original_temp.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Compare results\n",
    "comparison = pd.DataFrame({\n",
    "    'Original': original_temp.values,\n",
    "    'StandardScaler': temp_standard,\n",
    "    'MinMaxScaler': temp_minmax\n",
    "})\n",
    "print(\"Scaling method comparison (first 10 temperatures):\")\n",
    "print(comparison.round(3))\n",
    "\n",
    "print(\"\\nKey differences:\")\n",
    "print(f\"StandardScaler range: {temp_standard.min():.2f} to {temp_standard.max():.2f}\")\n",
    "print(f\"MinMaxScaler range: {temp_minmax.min():.2f} to {temp_minmax.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bddd64",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ <strong>Tip</strong> (click to expand)</summary>\n",
    "\n",
    "\n",
    "Load fresh unscaled data, then apply both StandardScaler and MinMaxScaler to the same values to see their different behaviors. Remember `.reshape(-1, 1)` when scaling a single column because sklearn scalers expect 2D arrays, then use `.flatten()` to convert back to 1D after scaling. Choose StandardScaler for unbounded features like temperature (approximately normal distribution) and MinMaxScaler for bounded features like hour-of-day (uniform distribution). Wrong choice won't break models but may slow learning!\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7234eeaa",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ¤« <strong>Solution</strong> (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "# Get fresh temperature data (before scaling)\n",
    "df_fresh = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\")\n",
    "original_temp = df_fresh['temp'].head(10)\n",
    "\n",
    "# Apply StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "standard_scaler = StandardScaler()\n",
    "temp_standard = standard_scaler.fit_transform(original_temp.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Apply MinMaxScaler\n",
    "minmax_scaler = MinMaxScaler()\n",
    "temp_minmax = minmax_scaler.fit_transform(original_temp.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Compare results\n",
    "comparison = pd.DataFrame({\n",
    "    'Original': original_temp.values,\n",
    "    'StandardScaler': temp_standard,\n",
    "    'MinMaxScaler': temp_minmax\n",
    "})\n",
    "print(\"Scaling method comparison (first 10 temperatures):\")\n",
    "print(comparison.round(3))\n",
    "\n",
    "print(\"\\nKey differences:\")\n",
    "print(f\"StandardScaler range: {temp_standard.min():.2f} to {temp_standard.max():.2f}\")\n",
    "print(f\"MinMaxScaler range: {temp_minmax.min():.2f} to {temp_minmax.max():.2f}\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd25e602",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed145a3",
   "metadata": {},
   "source": [
    "## Step 5: Cyclical Encoding for Continuous Time\n",
    "\n",
    "Now that you've mastered categorical encoding and scaling, we turn to the most critical dimension in transportation data: time. Raw time values like hour=23 and hour=0 appear numerically distant, but they represent adjacent hours on a clock. Time-based feature engineering solves this problem through cyclical encoding.\n",
    "\n",
    "Let's implement cyclical encoding for hours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc8f206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\")\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "# Extract hour from datetime\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "\n",
    "# Create cyclical encoding for 24-hour cycle\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "\n",
    "# Compare linear hour vs. cyclical encoding\n",
    "print(\"Hour encoding comparison:\")\n",
    "comparison = df[['hour', 'hour_sin', 'hour_cos']].drop_duplicates().sort_values('hour')\n",
    "print(comparison.head(12))\n",
    "\n",
    "# Verify hour 23 and hour 0 are close\n",
    "hour_23 = df[df['hour'] == 23][['hour_sin', 'hour_cos']].iloc[0]\n",
    "hour_0 = df[df['hour'] == 0][['hour_sin', 'hour_cos']].iloc[0]\n",
    "print(f\"\\nHour 23: sin={hour_23['hour_sin']:.3f}, cos={hour_23['hour_cos']:.3f}\")\n",
    "print(f\"Hour 0:  sin={hour_0['hour_sin']:.3f}, cos={hour_0['hour_cos']:.3f}\")\n",
    "print(\"Notice how close these values are despite hours being 23 apart numerically!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dce65fd",
   "metadata": {},
   "source": [
    "**What this accomplishes:**\n",
    "- Hours now represented as circular coordinates instead of linear numbers\n",
    "- Machine learning algorithms can recognize that 11 PM and midnight are adjacent\n",
    "- Continuous temporal patterns preserved without artificial breaks\n",
    "\n",
    "**Business value:**\n",
    "Enables models to learn that late-night demand (11 PM) transitions smoothly into early-morning demand (midnight-1 AM), rather than treating them as disconnected time periods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702a99f2",
   "metadata": {},
   "source": [
    "### Challenge 5: Create Cyclical Encoding for Day of Week\n",
    "Apply cyclical encoding to the 7-day week cycle to capture how Monday transitions into Tuesday, and Sunday loops back to Monday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5fe82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - create cyclical encoding for day of week (7-day cycle)\n",
    "\n",
    "# Create sine and cosine features for 7-day cycle\n",
    "df['dow_sin'] = np._____(2 * np.pi * df['day_of_week'] / _____)  # Fill in: sin, 7\n",
    "df['dow_cos'] = np._____(2 * np.pi * df['day_of_week'] / _____)  # Fill in: cos, 7\n",
    "\n",
    "# Verify Sunday (6) and Monday (0) are close\n",
    "print(\"Day of week cyclical encoding:\")\n",
    "dow_comparison = df[['day_of_week', 'dow_sin', 'dow_cos']].drop_duplicates().sort_values('day_of_week')\n",
    "print(dow_comparison)\n",
    "\n",
    "# Calculate how close Sunday and Monday are in cyclical space\n",
    "sunday = df[df['day_of_week'] == 6][['dow_sin', 'dow_cos']].iloc[0]\n",
    "monday = df[df['day_of_week'] == 0][['dow_sin', 'dow_cos']].iloc[0]\n",
    "print(f\"\\nSunday: sin={sunday['dow_sin']:.3f}, cos={sunday['dow_cos']:.3f}\")\n",
    "print(f\"Monday: sin={monday['dow_sin']:.3f}, cos={monday['dow_cos']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55811a6",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ <strong>Tip</strong> (click to expand)</summary>\n",
    "\n",
    "\n",
    "Follow the exact same pattern as hour encoding, just change the cycle length to 7 days instead of 24 hours. Apply both `np.sin()` and `np.cos()` with the formula: `2 * np.pi * day_of_week / 7`. You need BOTH sine and cosine to uniquely identify each day position - using only one won't work. Bike-sharing demand on Sunday evening (preparing for work week) should be more similar to Monday morning than to Saturday evening (weekend recreation), and cyclical encoding captures this weekly rhythm.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb24f76a",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ¤« <strong>Solution</strong> (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "# Create sine and cosine features for 7-day cycle\n",
    "df['dow_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "df['dow_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "\n",
    "# Verify Sunday (6) and Monday (0) are close\n",
    "print(\"Day of week cyclical encoding:\")\n",
    "dow_comparison = df[['day_of_week', 'dow_sin', 'dow_cos']].drop_duplicates().sort_values('day_of_week')\n",
    "print(dow_comparison)\n",
    "\n",
    "# Calculate how close Sunday and Monday are in cyclical space\n",
    "sunday = df[df['day_of_week'] == 6][['dow_sin', 'dow_cos']].iloc[0]\n",
    "monday = df[df['day_of_week'] == 0][['dow_sin', 'dow_cos']].iloc[0]\n",
    "print(f\"\\nSunday: sin={sunday['dow_sin']:.3f}, cos={sunday['dow_cos']:.3f}\")\n",
    "print(f\"Monday: sin={monday['dow_sin']:.3f}, cos={monday['dow_cos']:.3f}\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517aaafc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806b779e",
   "metadata": {},
   "source": [
    "## Step 6: Lag Features for Sequential Patterns\n",
    "\n",
    "You've learned how cyclical encoding helps models understand that midnight follows 11 PM. Now we'll introduce lag features, which explicitly provide historical context by using past values as predictors for current observations.\n",
    "\n",
    "Lag features use pandas' `.shift()` method to access historical values as predictors. The `.shift(n)` function moves each value down by `n` positions, so `shift(1)` gives you the previous hour's value, `shift(24)` gives you the same hour yesterday, and so on.\n",
    "\n",
    "**Critical requirement:** Always sort your data by datetime before creating lag features. Without proper sorting, `.shift()` will create meaningless relationships between unrelated time periods.\n",
    "\n",
    "Let's create lag features at multiple time scales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b42089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\")\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "# CRITICAL: Sort by datetime before creating lag features\n",
    "df = df.sort_values('datetime').reset_index(drop=True)\n",
    "\n",
    "# Create lag features at different time scales\n",
    "df['count_lag_1h'] = df['count'].shift(1)      # 1 hour ago (immediate momentum)\n",
    "df['count_lag_24h'] = df['count'].shift(24)    # Same hour yesterday (daily cycle)\n",
    "df['count_lag_168h'] = df['count'].shift(168)  # Same hour last week (weekly cycle)\n",
    "\n",
    "# Display lag features\n",
    "print(\"Lag feature examples (first 30 rows):\")\n",
    "print(df[['datetime', 'count', 'count_lag_1h', 'count_lag_24h', 'count_lag_168h']].head(30))\n",
    "\n",
    "# Check how many NaN values were created\n",
    "print(\"\\nMissing values created by lag features:\")\n",
    "print(f\"1-hour lag: {df['count_lag_1h'].isnull().sum()} NaN values\")\n",
    "print(f\"24-hour lag: {df['count_lag_24h'].isnull().sum()} NaN values\")\n",
    "print(f\"168-hour lag: {df['count_lag_168h'].isnull().sum()} NaN values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba6a6d3",
   "metadata": {},
   "source": [
    "**What this accomplishes:**\n",
    "- Historical demand patterns explicitly available as features\n",
    "- Three time scales capture immediate momentum, daily cycles, and weekly patterns\n",
    "- Model can learn \"if demand was high 24 hours ago, it's likely high now\"\n",
    "\n",
    "**Business value:**\n",
    "Lag features are among the strongest predictors for time series. If 200 bikes were rented last hour, demand probably remains high this hour - lag features encode this critical pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01435804",
   "metadata": {},
   "source": [
    "### Challenge 6: Create Temperature Lag Features\n",
    "Create lag features for temperature to help the model understand temperature trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaebe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - create temperature lag features\n",
    "\n",
    "# Create 1-hour and 6-hour temperature lags\n",
    "df['temp_lag_1h'] = df['temp']._____(_____)   # Fill in: shift, 1\n",
    "df['temp_lag_6h'] = df['temp']._____(_____)   # Fill in: shift, 6\n",
    "\n",
    "# Calculate temperature change over past hour\n",
    "df['temp_change_1h'] = df['temp'] - df['_____']  # Fill in: temp_lag_1h\n",
    "\n",
    "# Display temperature trends\n",
    "print(\"Temperature lag features (rows 10-20):\")\n",
    "print(df[['datetime', 'temp', 'temp_lag_1h', 'temp_lag_6h', 'temp_change_1h']].iloc[10:20])\n",
    "\n",
    "# Identify periods with rapid temperature changes\n",
    "rapid_change = df[abs(df['temp_change_1h']) > 0.5]  # More than 0.5Â°C change per hour\n",
    "print(f\"\\nPeriods with rapid temperature change: {len(rapid_change)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dcbc80",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ <strong>Tip</strong> (click to expand)</summary>\n",
    "\n",
    "\n",
    "Use the same `.shift()` pattern as before: `df['temp'].shift(1)` for 1-hour lag and `df['temp'].shift(6)` for 6-hour lag. Calculate temperature change by subtracting lag from current: `df['temp'] - df['temp_lag_1h']` (this gives positive values when temperature rises). Use `abs()` when checking for rapid changes because we care about magnitude regardless of direction - both +0.6 and -0.6 are rapid changes. Rapid temperature swings affect bike demand differently than gradual changes - a sudden 5-degree warming at 2 PM might trigger recreational trips that wouldn't happen if the same temperature was reached gradually.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35ed2c0",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ¤« <strong>Solution</strong> (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "# Create 1-hour and 6-hour temperature lags\n",
    "df['temp_lag_1h'] = df['temp'].shift(1)\n",
    "df['temp_lag_6h'] = df['temp'].shift(6)\n",
    "\n",
    "# Calculate temperature change over past hour\n",
    "df['temp_change_1h'] = df['temp'] - df['temp_lag_1h']\n",
    "\n",
    "# Display temperature trends\n",
    "print(\"Temperature lag features (rows 10-20):\")\n",
    "print(df[['datetime', 'temp', 'temp_lag_1h', 'temp_lag_6h', 'temp_change_1h']].iloc[10:20])\n",
    "\n",
    "# Identify periods with rapid temperature changes\n",
    "rapid_change = df[abs(df['temp_change_1h']) > 0.5]\n",
    "print(f\"\\nPeriods with rapid temperature change: {len(rapid_change)}\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569ab438",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78f2cc1",
   "metadata": {},
   "source": [
    "## Step 8: Rolling Window Features for Trend Detection\n",
    "\n",
    "Lag features provide point-in-time historical values, but transportation demand also depends on recent trends. Is demand rising or falling? Has weather been stable or volatile? Rolling window features answer these questions by aggregating recent history.\n",
    "\n",
    "We'll use pandas' `.rolling()` method to create rolling window features. The `.rolling(window=n)` method creates a moving window that slides through your data, calculating statistics over the most recent n observations.\n",
    "\n",
    "**Critical:** Always use `.shift(1)` before `.rolling()` to prevent data leakage. Without `.shift(1)`, the rolling average includes the current value you're trying to predict - that's cheating!\n",
    "\n",
    "Let's create rolling window features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480eca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\")\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "# Sort by datetime before creating rolling features\n",
    "df = df.sort_values('datetime').reset_index(drop=True)\n",
    "\n",
    "# Create rolling window features (with proper shift to avoid data leakage)\n",
    "df['count_rolling_3h'] = df['count'].shift(1).rolling(window=3).mean()    # Average past 3 hours\n",
    "df['count_rolling_24h'] = df['count'].shift(1).rolling(window=24).mean()  # Average past 24 hours\n",
    "\n",
    "# Temperature volatility over past 6 hours\n",
    "df['temp_rolling_std_6h'] = df['temp'].shift(1).rolling(window=6).std()\n",
    "\n",
    "# Display rolling features\n",
    "print(\"Rolling window features (rows 30-40):\")\n",
    "print(df[['datetime', 'count', 'count_rolling_3h', 'count_rolling_24h', 'temp_rolling_std_6h']].iloc[30:40])\n",
    "\n",
    "# Check missing values (first few rows won't have full windows)\n",
    "print(f\"\\n3-hour rolling: {df['count_rolling_3h'].isnull().sum()} NaN values\")\n",
    "print(f\"24-hour rolling: {df['count_rolling_24h'].isnull().sum()} NaN values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fd19cc",
   "metadata": {},
   "source": [
    "**What this accomplishes:**\n",
    "- 3-hour average captures immediate demand trends (accelerating or declining)\n",
    "- 24-hour average provides daily baseline context\n",
    "- Temperature volatility indicates weather stability (affects user confidence)\n",
    "\n",
    "**Business value:**\n",
    "If 3-hour average is 150 bikes/hour but 24-hour average is 100, demand is surging above normal - signal operations to prioritize bike rebalancing!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b7d7ee",
   "metadata": {},
   "source": [
    "### Challenge 8: Create Weather Stability Rolling Features\n",
    "Create rolling features that measure weather stability, which affects user willingness to plan bike trips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9b231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - create weather stability rolling features\n",
    "\n",
    "# Humidity volatility over past 6 hours (standard deviation)\n",
    "df['humidity_rolling_std_6h'] = df['humidity'].shift(1)._____(window=_____).std()  # Fill in: rolling, 6\n",
    "\n",
    "# Maximum windspeed over past 3 hours\n",
    "df['windspeed_rolling_max_3h'] = df['windspeed'].shift(1)._____(window=_____).max()  # Fill in: rolling, 3\n",
    "\n",
    "# Temperature range over past 12 hours (max - min)\n",
    "temp_rolling_12h = df['temp'].shift(1).rolling(window=12)\n",
    "df['temp_range_12h'] = temp_rolling_12h.max() - temp_rolling_12h._____()  # Fill in: min\n",
    "\n",
    "# Display weather stability features\n",
    "print(\"Weather stability rolling features (rows 40-50):\")\n",
    "print(df[['datetime', 'humidity_rolling_std_6h', 'windspeed_rolling_max_3h', 'temp_range_12h']].iloc[40:50])\n",
    "\n",
    "# Identify periods with unstable weather (high humidity volatility)\n",
    "unstable = df[df['humidity_rolling_std_6h'] > 15]  # High humidity variation\n",
    "print(f\"\\nPeriods with unstable weather: {len(unstable)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a740628",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ <strong>Tip</strong> (click to expand)</summary>\n",
    "\n",
    "\n",
    "Apply different aggregations to rolling windows: `.std()` for volatility, `.max()` for peak values, and `.max() - .min()` for range. ALWAYS include `.shift(1)` before `.rolling()` to avoid data leakage - otherwise you're including the current value in your \"past\" window! For range calculations, create the rolling object once, then apply both `.max()` and `.min()` to it. Watch your window sizes: 6-hour for humidity, 3-hour for windspeed, 12-hour for temperature. Stable weather (low volatility) encourages casual riders planning ahead, while chaotic weather (gusts, swings) reduces recreational demand but maintains commuter patterns.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ee6feb",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ¤« <strong>Solution</strong> (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "# Humidity volatility over past 6 hours (standard deviation)\n",
    "df['humidity_rolling_std_6h'] = df['humidity'].shift(1).rolling(window=6).std()\n",
    "\n",
    "# Maximum windspeed over past 3 hours\n",
    "df['windspeed_rolling_max_3h'] = df['windspeed'].shift(1).rolling(window=3).max()\n",
    "\n",
    "# Temperature range over past 12 hours (max - min)\n",
    "temp_rolling_12h = df['temp'].shift(1).rolling(window=12)\n",
    "df['temp_range_12h'] = temp_rolling_12h.max() - temp_rolling_12h.min()\n",
    "\n",
    "# Display weather stability features\n",
    "print(\"Weather stability rolling features (rows 40-50):\")\n",
    "print(df[['datetime', 'humidity_rolling_std_6h', 'windspeed_rolling_max_3h', 'temp_range_12h']].iloc[40:50])\n",
    "\n",
    "# Identify periods with unstable weather (high humidity volatility)\n",
    "unstable = df[df['humidity_rolling_std_6h'] > 15]\n",
    "print(f\"\\nPeriods with unstable weather: {len(unstable)}\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bf33a8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bdb8cb",
   "metadata": {},
   "source": [
    "## Summary: Professional Feature Engineering Pipeline Completed\n",
    "\n",
    "**What We've Accomplished**:\n",
    "- Implemented comprehensive categorical encoding transformations using one-hot encoding for nominal variables and binary indicators for business-critical conditions\n",
    "- Applied advanced feature scaling methodologies through StandardScaler for statistical normalization and MinMaxScaler for range-bounded temporal variables\n",
    "- Developed cyclical encoding frameworks for continuous time variables ensuring proper representation of circular temporal patterns in hours and days\n",
    "- Engineered lag feature architectures capturing sequential dependencies across multiple time scales from immediate momentum to weekly cycles\n",
    "- Constructed rolling window aggregation features for trend detection and volatility measurement in demand and weather stability patterns\n",
    "- Established systematic feature validation protocols ensuring scaling operations preserved underlying data relationships and statistical properties\n",
    "\n",
    "**Key Technical Skills Mastered**:\n",
    "- One-hot encoding implementation for converting categorical weather and day-type variables into machine-readable binary column structures\n",
    "- Binary indicator creation for operationally significant conditions including rush hours, weekends, seasons, and favorable weather patterns\n",
    "- Z-score standardization techniques through StandardScaler achieving mean-centered unit-variance feature distributions for algorithm compatibility\n",
    "- Range normalization methodologies via MinMaxScaler compressing bounded temporal features into consistent zero-to-one intervals\n",
    "- Trigonometric cyclical transformation using sine and cosine functions for preserving circular continuity in time-based variables\n",
    "- Historical value propagation through shift operations creating lag features that encode sequential patterns and temporal dependencies\n",
    "- Moving window statistical computation implementing rolling averages, volatility measures, and trend indicators with proper data leakage prevention\n",
    "\n",
    "**Next Steps**: Next, we'll advance to exploratory data analysis and visualization techniques, examining engineered feature distributions, validating temporal pattern capture effectiveness, and generating actionable business insights that inform machine learning model architecture selection and hyperparameter optimization strategies.\n",
    "\n",
    "Your bike-sharing client now possesses production-grade feature engineering pipelines transforming raw operational datasets into optimized machine learning inputs - demonstrating the systematic preprocessing methodologies and advanced feature construction techniques that consulting firms require for high-performance predictive modeling and strategic transportation demand forecasting applications."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
