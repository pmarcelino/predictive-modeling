{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "787fac48",
   "metadata": {},
   "source": [
    "# Lecture 3: Programming Example - Pandas Fundamentals with Washington D.C. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f323f770",
   "metadata": {},
   "source": [
    "## Introduction: Your First Day as a Data Consultant\n",
    "\n",
    "Welcome to your first hands-on session as a junior data consultant! Today, you'll work with real Washington D.C. bike-sharing data, learning pandas step-by-step. We'll start with absolute basics and build up to loading and exploring your client's dataset.\n",
    "\n",
    "> **ðŸš€ Interactive Learning Alert**\n",
    "> \n",
    "> This is a hands-on programming tutorial with code examples and challenges. For the best learning experience:\n",
    "> \n",
    "> - **Click \"Open in Colab\"** at the top of this notebook to run it in Google Colab\n",
    "> - **Execute each code cell** by pressing **Shift + Enter** to see the results\n",
    "> - **Complete the challenges** to practice what you learn\n",
    "> \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cd9927",
   "metadata": {},
   "source": [
    "## Step 1: Setting Up Your Data Analysis Environment\n",
    "\n",
    "Let's start by importing the tools you'll need for data analysis. Think of this like setting up your workbench before starting a project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef2738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas - your primary data manipulation tool\n",
    "import pandas as pd\n",
    "\n",
    "# Print confirmation\n",
    "print(\"Pandas imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dd822d",
   "metadata": {},
   "source": [
    "> **Note:** To run a code cell in Jupyter Notebook, click inside the cell and press **Shift + Enter**. This will execute the code and show the output directly below the cell.\n",
    "\n",
    "**What this does:**\n",
    "- `import pandas as pd` makes the pandas library available with the shorthand \"pd\"\n",
    "- The shorthand `pd` is a universal convention - all pandas users worldwide use this\n",
    "- Checking the version ensures you're working with up-to-date tools. In real-world projects, you usually just import pandas and skip printing the version or confirmation messages. We're doing it here only for teaching purposes, so you can see exactly what's happening step by step.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c781f80e",
   "metadata": {},
   "source": [
    "### Challenge 1: Import Practice\n",
    "Import pandas yourself, then check that it worked by running `type(pd)`. The output should be `<class 'module'>`, confirming that `pd` is a module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad58b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "import _____ as pd  # Fill in the library name\n",
    "print(type(pd))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117de1d6",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ¤« <strong>Solution</strong> (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "# Your code here\n",
    "import pandas as pd  # Fill in the library name\n",
    "print(type(pd))\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321aa460",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608a7aa2",
   "metadata": {},
   "source": [
    "## Step 2: Understanding Series - Single Column Data\n",
    "\n",
    "Let's start with Series by creating bike rental data for a typical Monday morning. Think of Series as a single column from a spreadsheet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39164911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Series with hourly bike rentals\n",
    "morning_rentals = pd.Series([15, 23, 45, 67, 89, 156, 234, 287])\n",
    "print(\"Morning Bike Rentals:\")\n",
    "print(morning_rentals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366f90eb",
   "metadata": {},
   "source": [
    "**What this does:**\n",
    "- `morning_rentals = pd.Series([...])` creates a pandas Series object and saves it in the variable `morning_rentals`\n",
    "- The numbers on the right column represent bike rentals for each hour\n",
    "- The numbers on the left column are index numbers that Pandas automatically assigns (0, 1, 2, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaf903a",
   "metadata": {},
   "source": [
    "### Challenge 2: Create Your Own Series\n",
    "Create a Series representing temperature readings for the same morning hours. Use these temperatures: [42, 44, 47, 50, 53, 56, 58, 61]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5875a1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas for Series creation\n",
    "import pandas as pd\n",
    "\n",
    "# Your code here - create a Series called 'morning_temps'\n",
    "morning_temps = pd.Series(_____)  # Fill in the temperature list\n",
    "print(_____)  # Print your Series to verify it worked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68097ff9",
   "metadata": {},
   "source": [
    "> **Note:** To solve the exercise, replace each `_____` with the correct value or code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e559a9",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ <strong>Tip</strong> (click to expand)</summary>\n",
    "\n",
    "\n",
    "When creating Series objects, consider these best practices:\n",
    "- Use descriptive variable names like `morning_temps` instead of generic names like `data`\n",
    "- Print the Series to verify it contains the expected values and structure\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a085a9bf",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ¤« <strong>Solution</strong> (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "# Import pandas for Series creation\n",
    "import pandas as pd\n",
    "\n",
    "# Your code here - create a Series called 'morning_temps'\n",
    "morning_temps = pd.Series([42, 44, 47, 50, 53, 56, 58, 61])  # Fill in the temperature list\n",
    "print(morning_temps)  # Print your Series to verify it worked\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c28e33f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c459a4a4",
   "metadata": {},
   "source": [
    "## Step 3: Adding Meaningful Labels to Series\n",
    "\n",
    "Raw index numbers (0, 1, 2) aren't very business-friendly. Let's add meaningful labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b07348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Series with meaningful hour labels\n",
    "hour_labels = ['6 AM', '7 AM', '8 AM', '9 AM', '10 AM', '11 AM', '12 PM', '1 PM']\n",
    "morning_rentals_labeled = pd.Series([15, 23, 45, 67, 89, 156, 234, 287], \n",
    "                                   index=hour_labels)\n",
    "print(\"Labeled Morning Bike Rentals:\")\n",
    "print(morning_rentals_labeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705b58fc",
   "metadata": {},
   "source": [
    "**What this does:**\n",
    "- `hour_labels = ['6 AM', '7 AM', '8 AM', '9 AM', '10 AM', '11 AM', '12 PM', '1 PM']` creates a list of time labels that represent each hour in the morning\n",
    "- `index=hour_labels` replaces default numbers with business-meaningful labels created in the previous line\n",
    "- Now each rental count is clearly connected to its time period\n",
    "\n",
    "Now when you show this to your client, they immediately understand that 1 PM has the highest rentals (287), which makes perfect sense for lunch-time bike usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af797f16",
   "metadata": {},
   "source": [
    "### Challenge 3: Access Specific Data Points\n",
    "Using your labeled Series, find the bike rentals at 9 AM. Use this syntax: `morning_rentals_labeled['9 AM']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdcbfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas and create the labeled Series\n",
    "import pandas as pd\n",
    "\n",
    "# Create Series with meaningful hour labels\n",
    "hour_labels = ['6 AM', '7 AM', '8 AM', '9 AM', '10 AM', '11 AM', '12 PM', '1 PM']\n",
    "morning_rentals_labeled = pd.Series([15, 23, 45, 67, 89, 156, 234, 287], \n",
    "                                   index=hour_labels)\n",
    "\n",
    "# Your code here - access the 9 AM value from the labeled Series\n",
    "nine_am_rentals = morning_rentals_labeled[_____]  # Fill in the correct label\n",
    "print(f\"Bike rentals at 9 AM: {_____}\")  # Print the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d24396e",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ <strong>Tip</strong> (click to expand)</summary>\n",
    "\n",
    "\n",
    "When accessing Series data by label, keep these techniques in mind:\n",
    "- In this challenge, we're using square brackets with the exact label: `series_name['label']`\n",
    "- Check available labels with `series_name.index` to see all options\n",
    "- Access multiple values: `series_name[['9 AM', '10 AM']]` (note double brackets)\n",
    "- Use `.loc[]` for explicit label-based selection: `series_name.loc['9 AM']`\n",
    "- Be careful with exact spelling and spacing in labels to avoid KeyError\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0181e76c",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ¤« <strong>Solution</strong> (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "# Import pandas and create the labeled Series\n",
    "import pandas as pd\n",
    "\n",
    "# Create Series with meaningful hour labels\n",
    "hour_labels = ['6 AM', '7 AM', '8 AM', '9 AM', '10 AM', '11 AM', '12 PM', '1 PM']\n",
    "morning_rentals_labeled = pd.Series([15, 23, 45, 67, 89, 156, 234, 287], \n",
    "                                   index=hour_labels)\n",
    "\n",
    "# Your code here - access the 9 AM value from the labeled Series\n",
    "nine_am_rentals = morning_rentals_labeled['9 AM']  # Fill in the correct label\n",
    "print(f\"Bike rentals at 9 AM: {nine_am_rentals}\")  # Print the result\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5920e0f8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5393ea7a",
   "metadata": {},
   "source": [
    "## Step 4: Creating Your First DataFrame - Complete Business Data\n",
    "\n",
    "Now let's combine multiple pieces of information into a DataFrame. Think of this as creating a complete spreadsheet with multiple columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4180a748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive DataFrame with multiple variables\n",
    "bike_operations_data = pd.DataFrame({\n",
    "    'hour': ['6 AM', '7 AM', '8 AM', '9 AM', '10 AM'],\n",
    "    'temperature': [5.6, 6.7, 8.3, 10.0, 11.7],\n",
    "    'bike_rentals': [15, 23, 45, 67, 89],\n",
    "    'weather_condition': ['Clear', 'Clear', 'Partly Cloudy', 'Clear', 'Clear']\n",
    "})\n",
    "\n",
    "print(\"Complete Bike Operations Data:\")\n",
    "print(bike_operations_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bb99e4",
   "metadata": {},
   "source": [
    "**What this does:**\n",
    "- `pd.DataFrame({...})` creates a DataFrame with multiple columns\n",
    "- The DataFrame constructor accepts a dictionary where:\n",
    "  - each key (like 'hour', 'temperature', etc.) becomes a column name\n",
    "  - each list in the dictionary provides the values for that column\n",
    "- All rows stay aligned (first hour corresponds to first temperature, etc.)\n",
    "\n",
    "You can immediately see patterns - bike rentals increase with temperature and time, giving your client valuable operational insights.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20348554",
   "metadata": {},
   "source": [
    "### Challenge 4: Add a New Column\n",
    "Add a column called 'user_satisfaction' with values [3.2, 3.5, 3.8, 4.1, 4.3] representing customer satisfaction ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db507d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas and create the DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "# Create a comprehensive DataFrame with multiple variables\n",
    "bike_operations_data = pd.DataFrame({\n",
    "    'hour': ['6 AM', '7 AM', '8 AM', '9 AM', '10 AM'],\n",
    "    'temperature': [5.6, 6.7, 8.3, 10.0, 11.7],\n",
    "    'bike_rentals': [15, 23, 45, 67, 89],\n",
    "    'weather_condition': ['Clear', 'Clear', 'Partly Cloudy', 'Clear', 'Clear']\n",
    "})\n",
    "\n",
    "# Your code here - add a new column with satisfaction ratings\n",
    "bike_operations_data[_____] = [3.2, 3.5, 3.8, 4.1, 4.3]  # Fill in column name\n",
    "print(_____)  # Print the updated DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1588f21",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ <strong>Tip</strong> (click to expand)</summary>\n",
    "\n",
    "\n",
    "When adding new columns to DataFrames, follow these best practices:\n",
    "- Ensure the new data list has the same length as existing rows: `len(new_data) == len(df)`\n",
    "- Use descriptive column names that clearly indicate what the data represents\n",
    "- Verify the addition worked: `df.columns` shows all column names including the new one\n",
    "- Check data types: `df.dtypes` to ensure the new column has appropriate type (float64 for ratings)\n",
    "- You can also add columns using `df.assign(column_name=values)` for a more functional approach\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3794bb2b",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ¤« <strong>Solution</strong> (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "# Import pandas and create the DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "# Create a comprehensive DataFrame with multiple variables\n",
    "bike_operations_data = pd.DataFrame({\n",
    "    'hour': ['6 AM', '7 AM', '8 AM', '9 AM', '10 AM'],\n",
    "    'temperature': [5.6, 6.7, 8.3, 10.0, 11.7],\n",
    "    'bike_rentals': [15, 23, 45, 67, 89],\n",
    "    'weather_condition': ['Clear', 'Clear', 'Partly Cloudy', 'Clear', 'Clear']\n",
    "})\n",
    "\n",
    "# Your code here - add a new column with satisfaction ratings\n",
    "bike_operations_data['user_satisfaction'] = [3.2, 3.5, 3.8, 4.1, 4.3]  # Fill in column name\n",
    "print(bike_operations_data)  # Print the updated DataFrame\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd03373",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0b260e",
   "metadata": {},
   "source": [
    "## Step 5: Loading Real Washington D.C. Dataset\n",
    "\n",
    "Now for the real challenge - loading your client's actual historical data. This is where professional consulting begins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e668115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Washington D.C. bike-sharing dataset\n",
    "data_file_path = \"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/dataset/dataset.csv\"\n",
    "df = pd.read_csv(data_file_path)\n",
    "\n",
    "# Confirm successful loading\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape[0]} rows Ã— {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aebcdd",
   "metadata": {},
   "source": [
    "**What this does:**\n",
    "- `pd.read_csv()` reads data from a CSV file into a DataFrame\n",
    "- CSV (Comma-Separated Values) a standard format for sharing tabular data\n",
    "- The shape tells you how much data you have to work with\n",
    "\n",
    "Always check the shape immediately after loading - it confirms the file loaded correctly and gives you a sense of your dataset size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f875b9",
   "metadata": {},
   "source": [
    "### Challenge 5: Explore the Column Names\n",
    "Print the column names using `df.columns` to see what variables are available in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daad07d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and load data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Washington D.C. bike-sharing dataset\n",
    "data_file_path = \"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/dataset/dataset.csv\"\n",
    "df = pd.read_csv(data_file_path)\n",
    "\n",
    "# Your code here - explore the available columns\n",
    "print(\"Available columns:\")\n",
    "print(list(_____.columns))  # Fill in the DataFrame name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bb1899",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ <strong>Tip</strong> (click to expand)</summary>\n",
    "\n",
    "\n",
    "When exploring column names in a new dataset, use these investigation techniques:\n",
    "- `df.columns` returns an Index object with all column names\n",
    "- `list(df.columns)` converts to a regular Python list for easier reading\n",
    "- `len(df.columns)` tells you how many variables you have to work with\n",
    "- Look for patterns in naming conventions to understand data structure\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec146e51",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ¤« <strong>Solution</strong> (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "# Import required libraries and load data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Washington D.C. bike-sharing dataset\n",
    "data_file_path = \"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/dataset/dataset.csv\"\n",
    "df = pd.read_csv(data_file_path)\n",
    "\n",
    "# Your code here - explore the available columns\n",
    "print(\"Available columns:\")\n",
    "print(list(df.columns))  # Fill in the DataFrame name\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80698513",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864f8fb4",
   "metadata": {},
   "source": [
    "## Step 6: First Look at Real Transportation Data\n",
    "\n",
    "Let's examine the first few rows to understand the data structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff9f936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 rows\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbbde6d",
   "metadata": {},
   "source": [
    "**What this shows:**\n",
    "- `head()` displays the first 5 rows by default (you can specify a different number like `df.head(10)` to see 10 rows)\n",
    "- You'll see actual bike-sharing data with timestamps, weather, and usage counts\n",
    "- Each row represents one hour of bike-sharing operations\n",
    "\n",
    "**Understanding the real data:**\n",
    "- `datetime`: When this data was recorded\n",
    "- `season`, `holiday`, `workingday`: Operational context\n",
    "- `weather`, `temp`, `humidity`, `windspeed`: Weather conditions\n",
    "- `casual`, `registered`, `count`: Different types of users and total rentals\n",
    "\n",
    "This is the foundation of all your future analysis for this client."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbc81a2",
   "metadata": {},
   "source": [
    "### Challenge 6: Look at the Last Few Rows\n",
    "Use `df.tail()` to see the last 5 rows of the dataset. This helps verify you have complete data coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80d8fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and load data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Washington D.C. bike-sharing dataset\n",
    "data_file_path = \"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/dataset/dataset.csv\"\n",
    "df = pd.read_csv(data_file_path)\n",
    "\n",
    "# Your code here - examine the last few rows\n",
    "print(\"Last 5 rows of the dataset:\")\n",
    "print(_____._____)  # Fill in the DataFrame name and method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfa5217",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ <strong>Tip</strong> (click to expand)</summary>\n",
    "\n",
    "\n",
    "When examining the end of your dataset, consider these data quality checks:\n",
    "- Compare last row's datetime to first row's datetime to understand time coverage\n",
    "- Check if the last rows have complete data or if there are missing values\n",
    "- Use `df.tail(10)` to see more rows if you want a larger sample\n",
    "- Look for any unusual patterns or data entry errors in the final records\n",
    "- Verify that the data collection didn't stop abruptly mid-period\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf2fc09",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ¤« <strong>Solution</strong> (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "# Import required libraries and load data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Washington D.C. bike-sharing dataset\n",
    "data_file_path = \"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/dataset/dataset.csv\"\n",
    "df = pd.read_csv(data_file_path)\n",
    "\n",
    "# Your code here - examine the last few rows\n",
    "print(\"Last 5 rows of the dataset:\")\n",
    "print(df.tail())  # Fill in the DataFrame name and method\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1042661",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc510268",
   "metadata": {},
   "source": [
    "## Step 7: Understanding Your Dataset Size and Structure\n",
    "\n",
    "Professional data analysis requires understanding exactly what you're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139244e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed information about the dataset\n",
    "print(\"Dataset Information:\")\n",
    "print(f\"Total records: {len(df)}\")\n",
    "print(f\"Total variables: {len(df.columns)}\")\n",
    "\n",
    "# Show data types for each column\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0447af97",
   "metadata": {},
   "source": [
    "**What this tells you:**\n",
    "- **Total records**: How much historical data your client has collected\n",
    "- **Total variables**: How many different factors you can analyze\n",
    "- **Data types**: What kind of analysis you can perform on each variable\n",
    "\n",
    "More historical data means more reliable predictions. The variety of variables (weather, time, user types) means you can build sophisticated demand forecasting models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c3ccd3",
   "metadata": {},
   "source": [
    "### Challenge 7: Calculate Time Coverage\n",
    "The dataset contains hourly data. Calculate how many days of data you have by dividing the total rows by 24."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe63daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and load data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Washington D.C. bike-sharing dataset\n",
    "data_file_path = \"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/dataset/dataset.csv\"\n",
    "df = pd.read_csv(data_file_path)\n",
    "\n",
    "# Your code here - calculate days of coverage\n",
    "total_days = len(_____) / _____  # Fill in DataFrame name and divisor\n",
    "print(f\"Dataset covers approximately {_____:.1f} days\")  # Fill in variable name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d2aee3",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ <strong>Tip</strong> (click to expand)</summary>\n",
    "\n",
    "\n",
    "When calculating time coverage for time series data, consider these analysis approaches:\n",
    "- Use `:.1f` formatting to display days with one decimal place for readability\n",
    "- Calculate weeks as well: `total_days / 7` for business planning context\n",
    "- Consider if you have complete days: `len(df) % 24` shows any partial days\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8038c973",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ¤« <strong>Solution</strong> (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "# Import required libraries and load data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Washington D.C. bike-sharing dataset\n",
    "data_file_path = \"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/dataset/dataset.csv\"\n",
    "df = pd.read_csv(data_file_path)\n",
    "\n",
    "# Your code here - calculate days of coverage\n",
    "total_days = len(df) / 24  # Fill in DataFrame name and divisor\n",
    "print(f\"Dataset covers approximately {total_days:.1f} days\")  # Fill in variable name\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0914eb8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fa219b",
   "metadata": {},
   "source": [
    "## Step 8: Basic Statistical Summary\n",
    "\n",
    "Understanding the data distribution helps identify patterns and potential issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ab4d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate statistical summary for numerical variables\n",
    "print(\"Statistical Summary:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b665c353",
   "metadata": {},
   "source": [
    "**What this shows:**\n",
    "- **count**: How many non-missing values exist for each variable\n",
    "- **mean**: Average values (useful for understanding typical conditions)\n",
    "- **std**: Standard deviation (shows how much values vary)\n",
    "- **min/max**: Range of values (helps identify outliers or impossible values)\n",
    "- **25%, 50%, 75%**: Quartiles (show data distribution)\n",
    "\n",
    "If minimum bike counts are 1 and maximum is 977, that's a huge range! This suggests your client experiences very different demand conditions that you'll need to understand and predict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ff4817",
   "metadata": {},
   "source": [
    "### Challenge 8: Focus on Key Business Metrics\n",
    "Create a summary focusing only on the key business variables: temperature (`temp`), humidity (`humidity`), and bike count (`count`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fdf8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and load data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Washington D.C. bike-sharing dataset\n",
    "data_file_path = \"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/dataset/dataset.csv\"\n",
    "df = pd.read_csv(data_file_path)\n",
    "\n",
    "# Your code here - create a summary of key metrics\n",
    "key_metrics = _____[[_____, _____, _____]].describe()  # Fill in DataFrame and column names\n",
    "print(\"Key Business Metrics Summary:\")\n",
    "print(_____)  # Print the summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ee5542",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ <strong>Tip</strong> (click to expand)</summary>\n",
    "\n",
    "\n",
    "When focusing on specific business metrics, use these analytical techniques:\n",
    "- Select columns using double brackets: `df[['col1', 'col2']]` to maintain DataFrame structure\n",
    "- Compare ranges across variables to understand which have more variation\n",
    "- Check for outliers: are max values realistic or potentially data entry errors?\n",
    "- Consider business thresholds: what temperature ranges are most relevant for operations?\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77147155",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ¤« <strong>Solution</strong> (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "# Import required libraries and load data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Washington D.C. bike-sharing dataset\n",
    "data_file_path = \"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/dataset/dataset.csv\"\n",
    "df = pd.read_csv(data_file_path)\n",
    "\n",
    "# Your code here - create a summary of key metrics\n",
    "key_metrics = df[['temp', 'humidity', 'count']].describe()  # Fill in DataFrame and column names\n",
    "print(\"Key Business Metrics Summary:\")\n",
    "print(key_metrics)  # Print the summary\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b988b30",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad70c2d0",
   "metadata": {},
   "source": [
    "## Step 9: Selecting Specific Data for Analysis\n",
    "\n",
    "Often you need to focus on specific parts of your dataset. Let's learn several ways to select data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f524feac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a single column (bike counts)\n",
    "bike_counts = df['count']\n",
    "print(f\"Bike counts - Type: {type(bike_counts)}\")\n",
    "print(f\"Average daily rentals: {bike_counts.mean():.1f}\")\n",
    "\n",
    "# Select multiple columns for weather analysis\n",
    "weather_data = df[['temp', 'humidity', 'windspeed']]\n",
    "print(f\"\\nWeather data shape: {weather_data.shape}\")\n",
    "print(weather_data.head(3))\n",
    "\n",
    "# Select first 12 rows for initial analysis\n",
    "sample_data = df.head(12)\n",
    "print(f\"\\nSample data covers first {len(sample_data)} hours\")\n",
    "print(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b45c70b",
   "metadata": {},
   "source": [
    "**What this demonstrates:**\n",
    "- **Single column selection**: Returns a Series (one-dimensional)\n",
    "- **Multiple column selection**: Returns a DataFrame (two-dimensional)\n",
    "- **Row selection**: Gets a subset of the full dataset\n",
    "\n",
    "You might analyze just weather data to understand seasonal patterns, or focus on the first few months to understand how the bike-sharing system performed during its early operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0421f18c",
   "metadata": {},
   "source": [
    "### Challenge 9: Create a Sample Dataset\n",
    "Select only the columns 'datetime', 'temp', 'count' and only the first 168 rows (first week of data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bc8eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and load data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Washington D.C. bike-sharing dataset\n",
    "data_file_path = \"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/dataset/dataset.csv\"\n",
    "df = pd.read_csv(data_file_path)\n",
    "\n",
    "# Your code here - create a focused dataset for rush hour analysis\n",
    "rush_hour_analysis = _____[[_____, _____, _____]].head(_____)  # Fill in details\n",
    "print(f\"Rush hour dataset shape: {_____.shape}\")  # Fill in variable name\n",
    "print(_____)  # Print sample data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5840b3ac",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ <strong>Tip</strong> (click to expand)</summary>\n",
    "\n",
    "\n",
    "When creating focused analysis datasets, use these selection strategies:\n",
    "- Verify the subset size: 168 hours = 7 days Ã— 24 hours = 1 week\n",
    "- Combine column and row selection: `df[['col1', 'col2']].head(n)`\n",
    "- Confirm time coverage: compare first and last datetime values - as you will see, it doesn't match (this may mean that some hour registers are missing)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b469914",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ¤« <strong>Solution</strong> (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "# Import required libraries and load data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Washington D.C. bike-sharing dataset\n",
    "data_file_path = \"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/dataset/dataset.csv\"\n",
    "df = pd.read_csv(data_file_path)\n",
    "\n",
    "# Your code here - create a focused dataset for rush hour analysis\n",
    "rush_hour_analysis = df[['datetime', 'temp', 'count']].head(168)  # Fill in details\n",
    "print(f\"Rush hour dataset shape: {rush_hour_analysis.shape}\")  # Fill in variable name\n",
    "print(rush_hour_analysis)  # Print sample data\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826462f5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842806bb",
   "metadata": {},
   "source": [
    "## Step 10: Understanding Time-Based Data\n",
    "\n",
    "Transportation data is inherently time-based. Let's work with the datetime information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcfa92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datetime column to pandas datetime format\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "print(f\"Datetime conversion successful. Type: {df['datetime'].dtype}\")\n",
    "\n",
    "# Ensure chronological order for time-based operations\n",
    "df = df.sort_values('datetime').reset_index(drop=True)\n",
    "\n",
    "# Extract useful time components\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['day_of_week'] = df['datetime'].dt.day_name()\n",
    "df['month'] = df['datetime'].dt.month\n",
    "\n",
    "# Show the first few rows with time components\n",
    "print(\"\\nData with extracted time components:\")\n",
    "print(df[['datetime', 'hour', 'day_of_week', 'month', 'count']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f3ad00",
   "metadata": {},
   "source": [
    "> Timedelta primer: A Timedelta represents a duration (difference between two timestamps). You'll use `pd.Timedelta(hours=1)` in the next step to flag gaps larger than one hour between records.\n",
    "\n",
    "**What this accomplishes:**\n",
    "- Converts text dates to actual datetime objects for analysis\n",
    "- Extracts hour, day, and month for business analysis\n",
    "- Enables time-based filtering and grouping\n",
    "\n",
    "These time-based features unlock powerful insights. Hour analysis allows you to identify peak usage times for bike rebalancing, while day analysis helps compare weekday vs. weekend patterns. Additionally, month analysis reveals seasonal trends that are crucial for capacity planning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6297c903",
   "metadata": {},
   "source": [
    "### Challenge 10: Find Peak Hour\n",
    "Use the new 'hour' column to find which hour of the day has the highest average bike rentals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cf43dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and load data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Washington D.C. bike-sharing dataset\n",
    "data_file_path = \"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/dataset/dataset.csv\"\n",
    "df = pd.read_csv(data_file_path)\n",
    "\n",
    "# Convert datetime column and extract time components\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['day_of_week'] = df['datetime'].dt.day_name()\n",
    "df['month'] = df['datetime'].dt.month\n",
    "\n",
    "# Your code here - find the peak hour for bike rentals\n",
    "hourly_average = _____.groupby(_____)[_____].mean()  # Fill in DataFrame, grouping column, target column\n",
    "peak_hour = hourly_average._____()  # Fill in method to find maximum index\n",
    "peak_rentals = hourly_average._____()  # Fill in method to get maximum value\n",
    "\n",
    "print(f\"Peak hour: {_____}:00\")  # Fill in variable name\n",
    "print(f\"Average rentals during peak hour: {_____:.1f}\")  # Fill in variable name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d81921",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ <strong>Tip</strong> (click to expand)</summary>\n",
    "\n",
    "\n",
    "When analyzing time-based patterns, use these grouping and aggregation techniques:\n",
    "- `df.groupby('hour')['count'].mean()` calculates average by hour\n",
    "- Use `.idxmax()` to find the index (hour) with maximum value\n",
    "- Use `.max()` to get the actual maximum value\n",
    "- Explore other time patterns: `df.groupby('day_of_week')['count'].mean()`\n",
    "- Consider multiple aggregations: `df.groupby('hour')['count'].agg(['mean', 'std', 'count'])`\n",
    "- Sort results for easier interpretation: `hourly_average.sort_values(ascending=False)`\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9452d05b",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ¤« <strong>Solution</strong> (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "# Import required libraries and load data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Washington D.C. bike-sharing dataset\n",
    "data_file_path = \"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/dataset/dataset.csv\"\n",
    "df = pd.read_csv(data_file_path)\n",
    "\n",
    "# Convert datetime column and extract time components\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['day_of_week'] = df['datetime'].dt.day_name()\n",
    "df['month'] = df['datetime'].dt.month\n",
    "\n",
    "# Your code here - find the peak hour for bike rentals\n",
    "hourly_average = df.groupby('hour')['count'].mean()  # Fill in DataFrame, grouping column, target column\n",
    "peak_hour = hourly_average.idxmax()  # Fill in method to find maximum index\n",
    "peak_rentals = hourly_average.max()  # Fill in method to get maximum value\n",
    "\n",
    "print(f\"Peak hour: {peak_hour}:00\")  # Fill in variable name\n",
    "print(f\"Average rentals during peak hour: {peak_rentals:.1f}\")  # Fill in variable name\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6122105b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fa3189",
   "metadata": {},
   "source": [
    "## Step 11: Data Quality Assessment - Missing Data Detection (Empty Cells)\n",
    "\n",
    "Real-world data often has missing values (empty cells). Let's check if any values are missing from our dataset. Note that we're checking for empty cells here - detecting missing time periods (like skipped hours) requires a different approach that we'll handle in Challenge 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23860aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing data in each column\n",
    "missing_data = df.isnull().sum()\n",
    "print(\"Missing Data Summary:\")\n",
    "print(missing_data)\n",
    "\n",
    "# Calculate percentage of missing data\n",
    "missing_percentage = (missing_data / len(df)) * 100\n",
    "print(\"\\nMissing Data Percentages:\")\n",
    "for column in df.columns:\n",
    "    if missing_data[column] > 0:\n",
    "        print(f\"{column}: {missing_percentage[column]:.1f}%\")\n",
    "    else:\n",
    "        print(f\"{column}: 0% (Complete)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a348ab79",
   "metadata": {},
   "source": [
    "**What this code accomplishes:**\n",
    "- `df.isnull()` creates a DataFrame of True/False values where True indicates a missing value\n",
    "- `.sum()` counts the True values (which represent missing entries) for each column - since True is treated as 1 and False as 0 in arithmetic operations\n",
    "- The percentage calculation shows the proportion of missing data relative to total records\n",
    "- The loop displays results in a business-friendly format, clearly marking complete vs incomplete columns\n",
    "- Converting to percentages helps understand the severity of missing data\n",
    "\n",
    "**What this reveals:**\n",
    "- By running this code, we can see that there is no missing data in the dataset - all columns are complete\n",
    "- Complete columns can be trusted for all analysis without data quality concerns\n",
    "- Missing data would require decisions: drop incomplete rows, exclude unreliable columns, or fill gaps using estimates (like averages or trends) - each choice impacts your analysis quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea51840",
   "metadata": {},
   "source": [
    "### Challenge 11: Identify Missing Datetime Gaps (Skipped Hours)\n",
    "Find where the gap between consecutive `datetime` values is greater than 1 hour â€” these indicate missing hourly records in the time series. This approach works because time series data should have consistent intervals. In our case, records are generated hourly, so any gap larger than 1 hour reveals missing records in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c12e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and load data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Washington D.C. bike-sharing dataset\n",
    "data_file_path = \"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/dataset/dataset.csv\"\n",
    "df = pd.read_csv(data_file_path)\n",
    "\n",
    "# Your code here - detect gaps > 1 hour in the datetime sequence\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df = df.sort_values(by='_____').reset_index(drop=True)  # Fill in the time column\n",
    "\n",
    "time_diffs = df['_____'].diff()  # Fill in the time column\n",
    "gaps = df[time_diffs > pd.Timedelta(hours=_____)]  # Fill in the gap size\n",
    "\n",
    "print(\"Rows that follow a gap > 1 hour:\")\n",
    "print(_____)  # Fill in variable name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e191be0c",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ <strong>Tip</strong> (click to expand)</summary>\n",
    "\n",
    "\n",
    "When detecting missing time periods, keep these practices in mind:\n",
    "- Convert the datetime column to pandas datetime format by using `pd.to_datetime()`\n",
    "- Ensure the data is sorted by time before calling `.diff()`\n",
    "- Remember that `.diff()` calculates the time difference between each row and the previous row\n",
    "- Use `pd.Timedelta(hours=1)` for a clear 1-hour threshold\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd591ee7",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ¤« <strong>Solution</strong> (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "# Import required libraries and load data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Washington D.C. bike-sharing dataset\n",
    "data_file_path = \"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/dataset/dataset.csv\"\n",
    "df = pd.read_csv(data_file_path)\n",
    "\n",
    "# Your code here - detect gaps > 1 hour in the datetime sequence\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df = df.sort_values(by='datetime').reset_index(drop=True)  # Fill in the time column\n",
    "\n",
    "time_diffs = df['datetime'].diff()  # Fill in the time column\n",
    "gaps = df[time_diffs > pd.Timedelta(hours=1)]  # Fill in the gap size\n",
    "\n",
    "print(\"Rows that follow a gap > 1 hour:\")\n",
    "print(gaps)  # Fill in variable name\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59a1b21",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addfe78a",
   "metadata": {},
   "source": [
    "## Step 12: Basic Data Filtering for Business Insights\n",
    "\n",
    "Now we'll use data filtering to answer critical business questions that help optimize bike-sharing operations. By creating targeted subsets of our data, we can identify peak demand periods, understand weather impacts on ridership, and develop insights for operational planning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479348a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find high-demand periods (above average usage)\n",
    "average_rentals = df['count'].mean()\n",
    "high_demand = df[df['count'] > average_rentals]\n",
    "print(f\"High-demand periods: {len(high_demand)} out of {len(df)} total hours\")\n",
    "print(f\"That's {len(high_demand)/len(df)*100:.1f}% of all hours\")\n",
    "\n",
    "# Find cold weather operations (temperature below 8Â°C)\n",
    "cold_weather = df[df['temp'] < 8]\n",
    "print(f\"\\nCold weather operations: {len(cold_weather)} hours\")\n",
    "print(f\"Average rentals in cold weather: {cold_weather['count'].mean():.1f}\")\n",
    "\n",
    "# Compare to warm weather (temperature above 26Â°C)\n",
    "warm_weather = df[df['temp'] >= 26]\n",
    "print(f\"Average rentals in warm weather: {warm_weather['count'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8af47b",
   "metadata": {},
   "source": [
    "The analysis reveals clear patterns in bike-sharing demand:\n",
    "\n",
    "- Out of more than **10,000 recorded hours**, around **40% qualify as high-demand periods**, showing that elevated usage is a regular occurrence rather than an exception\n",
    "- **Weather plays a particularly strong role**: when temperatures drop below **8Â°C**, bike rentals average only about **61 per hour**, reflecting how cold conditions discourage riders\n",
    "- In contrast, when the temperature climbs to **26Â°C or higher**, average rentals surge to over **277 per hour**â€”more than **four times the cold-weather figure**\n",
    "\n",
    "As you can imagine, these findings have significant implications for operational planning and resource allocation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bf8014",
   "metadata": {},
   "source": [
    "### Challenge 12: Weekend vs. Weekday Analysis\n",
    "Filter the data to compare average bike rentals on weekends (Saturday, Sunday) versus weekdays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da225c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and load data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Washington D.C. bike-sharing dataset\n",
    "data_file_path = \"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/dataset/dataset.csv\"\n",
    "df = pd.read_csv(data_file_path)\n",
    "\n",
    "# Convert datetime column and extract time components\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['day_of_week'] = df['datetime'].dt.day_name()\n",
    "df['month'] = df['datetime'].dt.month\n",
    "\n",
    "# Your code here - compare weekend vs weekday bike usage\n",
    "weekend_data = _____[_____['day_of_week'].isin([_____, _____])]  # Fill in details\n",
    "weekday_data = _____[~_____['day_of_week'].isin([_____, _____])]  # Fill in details\n",
    "\n",
    "weekend_avg = weekend_data[_____].mean()  # Fill in column name\n",
    "weekday_avg = weekday_data[_____].mean()  # Fill in column name\n",
    "\n",
    "print(f\"Weekend average rentals: {_____:.1f}\")  # Fill in variable name\n",
    "print(f\"Weekday average rentals: {_____:.1f}\")  # Fill in variable name\n",
    "print(f\"Difference: {abs(_____ - _____):.1f} rentals per hour\")  # Fill in variable names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf26f3e",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ <strong>Tip</strong> (click to expand)</summary>\n",
    "\n",
    "\n",
    "When comparing categorical groups like weekends vs weekdays, use these filtering strategies:\n",
    "- Use `.isin(['value1', 'value2'])` to match multiple values\n",
    "- Use `~` (tilde) for \"not\" to get the inverse: `~df['col'].isin(values)`\n",
    "- Alternative approach: `df['day_of_week'].str.contains('Saturday|Sunday')`\n",
    "- Consider statistical significance: do the groups have meaningful differences?\n",
    "- Calculate percentage difference: `((weekend_avg - weekday_avg) / weekday_avg) * 100`\n",
    "- Explore within-group variation: `weekend_data['count'].std()` vs `weekday_data['count'].std()`\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cd5168",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ¤« <strong>Solution</strong> (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "# Import required libraries and load data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Washington D.C. bike-sharing dataset\n",
    "data_file_path = \"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/dataset/dataset.csv\"\n",
    "df = pd.read_csv(data_file_path)\n",
    "\n",
    "# Convert datetime column and extract time components\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['day_of_week'] = df['datetime'].dt.day_name()\n",
    "df['month'] = df['datetime'].dt.month\n",
    "\n",
    "# Your code here - compare weekend vs weekday bike usage\n",
    "weekend_data = df[df['day_of_week'].isin(['Saturday', 'Sunday'])]  # Fill in details\n",
    "weekday_data = df[~df['day_of_week'].isin(['Saturday', 'Sunday'])]  # Fill in details\n",
    "\n",
    "weekend_avg = weekend_data['count'].mean()  # Fill in column name\n",
    "weekday_avg = weekday_data['count'].mean()  # Fill in column name\n",
    "\n",
    "print(f\"Weekend average rentals: {weekend_avg:.1f}\")  # Fill in variable name\n",
    "print(f\"Weekday average rentals: {weekday_avg:.1f}\")  # Fill in variable name\n",
    "print(f\"Difference: {abs(weekend_avg - weekday_avg):.1f} rentals per hour\")  # Fill in variable names\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e3823a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ed39bb",
   "metadata": {},
   "source": [
    "## Summary: Professional Pandas Data Analysis Fundamentals\n",
    "\n",
    "**What We've Accomplished**: \n",
    "- Established comprehensive pandas environment and data manipulation workflows\n",
    "- Implemented systematic data loading and exploration methodologies for real transportation data\n",
    "- Performed data quality assessment with missing data detection protocols\n",
    "- Created time-based feature extraction and business intelligence filtering frameworks\n",
    "\n",
    "**Key Technical Skills Mastered**:\n",
    "- Series and DataFrame creation with meaningful business labeling systems\n",
    "- CSV data loading and basic analysis for professional client datasets\n",
    "- Temporal data manipulation with datetime extraction and grouping operations\n",
    "- Data filtering and aggregation techniques for business insight generation\n",
    "\n",
    "**Next Steps**: Next, we'll advance to professional data cleaning techniques, mastering missing value handling, outlier identification, and data preparation protocols that ensure our transportation datasets meet the rigorous quality standards required for sophisticated predictive modeling and client-ready analysis.\n",
    "\n",
    "Your bike-sharing client now has a solid data foundation built with professional pandas techniques that demonstrate systematic data exploration and business-focused analytical thinking - the core competencies that consulting firms expect from junior transportation data analysts!"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
