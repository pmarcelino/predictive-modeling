{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3: Programming Example - Pandas Fundamentals with Washington D.C. Data\n",
    "\n",
    "## Introduction: Your First Day as a Data Consultant\n",
    "\n",
    "Welcome to your first hands-on session as a junior data consultant! Today, you'll work with real Washington D.C. bike-sharing data, learning pandas step-by-step. We'll start with absolute basics and build up to loading and exploring your client's dataset.\n",
    "\n",
    "Remember: Every line of code serves a business purpose. You're not just learning programming - you're developing the skills to help your bike-sharing client make better decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Setting Up Your Data Analysis Environment\n",
    "\n",
    "Let's start by importing the tools you'll need for data analysis. Think of this like setting up your workbench before starting a project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas - your primary data manipulation tool\n",
    "import pandas as pd\n",
    "\n",
    "# Print confirmation\n",
    "print(\"Pandas imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What this does:**\n",
    "- `import pandas as pd` makes the pandas library available with the shorthand \"pd\"\n",
    "- The shorthand `pd` is a universal convention - all pandas users worldwide use this\n",
    "- Checking the version ensures you're working with up-to-date tools\n",
    "\n",
    "**Why this matters for your client:**\n",
    "Just like a mechanic checks their tools before working on a car, you verify your data analysis tools are ready before handling valuable business data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Challenge 1: Import Practice\n",
    "Try importing pandas yourself and check if it worked by running `type(pd)`. You should see that pd is now a module object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Tip. Click here for a tip.**\n",
    ">\n",
    "> When testing if pandas imported correctly, you can use these verification approaches:\n",
    "> - `type(pd)` shows the object type (should be `<class 'module'>`)\n",
    "> - `pd.__version__` displays the pandas version number\n",
    "> - `dir(pd)` lists all available pandas functions and attributes\n",
    "> - If you get a `NameError`, the import failed and you need to run the import statement first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "import pandas as pd\n",
    "print(type(pd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "import pandas as pd\n",
    "print(type(pd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Understanding Series - Single Column Data\n",
    "\n",
    "Let's start with Series by creating bike rental data for a typical Monday morning. Think of Series as a single column from a spreadsheet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Series with hourly bike rentals\n",
    "morning_rentals = pd.Series([15, 23, 45, 67, 89, 156, 234, 287])\n",
    "print(\"Morning Bike Rentals:\")\n",
    "print(morning_rentals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What this does:**\n",
    "- `pd.Series([...])` creates a pandas Series object\n",
    "- The numbers represent bike rentals for each hour (6 AM through 1 PM)\n",
    "- Pandas automatically assigns index numbers (0, 1, 2, etc.)\n",
    "\n",
    "**Output you'll see:**\n",
    "```\n",
    "0     15\n",
    "1     23\n",
    "2     45\n",
    "3     67\n",
    "4     89\n",
    "5    156\n",
    "6    234\n",
    "7    287\n",
    "dtype: int64\n",
    "```\n",
    "\n",
    "**Understanding the output:**\n",
    "- Left column (0, 1, 2...): Index numbers (like row numbers)\n",
    "- Right column (15, 23, 45...): Actual bike rental counts\n",
    "- Bottom line: Data type information (int64 means integer numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Create Your Own Series\n",
    "Create a Series representing temperature readings for the same morning hours. Use these temperatures: [42, 44, 47, 50, 53, 56, 58, 61]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Tip (click to expand)</summary>\n",
    "\n",
    "> When creating Series objects, consider these best practices:  \n",
    "> \n",
    "> - Use descriptive variable names like `morning_temps` instead of generic names like `data`  \n",
    "> - Print the Series to verify it contains the expected values and structure  \n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas for Series creation\n",
    "import pandas as pd\n",
    "\n",
    "# Your code here - create a Series called 'morning_temps'\n",
    "morning_temps = pd.Series(_____) # Fill in the temperature list\n",
    "print(_____) # Print your Series to verify it worked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Solution (click to expand)</summary>\n",
    "# Import pandas for Series creation\n",
    "import pandas as pd\n",
    "\n",
    "# Your code here - create a Series called 'morning_temps'\n",
    "morning_temps = pd.Series([42, 44, 47, 50, 53, 56, 58, 61]) # Fill in the temperature list\n",
    "print(morning_temps) # Print your Series to verify it worked\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Adding Meaningful Labels to Series\n",
    "\n",
    "Raw index numbers (0, 1, 2) aren't very business-friendly. Let's add meaningful labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Series with meaningful hour labels\n",
    "hour_labels = ['6 AM', '7 AM', '8 AM', '9 AM', '10 AM', '11 AM', '12 PM', '1 PM']\n",
    "morning_rentals_labeled = pd.Series([15, 23, 45, 67, 89, 156, 234, 287], \n",
    "                                   index=hour_labels)\n",
    "print(\"Labeled Morning Bike Rentals:\")\n",
    "print(morning_rentals_labeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What this does:**\n",
    "- `index=hour_labels` replaces default numbers with business-meaningful labels\n",
    "- Now each rental count is clearly connected to its time period\n",
    "\n",
    "**Output you'll see:**\n",
    "```\n",
    "6 AM     15\n",
    "7 AM     23\n",
    "8 AM     45\n",
    "9 AM     67\n",
    "10 AM    89\n",
    "11 AM   156\n",
    "12 PM   234\n",
    "1 PM    287\n",
    "dtype: int64\n",
    "```\n",
    "\n",
    "**Business value:**\n",
    "Now when you show this to your client, they immediately understand that 12 PM has the highest rentals (234), which makes perfect sense for lunch-time bike usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: Access Specific Data Points\n",
    "Using your labeled Series, find the bike rentals at 9 AM. Use this syntax: `morning_rentals_labeled['9 AM']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Tip. Click here for a tip.**\n",
    "> \n",
    "> When accessing Series data by label, keep these techniques in mind:\n",
    "> - Use square brackets with the exact label: `series_name['label']`\n",
    "> - Check available labels with `series_name.index` to see all options\n",
    "> - Access multiple values: `series_name[['9 AM', '10 AM']]` (note double brackets)\n",
    "> - Use `.loc[]` for explicit label-based selection: `series_name.loc['9 AM']`\n",
    "> - Be careful with exact spelling and spacing in labels to avoid KeyError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas and create the labeled Series\n",
    "import pandas as pd\n",
    "\n",
    "# Create Series with meaningful hour labels\n",
    "hour_labels = ['6 AM', '7 AM', '8 AM', '9 AM', '10 AM', '11 AM', '12 PM', '1 PM']\n",
    "morning_rentals_labeled = pd.Series([15, 23, 45, 67, 89, 156, 234, 287], \n",
    "                                   index=hour_labels)\n",
    "\n",
    "# Your code here - access the 9 AM value from the labeled Series\n",
    "nine_am_rentals = morning_rentals_labeled[_____] # Fill in the correct label\n",
    "print(f\"Bike rentals at 9 AM: {_____}\") # Print the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas and create the labeled Series\n",
    "import pandas as pd\n",
    "\n",
    "# Create Series with meaningful hour labels\n",
    "hour_labels = ['6 AM', '7 AM', '8 AM', '9 AM', '10 AM', '11 AM', '12 PM', '1 PM']\n",
    "morning_rentals_labeled = pd.Series([15, 23, 45, 67, 89, 156, 234, 287], \n",
    "                                   index=hour_labels)\n",
    "\n",
    "# Your code here - access the 9 AM value from the labeled Series\n",
    "nine_am_rentals = morning_rentals_labeled['9 AM'] # Fill in the correct label\n",
    "print(f\"Bike rentals at 9 AM: {nine_am_rentals}\") # Print the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Creating Your First DataFrame - Complete Business Data\n",
    "\n",
    "Now let's combine multiple pieces of information into a DataFrame. Think of this as creating a complete spreadsheet with multiple columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive DataFrame with multiple variables\n",
    "bike_operations_data = pd.DataFrame({\n",
    "    'hour': ['6 AM', '7 AM', '8 AM', '9 AM', '10 AM'],\n",
    "    'temperature': [42, 44, 47, 50, 53],\n",
    "    'bike_rentals': [15, 23, 45, 67, 89],\n",
    "    'weather_condition': ['Clear', 'Clear', 'Partly Cloudy', 'Clear', 'Clear']\n",
    "})\n",
    "\n",
    "print(\"Complete Bike Operations Data:\")\n",
    "print(bike_operations_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What this does:**\n",
    "- `pd.DataFrame({...})` creates a DataFrame with multiple columns\n",
    "- Each key ('hour', 'temperature', etc.) becomes a column name\n",
    "- Each list becomes the values for that column\n",
    "- All rows stay aligned (first hour corresponds to first temperature, etc.)\n",
    "\n",
    "**Output you'll see:**\n",
    "```\n",
    "    hour  temperature  bike_rentals weather_condition\n",
    "0   6 AM           42            15             Clear\n",
    "1   7 AM           44            23             Clear\n",
    "2   8 AM           47            45    Partly Cloudy\n",
    "3   9 AM           50            67             Clear\n",
    "4  10 AM           53            89             Clear\n",
    "```\n",
    "\n",
    "**Business insight:**\n",
    "You can immediately see patterns - bike rentals increase with temperature and time, giving your client valuable operational insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 4: Add a New Column\n",
    "Add a column called 'user_satisfaction' with values [3.2, 3.5, 3.8, 4.1, 4.3] representing customer satisfaction ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Tip. Click here for a tip.**\n",
    "> \n",
    "> When adding new columns to DataFrames, follow these best practices:\n",
    "> - Ensure the new data list has the same length as existing rows: `len(new_data) == len(df)`\n",
    "> - Use descriptive column names that clearly indicate what the data represents\n",
    "> - Verify the addition worked: `df.columns` shows all column names including the new one\n",
    "> - Check data types: `df.dtypes` to ensure the new column has appropriate type (float64 for ratings)\n",
    "> - You can also add columns using `df.assign(column_name=values)` for a more functional approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas and create the DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "# Create a comprehensive DataFrame with multiple variables\n",
    "bike_operations_data = pd.DataFrame({\n",
    "    'hour': ['6 AM', '7 AM', '8 AM', '9 AM', '10 AM'],\n",
    "    'temperature': [42, 44, 47, 50, 53],\n",
    "    'bike_rentals': [15, 23, 45, 67, 89],\n",
    "    'weather_condition': ['Clear', 'Clear', 'Partly Cloudy', 'Clear', 'Clear']\n",
    "})\n",
    "\n",
    "# Your code here - add a new column with satisfaction ratings\n",
    "bike_operations_data[_____] = [3.2, 3.5, 3.8, 4.1, 4.3] # Fill in column name\n",
    "print(_____) # Print the updated DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas and create the DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "# Create a comprehensive DataFrame with multiple variables\n",
    "bike_operations_data = pd.DataFrame({\n",
    "    'hour': ['6 AM', '7 AM', '8 AM', '9 AM', '10 AM'],\n",
    "    'temperature': [42, 44, 47, 50, 53],\n",
    "    'bike_rentals': [15, 23, 45, 67, 89],\n",
    "    'weather_condition': ['Clear', 'Clear', 'Partly Cloudy', 'Clear', 'Clear']\n",
    "})\n",
    "\n",
    "# Your code here - add a new column with satisfaction ratings\n",
    "bike_operations_data['user_satisfaction'] = [3.2, 3.5, 3.8, 4.1, 4.3] # Fill in column name\n",
    "print(bike_operations_data) # Print the updated DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Loading Real Washington D.C. Dataset\n",
    "\n",
    "Now for the real challenge - loading your client's actual historical data. This is where professional consulting begins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Washington D.C. bike-sharing dataset\n",
    "data_file_path = \"dataset.csv\"\n",
    "df = pd.read_csv(data_file_path)\n",
    "\n",
    "# Confirm successful loading\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape[0]} rows Ã— {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What this does:**\n",
    "- `pd.read_csv()` reads data from a CSV file into a DataFrame\n",
    "- CSV (Comma-Separated Values) is the standard format for sharing tabular data\n",
    "- The shape tells you how much data you have to work with\n",
    "\n",
    "**Professional tip:**\n",
    "Always check the shape immediately after loading - it confirms the file loaded correctly and gives you a sense of your dataset size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 5: Explore the Column Names\n",
    "Print the column names using `df.columns` to see what variables are available in your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Tip. Click here for a tip.**\n",
    "> \n",
    "> When exploring column names in a new dataset, use these investigation techniques:\n",
    "> - `df.columns` returns an Index object with all column names\n",
    "> - `list(df.columns)` converts to a regular Python list for easier reading\n",
    "> - `len(df.columns)` tells you how many variables you have to work with\n",
    "> - Look for patterns in naming conventions to understand data structure\n",
    "> - Identify key business variables (dates, counts, categories) vs. supporting variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and load data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Washington D.C. bike-sharing dataset\n",
    "data_file_path = \"dataset.csv\"\n",
    "df = pd.read_csv(data_file_path)\n",
    "\n",
    "# Your code here - explore the available columns\n",
    "print(\"Available columns:\")\n",
    "print(list(_____.columns)) # Fill in the DataFrame name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and load data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Washington D.C. bike-sharing dataset\n",
    "data_file_path = \"dataset.csv\"\n",
    "df = pd.read_csv(data_file_path)\n",
    "\n",
    "# Your code here - explore the available columns\n",
    "print(\"Available columns:\")\n",
    "print(list(df.columns)) # Fill in the DataFrame name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: First Look at Real Transportation Data\n",
    "\n",
    "Let's examine the first few rows to understand the data structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 rows\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What this shows:**\n",
    "- `head()` displays the first 5 rows by default\n",
    "- You'll see actual bike-sharing data with timestamps, weather, and usage counts\n",
    "- Each row represents one hour of bike-sharing operations\n",
    "\n",
    "**Understanding the real data:**\n",
    "- `datetime`: When this data was recorded\n",
    "- `season`, `holiday`, `workingday`: Operational context\n",
    "- `weather`, `temp`, `humidity`, `windspeed`: Weather conditions\n",
    "- `casual`, `registered`, `count`: Different types of users and total rentals\n",
    "\n",
    "This is the foundation of all your future analysis for this client."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 6: Look at the Last Few Rows\n",
    "Use `df.tail()` to see the last 5 rows of the dataset. This helps verify you have complete data coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Tip. Click here for a tip.**\n",
    "> \n",
    "> When examining the end of your dataset, consider these data quality checks:\n",
    "> - Compare last row's datetime to first row's datetime to understand time coverage\n",
    "> - Check if the last rows have complete data or if there are missing values\n",
    "> - Use `df.tail(10)` to see more rows if you want a larger sample\n",
    "> - Look for any unusual patterns or data entry errors in the final records\n",
    "> - Verify that the data collection didn't stop abruptly mid-period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and load data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Washington D.C. bike-sharing dataset\n",
    "data_file_path = \"dataset.csv\"\n",
    "df = pd.read_csv(data_file_path)\n",
    "\n",
    "# Your code here - examine the last few rows\n",
    "print(\"Last 5 rows of the dataset:\")\n",
    "print(_____._____)  # Fill in the DataFrame name and method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and load data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Washington D.C. bike-sharing dataset\n",
    "data_file_path = \"dataset.csv\"\n",
    "df = pd.read_csv(data_file_path)\n",
    "\n",
    "# Your code here - examine the last few rows\n",
    "print(\"Last 5 rows of the dataset:\")\n",
    "print(df.tail())  # Fill in the DataFrame name and method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Understanding Your Dataset Size and Structure\n",
    "\n",
    "Professional data analysis requires understanding exactly what you're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed information about the dataset\n",
    "print(\"Dataset Information:\")\n",
    "print(f\"Total records: {len(df)}\")\n",
    "print(f\"Total variables: {len(df.columns)}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024:.1f} KB\")\n",
    "\n",
    "# Show data types for each column\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What this tells you:**\n",
    "- **Total records**: How much historical data your client has collected\n",
    "- **Total variables**: How many different factors you can analyze\n",
    "- **Memory usage**: Whether your computer can handle this dataset efficiently\n",
    "- **Data types**: What kind of analysis you can perform on each variable\n",
    "\n",
    "**Business implications:**\n",
    "More historical data means more reliable predictions. The variety of variables (weather, time, user types) means you can build sophisticated demand forecasting models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 7: Calculate Time Coverage\n",
    "The dataset contains hourly data. Calculate how many days of data you have by dividing the total rows by 24."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Tip. Click here for a tip.**\n",
    "> \n",
    "> When calculating time coverage for time series data, consider these analysis approaches:\n",
    "> - Use `.1f` formatting to display days with one decimal place for readability\n",
    "> - Verify your calculation makes sense: `total_days * 24` should equal `len(df)`\n",
    "> - Calculate weeks as well: `total_days / 7` for business planning context\n",
    "> - Consider if you have complete days: `len(df) % 24` shows any partial days\n",
    "> - More sophisticated approach: use actual datetime range with `(df['datetime'].max() - df['datetime'].min()).days`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and load data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Washington D.C. bike-sharing dataset\n",
    "data_file_path = \"dataset.csv\"\n",
    "df = pd.read_csv(data_file_path)\n",
    "\n",
    "# Your code here - calculate days of coverage\n",
    "total_days = len(_____) / _____ # Fill in DataFrame name and divisor\n",
    "print(f\"Dataset covers approximately {_____:.1f} days\") # Fill in variable name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and load data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Washington D.C. bike-sharing dataset\n",
    "data_file_path = \"dataset.csv\"\n",
    "df = pd.read_csv(data_file_path)\n",
    "\n",
    "# Your code here - calculate days of coverage\n",
    "total_days = len(df) / 24 # Fill in DataFrame name and divisor\n",
    "print(f\"Dataset covers approximately {total_days:.1f} days\") # Fill in variable name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 8: Basic Statistical Summary\n",
    "\n",
    "Understanding the data distribution helps identify patterns and potential issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate statistical summary for numerical variables\n",
    "print(\"Statistical Summary:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What this shows:**\n",
    "- **count**: How many non-missing values exist for each variable\n",
    "- **mean**: Average values (useful for understanding typical conditions)\n",
    "- **std**: Standard deviation (shows how much values vary)\n",
    "- **min/max**: Range of values (helps identify outliers or impossible values)\n",
    "- **25%, 50%, 75%**: Quartiles (show data distribution)\n",
    "\n",
    "**Professional insight:**\n",
    "If minimum bike counts are 1 and maximum is 977, that's a huge range! This suggests your client experiences very different demand conditions that you'll need to understand and predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 8: Focus on Key Business Metrics\n",
    "Create a summary focusing only on the key business variables: temperature, humidity, and bike count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Tip. Click here for a tip.**\n",
    "> \n",
    "> When focusing on specific business metrics, use these analytical techniques:\n",
    "> - Select columns using double brackets: `df[['col1', 'col2']]` to maintain DataFrame structure\n",
    "> - Compare ranges across variables to understand which have more variation\n",
    "> - Look for correlations: do higher temperatures generally coincide with higher bike counts?\n",
    "> - Check for outliers: are max values realistic or potentially data entry errors?\n",
    "> - Consider business thresholds: what temperature ranges are most relevant for operations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and load data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Washington D.C. bike-sharing dataset\n",
    "data_file_path = \"dataset.csv\"\n",
    "df = pd.read_csv(data_file_path)\n",
    "\n",
    "# Your code here - create a summary of key metrics\n",
    "key_metrics = _____[[_____, _____, _____]].describe() # Fill in DataFrame and column names\n",
    "print(\"Key Business Metrics Summary:\")\n",
    "print(_____) # Print the summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and load data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Washington D.C. bike-sharing dataset\n",
    "data_file_path = \"dataset.csv\"\n",
    "df = pd.read_csv(data_file_path)\n",
    "\n",
    "# Your code here - create a summary of key metrics\n",
    "key_metrics = df[['temp', 'humidity', 'count']].describe() # Fill in DataFrame and column names\n",
    "print(\"Key Business Metrics Summary:\")\n",
    "print(key_metrics) # Print the summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 9: Selecting Specific Data for Analysis\n",
    "\n",
    "Often you need to focus on specific parts of your dataset. Let's learn several ways to select data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a single column (bike counts)\n",
    "bike_counts = df['count']\n",
    "print(f\"Bike counts - Type: {type(bike_counts)}\")\n",
    "print(f\"Average daily rentals: {bike_counts.mean():.1f}\")\n",
    "\n",
    "# Select multiple columns for weather analysis\n",
    "weather_data = df[['temp', 'humidity', 'windspeed']]\n",
    "print(f\"\\nWeather data shape: {weather_data.shape}\")\n",
    "print(weather_data.head(3))\n",
    "\n",
    "# Select first 100 rows for initial analysis\n",
    "sample_data = df.head(100)\n",
    "print(f\"\\nSample data covers first {len(sample_data)} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What this demonstrates:**\n",
    "- **Single column selection**: Returns a Series (one-dimensional)\n",
    "- **Multiple column selection**: Returns a DataFrame (two-dimensional)\n",
    "- **Row selection**: Gets a subset of the full dataset\n",
    "\n",
    "**Professional application:**\n",
    "You might analyze just weather data to understand seasonal patterns, or focus on the first few months to understand how the bike-sharing system performed during its early operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 9: Create a Rush Hour Analysis Dataset\n",
    "Select only the columns 'datetime', 'temp', 'count' and only the first 168 rows (first week of data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Tip. Click here for a tip.**\n",
    "> \n",
    "> When creating focused analysis datasets, use these selection strategies:\n",
    "> - Combine column and row selection: `df[['col1', 'col2']].head(n)`\n",
    "> - Verify the subset size: 168 hours = 7 days Ã— 24 hours = 1 week\n",
    "> - Check that you have the right columns: `rush_hour_analysis.columns`\n",
    "> - Confirm time coverage: compare first and last datetime values\n",
    "> - Consider if this sample represents typical operations or includes holidays/special events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and load data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Washington D.C. bike-sharing dataset\n",
    "data_file_path = \"dataset.csv\"\n",
    "df = pd.read_csv(data_file_path)\n",
    "\n",
    "# Your code here - create a focused dataset for rush hour analysis\n",
    "rush_hour_analysis = _____[[_____, _____, _____]].head(_____) # Fill in details\n",
    "print(f\"Rush hour dataset shape: {_____.shape}\") # Fill in variable name\n",
    "print(_____.head()) # Print first few rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and load data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Washington D.C. bike-sharing dataset\n",
    "data_file_path = \"dataset.csv\"\n",
    "df = pd.read_csv(data_file_path)\n",
    "\n",
    "# Your code here - create a focused dataset for rush hour analysis\n",
    "rush_hour_analysis = df[['datetime', 'temp', 'count']].head(168) # Fill in details\n",
    "print(f\"Rush hour dataset shape: {rush_hour_analysis.shape}\") # Fill in variable name\n",
    "print(rush_hour_analysis.head()) # Print first few rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 10: Data Quality Assessment - Missing Data Detection\n",
    "\n",
    "Real-world data is often incomplete. Let's check for missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing data in each column\n",
    "missing_data = df.isnull().sum()\n",
    "print(\"Missing Data Summary:\")\n",
    "print(missing_data)\n",
    "\n",
    "# Calculate percentage of missing data\n",
    "missing_percentage = (missing_data / len(df)) * 100\n",
    "print(\"\\nMissing Data Percentages:\")\n",
    "for column in df.columns:\n",
    "    if missing_data[column] > 0:\n",
    "        print(f\"{column}: {missing_percentage[column]:.1f}%\")\n",
    "    else:\n",
    "        print(f\"{column}: 0% (Complete)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What this reveals:**\n",
    "- `isnull().sum()` counts missing values in each column\n",
    "- Converting to percentages helps understand the severity of missing data\n",
    "- Complete columns can be trusted for all analysis\n",
    "- Columns with missing data need special handling\n",
    "\n",
    "**Professional implication:**\n",
    "If weather data is missing for certain periods, you'll need to either exclude those periods or find ways to estimate the missing values. This affects the reliability of your predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 10: Identify the Most Complete Variables\n",
    "Find which columns have zero missing values - these are your most reliable variables for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Tip. Click here for a tip.**\n",
    "> \n",
    "> When identifying complete variables, use these data quality approaches:\n",
    "> - Alternative approach: `df.isnull().sum() == 0` returns a boolean Series of complete columns\n",
    "> - Get complete columns directly: `complete_cols = df.columns[df.isnull().sum() == 0].tolist()`\n",
    "> - Count complete columns: `(df.isnull().sum() == 0).sum()`\n",
    "> - Prioritize these complete columns for initial analysis and model building\n",
    "> - Consider why some columns have missing data - is it systematic or random?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and load data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Washington D.C. bike-sharing dataset\n",
    "data_file_path = \"dataset.csv\"\n",
    "df = pd.read_csv(data_file_path)\n",
    "\n",
    "# Your code here - find columns with no missing values\n",
    "complete_columns = []\n",
    "for column in _____.columns: # Fill in DataFrame name\n",
    "    if _____[column].isnull().sum() == _____: # Fill in DataFrame name and comparison value\n",
    "        complete_columns.append(column)\n",
    "\n",
    "print(\"Completely populated columns:\")\n",
    "for col in complete_columns:\n",
    "    print(f\"- {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and load data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Washington D.C. bike-sharing dataset\n",
    "data_file_path = \"dataset.csv\"\n",
    "df = pd.read_csv(data_file_path)\n",
    "\n",
    "# Your code here - find columns with no missing values\n",
    "complete_columns = []\n",
    "for column in df.columns: # Fill in DataFrame name\n",
    "    if df[column].isnull().sum() == 0: # Fill in DataFrame name and comparison value\n",
    "        complete_columns.append(column)\n",
    "\n",
    "print(\"Completely populated columns:\")\n",
    "for col in complete_columns:\n",
    "    print(f\"- {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 11: Understanding Time-Based Data\n",
    "\n",
    "Transportation data is inherently time-based. Let's work with the datetime information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datetime column to pandas datetime format\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "print(f\"Datetime conversion successful. Type: {df['datetime'].dtype}\")\n",
    "\n",
    "# Extract useful time components\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['day_of_week'] = df['datetime'].dt.day_name()\n",
    "df['month'] = df['datetime'].dt.month\n",
    "\n",
    "# Show the first few rows with time components\n",
    "print(\"\\nData with extracted time components:\")\n",
    "print(df[['datetime', 'hour', 'day_of_week', 'month', 'count']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What this accomplishes:**\n",
    "- Converts text dates to actual datetime objects for analysis\n",
    "- Extracts hour, day, and month for business analysis\n",
    "- Enables time-based filtering and grouping\n",
    "\n",
    "**Business applications:**\n",
    "- **Hour analysis**: Identify peak usage times for bike rebalancing\n",
    "- **Day analysis**: Compare weekday vs. weekend patterns\n",
    "- **Month analysis**: Understand seasonal trends for capacity planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 11: Find Peak Hour\n",
    "Use the new 'hour' column to find which hour of the day has the highest average bike rentals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Tip. Click here for a tip.**\n",
    "> \n",
    "> When analyzing time-based patterns, use these grouping and aggregation techniques:\n",
    "> - `df.groupby('hour')['count'].mean()` calculates average by hour\n",
    "> - Use `.idxmax()` to find the index (hour) with maximum value\n",
    "> - Use `.max()` to get the actual maximum value\n",
    "> - Explore other time patterns: `df.groupby('day_of_week')['count'].mean()`\n",
    "> - Consider multiple aggregations: `df.groupby('hour')['count'].agg(['mean', 'std', 'count'])`\n",
    "> - Sort results for easier interpretation: `hourly_average.sort_values(ascending=False)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and load data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Washington D.C. bike-sharing dataset\n",
    "data_file_path = \"dataset.csv\"\n",
    "df = pd.read_csv(data_file_path)\n",
    "\n",
    "# Convert datetime column and extract time components\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['day_of_week'] = df['datetime'].dt.day_name()\n",
    "df['month'] = df['datetime'].dt.month\n",
    "\n",
    "# Your code here - find the peak hour for bike rentals\n",
    "hourly_average = _____.groupby(_____)[_____].mean() # Fill in DataFrame, grouping column, target column\n",
    "peak_hour = hourly_average._____() # Fill in method to find maximum index\n",
    "peak_rentals = hourly_average._____() # Fill in method to get maximum value\n",
    "\n",
    "print(f\"Peak hour: {_____}:00\") # Fill in variable name\n",
    "print(f\"Average rentals during peak hour: {_____:.1f}\") # Fill in variable name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and load data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Washington D.C. bike-sharing dataset\n",
    "data_file_path = \"dataset.csv\"\n",
    "df = pd.read_csv(data_file_path)\n",
    "\n",
    "# Convert datetime column and extract time components\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['day_of_week'] = df['datetime'].dt.day_name()\n",
    "df['month'] = df['datetime'].dt.month\n",
    "\n",
    "# Your code here - find the peak hour for bike rentals\n",
    "hourly_average = df.groupby('hour')['count'].mean() # Fill in DataFrame, grouping column, target column\n",
    "peak_hour = hourly_average.idxmax() # Fill in method to find maximum index\n",
    "peak_rentals = hourly_average.max() # Fill in method to get maximum value\n",
    "\n",
    "print(f\"Peak hour: {peak_hour}:00\") # Fill in variable name\n",
    "print(f\"Average rentals during peak hour: {peak_rentals:.1f}\") # Fill in variable name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 12: Basic Data Filtering for Business Insights\n",
    "\n",
    "Let's filter the data to answer specific business questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find high-demand periods (above average usage)\n",
    "average_rentals = df['count'].mean()\n",
    "high_demand = df[df['count'] > average_rentals]\n",
    "print(f\"High-demand periods: {len(high_demand)} out of {len(df)} total hours\")\n",
    "print(f\"That's {len(high_demand)/len(df)*100:.1f}% of all hours\")\n",
    "\n",
    "# Find cold weather operations (temperature below 50)\n",
    "cold_weather = df[df['temp'] < 50]\n",
    "print(f\"\\nCold weather operations: {len(cold_weather)} hours\")\n",
    "print(f\"Average rentals in cold weather: {cold_weather['count'].mean():.1f}\")\n",
    "\n",
    "# Compare to warm weather\n",
    "warm_weather = df[df['temp'] >= 50]\n",
    "print(f\"Average rentals in warm weather: {warm_weather['count'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Business insights generated:**\n",
    "- **High-demand identification**: Helps predict when extra bikes will be needed\n",
    "- **Weather impact analysis**: Shows how temperature affects demand\n",
    "- **Operational planning**: Cold weather requires different preparation than warm weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 12: Weekend vs. Weekday Analysis\n",
    "Filter the data to compare average bike rentals on weekends (Saturday, Sunday) versus weekdays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Tip. Click here for a tip.**\n",
    "> \n",
    "> When comparing categorical groups like weekends vs weekdays, use these filtering strategies:\n",
    "> - Use `.isin(['value1', 'value2'])` to match multiple values\n",
    "> - Use `~` (tilde) for \"not\" to get the inverse: `~df['col'].isin(values)`\n",
    "> - Alternative approach: `df['day_of_week'].str.contains('Saturday|Sunday')`\n",
    "> - Consider statistical significance: do the groups have meaningful differences?\n",
    "> - Calculate percentage difference: `((weekend_avg - weekday_avg) / weekday_avg) * 100`\n",
    "> - Explore within-group variation: `weekend_data['count'].std()` vs `weekday_data['count'].std()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and load data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Washington D.C. bike-sharing dataset\n",
    "data_file_path = \"dataset.csv\"\n",
    "df = pd.read_csv(data_file_path)\n",
    "\n",
    "# Convert datetime column and extract time components\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['day_of_week'] = df['datetime'].dt.day_name()\n",
    "df['month'] = df['datetime'].dt.month\n",
    "\n",
    "# Your code here - compare weekend vs weekday bike usage\n",
    "weekend_data = _____[_____['day_of_week'].isin([_____, _____])] # Fill in details\n",
    "weekday_data = _____[~_____['day_of_week'].isin([_____, _____])] # Fill in details\n",
    "\n",
    "weekend_avg = weekend_data[_____].mean() # Fill in column name\n",
    "weekday_avg = weekday_data[_____].mean() # Fill in column name\n",
    "\n",
    "print(f\"Weekend average rentals: {_____:.1f}\") # Fill in variable name\n",
    "print(f\"Weekday average rentals: {_____:.1f}\") # Fill in variable name\n",
    "print(f\"Difference: {abs(_____ - _____):.1f} rentals per hour\") # Fill in variable names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and load data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Washington D.C. bike-sharing dataset\n",
    "data_file_path = \"dataset.csv\"\n",
    "df = pd.read_csv(data_file_path)\n",
    "\n",
    "# Convert datetime column and extract time components\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['day_of_week'] = df['datetime'].dt.day_name()\n",
    "df['month'] = df['datetime'].dt.month\n",
    "\n",
    "# Your code here - compare weekend vs weekday bike usage\n",
    "weekend_data = df[df['day_of_week'].isin(['Saturday', 'Sunday'])] # Fill in details\n",
    "weekday_data = df[~df['day_of_week'].isin(['Saturday', 'Sunday'])] # Fill in details\n",
    "\n",
    "weekend_avg = weekend_data['count'].mean() # Fill in column name\n",
    "weekday_avg = weekday_data['count'].mean() # Fill in column name\n",
    "\n",
    "print(f\"Weekend average rentals: {weekend_avg:.1f}\") # Fill in variable name\n",
    "print(f\"Weekday average rentals: {weekday_avg:.1f}\") # Fill in variable name\n",
    "print(f\"Difference: {abs(weekend_avg - weekday_avg):.1f} rentals per hour\") # Fill in variable names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: Professional Pandas Data Analysis Fundamentals\n",
    "\n",
    "**What We've Accomplished**: \n",
    "- Established comprehensive pandas environment and data manipulation workflows\n",
    "- Implemented systematic data loading and exploration methodologies for real transportation data\n",
    "- Performed professional data quality assessment with missing data detection protocols\n",
    "- Created time-based feature extraction and business intelligence filtering frameworks\n",
    "\n",
    "**Key Technical Skills Mastered**:\n",
    "- Series and DataFrame creation with meaningful business labeling systems\n",
    "- CSV data loading and structural analysis for professional client datasets\n",
    "- Temporal data manipulation with datetime extraction and grouping operations\n",
    "- Data filtering and aggregation techniques for business insight generation\n",
    "\n",
    "**Next Steps**: Next, we'll advance to professional data cleaning techniques, mastering missing value handling, outlier identification, and data preparation protocols that ensure our transportation datasets meet the rigorous quality standards required for sophisticated predictive modeling and client-ready analysis.\n",
    "\n",
    "Your bike-sharing client now has a solid data foundation built with professional pandas techniques that demonstrate systematic data exploration and business-focused analytical thinking - the core competencies that consulting firms expect from junior transportation data analysts!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
