{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d013bef7",
   "metadata": {},
   "source": [
    "# Lecture 4: Data Quality & Cleaning Essentials - Professional Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619f7d40",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this lecture, you will be able to:\n",
    "- Identify and categorize different types of data quality issues in transportation datasets\n",
    "- Apply systematic approaches to detect inconsistencies, missing data, and outliers\n",
    "- Choose appropriate strategies for handling data quality issues\n",
    "- Implement professional data cleaning workflows using pandas methods\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5b7f65",
   "metadata": {},
   "source": [
    "## 1. The Reality of Real-World Transportation Data\n",
    "\n",
    "As your consulting work progresses, you'll quickly discover a fundamental truth: **real-world data is messy**. The clean datasets you see in textbooks don't exist in professional practice. Your bike-sharing client's data comes from sensors that malfunction, weather stations that go offline, and databases that occasionally corrupt records.\n",
    "\n",
    "This messiness isn't just a technical inconvenience - it's a business-critical challenge. **Poor data quality can lead to incorrect demand predictions**, resulting in empty bike stations when customers need them or overflow situations where returning bikes becomes impossible. These operational failures directly impact customer satisfaction and revenue.\n",
    "\n",
    "Your role as a professional consultant is to **transform messy, incomplete data into reliable foundations for business decision-making**. This requires systematic approaches, clear documentation, and transparent communication about data limitations and cleaning procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408924dd",
   "metadata": {},
   "source": [
    "## 2. Common Data Quality Issues in Transportation Systems\n",
    "\n",
    "Messiness isn’t a single problem—it comes in many forms, each affecting your analysis in different ways. As a consultant, your job is to **recognize these issues early** and decide how to handle them before they undermine your predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbdb93d",
   "metadata": {},
   "source": [
    "### 2.1. Understanding Data Quality Dimensions\n",
    "\n",
    "Data quality exists along multiple dimensions, each of which can influence your analysis in different ways. Let’s break down the five most important ones and see how they appear in transportation data:\n",
    "\n",
    "1. **Completeness**\n",
    "   - *Definition:* Whether all expected values are present in the dataset.\n",
    "   - *Why it matters:* Missing values create blind spots, especially when the missingness is not random.\n",
    "   - *Example:* If bike-sharing sensors fail during storms, you may lose exactly the data needed to understand weather impacts on demand.\n",
    "\n",
    "2. **Accuracy**\n",
    "   - *Definition:* The extent to which recorded values reflect reality.\n",
    "   - *Why it matters:* Inaccurate values can mislead both descriptive analysis and predictive models.\n",
    "   - *Example:* A temperature of -50°C recorded in Washington D.C. in July is a clear sensor error that could confuse demand models.\n",
    "\n",
    "3. **Consistency**\n",
    "   - *Definition:* Whether data follows uniform formats, units, and scales.\n",
    "   - *Why it matters:* Inconsistent formats can corrupt calculations and comparisons.\n",
    "   - *Example:* Mixing Celsius and Fahrenheit in the same column, or having timestamps in multiple formats, leads to corrupted analysis.\n",
    "\n",
    "4. **Validity**\n",
    "   - *Definition:* Whether values fall within logical or physically possible ranges.\n",
    "   - *Why it matters:* Invalid data points indicate measurement or collection errors.\n",
    "   - *Example:* Negative bike counts or humidity above 100% are impossible values that reveal collection problems.\n",
    "\n",
    "5. **Uniqueness**\n",
    "   - *Definition:* Whether each observation is recorded only once.\n",
    "   - *Why it matters:* Duplicate records inflate usage counts and distort demand predictions.\n",
    "   - *Example:* If the same rental transaction is logged twice, it looks like demand is higher than it really was.\n",
    "\n",
    "Together, these five dimensions reveal how data quality problems can undermine predictions in different ways. Recognizing these distinct aspects helps you target specific problems rather than applying generic \"data cleaning\" approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a942444f",
   "metadata": {},
   "source": [
    "### 2.2. Missing Data Patterns and Business Implications\n",
    "\n",
    "Not all missing data is created equal. In transportation systems, gaps in the dataset often occur under very specific conditions—the very conditions you want to analyze. Understanding when and why data goes missing helps you choose the right handling strategies and communicate risks to clients.\n",
    "\n",
    "**Weather-Driven Data Loss**\n",
    "\n",
    "- The most common pattern involves sensor failures during extreme weather events. Storms, heavy rain, and temperature extremes can knock out monitoring equipment precisely when weather has the strongest influence on bike usage. This creates a double problem: you lose data exactly when you need it most, and standard statistical assumptions about \"random\" missingness don't apply.\n",
    "\n",
    "**Operational and Maintenance Gaps**\n",
    "\n",
    "- Planned maintenance creates predictable but significant gaps in transportation data. These periods often overlap with major infrastructure changes—like opening new stations or updating software systems—meaning that ignoring the operational context could hide important business insights about system growth and performance.\n",
    "\n",
    "**Network Effects and Cascade Failures**\n",
    "\n",
    "- Transportation systems are interconnected. When a high-demand station goes offline, neighboring stations typically experience unusual demand spikes. Without data from the offline station, it becomes difficult to distinguish between genuine demand growth and temporary displacement of riders to nearby locations.\n",
    "\n",
    "**Peak Period Vulnerabilities**\n",
    "\n",
    "- Finally, data failures during rush hours pose special challenges because peak-period predictions drive critical business decisions. Missing data during these high-stakes periods requires more sophisticated handling strategies than gaps occurring during quiet off-peak hours when the business impact is minimal.\n",
    "\n",
    "These four patterns demonstrate why missing data analysis goes beyond simple counts and percentages. Each pattern creates specific risks for demand forecasting and requires tailored handling strategies. By understanding when and why data disappears, you can make informed decisions about imputation, communicate limitations clearly to clients, and avoid building models on unreliable foundations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61264af2",
   "metadata": {},
   "source": [
    "## 3. Data Quality Assessment Process\n",
    "\n",
    "As a consultant, clients will expect you to follow a systematic, defensible process for data quality assessment — not ad-hoc checking. The 4-step process we'll use gives you a professional framework that you can explain and justify to any client:\n",
    "\n",
    "1. **Quick Data Quality Checks** – your first diagnostic scan to flag obvious issues.\n",
    "2. **Time Series Integrity Check** – systematic analysis of temporal continuity.\n",
    "3. **Outlier Detection** – comprehensive detection of anomalies.\n",
    "4. **Missing Data Detection** – detailed analysis of completeness.\n",
    "\n",
    "This systematic approach demonstrates expertise and builds client confidence from day one. Steps 1, 3, and 4 apply universally across domains. Step 2 becomes critical for time-series data, like the one we usually see in transportation problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad329398",
   "metadata": {},
   "source": [
    "### 3.1. Quick Data Quality Checks\n",
    "\n",
    "Quick data quality checks are rapid diagnostic scans that assess whether a dataset is fundamentally sound before investing time in detailed analysis or modeling.\n",
    "\n",
    "Think of this like a consultant's \"triage\" — in just a few minutes, you want to know whether the dataset looks broadly reliable, where the biggest risks lie, and which areas deserve closer investigation. When you receive a new dataset from a client, this scan is your first step, not modeling.\n",
    "\n",
    "These lightweight checks flag obvious issues across structure, value ranges, and cross-variable plausibility. We won't yet explain problems in depth or attempt fixes — that comes later. The goal is rapid risk assessment.\n",
    "\n",
    "Our quick quality assessment will be based on three steps:\n",
    "\n",
    "1. Structural Snapshot\n",
    "2. Value Sanity Checks\n",
    "3. Cross-Variable Plausibility\n",
    "\n",
    "**1. Structural Snapshot**\n",
    "\n",
    "The first thing we do is take a **structural snapshot** of the dataset: how many rows and columns it has, and whether the variables are of the expected type. This step sounds simple, but it’s one of the fastest ways to detect import errors, unexpected row counts, or inconsistencies in data types — all of which can indicate bigger problems lurking beneath the surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70edf37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Washington D.C. bike-sharing dataset (intentionally messy version)\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset-teaching-lec-04.csv\")\n",
    "\n",
    "# Check dataset dimensions and data types\n",
    "print(f\"Rows: {df.shape[0]}, Columns: {df.shape[1]}\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b24279",
   "metadata": {},
   "source": [
    "> **Note:** You may have noticed something unusual here: the results don't look exactly like what we saw in the previous lecture. That's intentional. From this point forward, we'll sometimes work with a slightly modified version of the Washington D.C. dataset. We've made it \"messy\" on purpose so you can practice handling real-world problems. As a consultant, you'll rarely get a dataset that's perfectly clean — each phase of the course will bring new challenges for you to detect and resolve.\n",
    "\n",
    "When we run this quick check, a few important concerns stand out immediately:\n",
    "\n",
    "* The **`holiday` column has missing values**. This means that not every day is properly labeled as a holiday or not — a detail that could easily distort demand forecasts, since holiday patterns differ sharply from regular weekdays.\n",
    "* The **weather-related variables** such as `temp`, `humidity`, and `windspeed` also contain gaps. Because these are some of the most important explanatory variables in our forecasting model, missingness here reduces our ability to explain variation in bike rentals.\n",
    "* Most critically, the **`count` column — our target variable — is missing in several rows**. This is a red flag: every missing entry in `count` means lost training data, and the reliability of our model hinges on how much usable demand history we have.\n",
    "\n",
    "This single, simple scan already tells us that the dataset cannot be used “as is” for modeling. More importantly, it shows why **structural checks are powerful**: with just one command, we’ve uncovered problems in both our explanatory variables and our target.\n",
    "\n",
    "This is exactly the kind of insight to highlight at the project start: *\"Before we can move into forecasting, we've already identified major gaps in the dataset that could affect both explanatory power and prediction accuracy.\"*\n",
    "\n",
    "**2. Value Sanity Checks**\n",
    "\n",
    "After confirming the dataset’s structure, the next step is to ask: *“Do the values themselves make sense?”*\n",
    "\n",
    "Every variable has **natural boundaries** defined by either business rules or physical limits:\n",
    "\n",
    "- Bike rentals cannot be negative.\n",
    "- Humidity must fall between 0% and 100%.\n",
    "- Local temperatures should stay within climate-appropriate ranges.\n",
    "\n",
    "Values that fall outside these ranges are not just unusual — they are almost always errors caused by faulty sensors, bad data entry, or processing mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2210ad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic range checks\n",
    "print(\"Negative rentals:\", (df[\"count\"] < 0).sum())\n",
    "print(\"Humidity out of range:\", ((df[\"humidity\"] < 0) | (df[\"humidity\"] > 100)).sum())\n",
    "print(\"Temperature extremes:\", df[\"temp\"].describe()[[\"min\", \"max\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ec2db8",
   "metadata": {},
   "source": [
    "Running these quick checks reveals three immediate red flags:\n",
    "\n",
    "- We find **6 cases of negative rentals**, which is logically impossible — you can’t rent fewer than zero bikes. This is most likely a logging or entry error.\n",
    "- We spot **10 humidity values above 100%**, which is physically impossible. This usually points to a faulty sensor reading or an ingestion problem.\n",
    "- Finally, the **temperature maximum is close to 100°C**. While summers in Washington D.C. can be hot, they certainly don’t reach boiling point! This extreme value is almost certainly an error that could distort averages or mislead a forecasting model.\n",
    "\n",
    "Together, these findings show why **range validation is essential**. With a few simple checks, we can identify values that clearly break real-world rules — and if left undetected, they could slip into analysis, biasing results and damaging credibility with clients.\n",
    "\n",
    "This step builds trust: you demonstrate that you're not just running models blindly, but verifying whether the data itself reflects reality.\n",
    "\n",
    "**3. Cross-Variable Plausibility**\n",
    "\n",
    "Numbers can look fine in isolation but make no sense once you compare them across variables. That’s why a good quick check also includes a **plausibility scan across related variables**. In transportation data, the most important relationship to test is usually between **demand** and **context variables** like weather.\n",
    "\n",
    "For example, common sense (and business experience) tells us that **bike rentals should fall when weather conditions worsen**. If the dataset shows the opposite, that’s a red flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b94945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick plausibility check: average rentals by weather condition\n",
    "avg_rentals_by_weather = df.groupby(\"weather\")[\"count\"].mean()\n",
    "print(avg_rentals_by_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8585b800",
   "metadata": {},
   "source": [
    "> **Note:** The weather variable is coded as follows:\n",
    "> - **1**: Clear, Few clouds, Partly cloudy\n",
    "> - **2**: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist  \n",
    "> - **3**: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "> - **4**: Heavy Rain + Ice Pellets + Thunderstorm + Mist, Snow + Fog\n",
    "\n",
    "At first glance, the results follow expectations:\n",
    "\n",
    "- Rentals are **highest on clear days (weather = 1)**.\n",
    "- They gradually decrease as conditions worsen to misty or light rain.\n",
    "\n",
    "But the final category is suspicious. Under **heavy rain or storms (weather = 4)**, the dataset shows an **average of more than 600 rentals per hour** — even higher than on sunny days.\n",
    "\n",
    "This makes little business sense: real-world demand should drop sharply in severe weather, not skyrocket. Such an inconsistency usually points to:\n",
    "\n",
    "- **Mislabelled weather codes**, or\n",
    "- A **misalignment between weather feeds and rental logs**.\n",
    "\n",
    "Left uncorrected, such errors could lead to false conclusions like *\"bike demand is resilient during storms\"* — which in turn could drive poor operational decisions, such as overstocking bikes or overscheduling staff during extreme weather events.\n",
    "\n",
    "This kind of cross-variable check is a reminder: **some errors only appear when you look at relationships, not just single columns**. That’s why you should test whether the data’s “story” matches real-world logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79bf6af",
   "metadata": {},
   "source": [
    "### 3.2. Temporal Continuity Check\n",
    "\n",
    "Temporal continuity refers to whether your time-series data has a complete, consistent timeline without gaps, duplicates, or misaligned sequences.\n",
    "\n",
    "Transportation data is inherently time-based. If the timeline itself is broken, then any further cleaning, imputation, or modeling will rest on shaky foundations. Missing hours create false patterns, duplicates skew averages, and misaligned sequences break seasonal analysis.\n",
    "\n",
    "Before we tackle outliers or missing values, we must verify the timeline's integrity to ensure reliable foundations for all subsequent work. To do so, we run a timeline diagnostic that tells us:\n",
    "\n",
    "- The first and last timestamp in the dataset.\n",
    "- How many hours should exist in that range.\n",
    "- How many unique hours actually exist.\n",
    "- How many are missing.\n",
    "- How many duplicate rows we have for the same hour.\n",
    "\n",
    "This gives us a quick sense of whether the dataset is complete and well-aligned, or whether we’re missing entire blocks of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b641339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ensure datetime is properly parsed\n",
    "data_path = \"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset-teaching-lec-04.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "df[\"datetime\"] = pd.to_datetime(df[\"datetime\"], errors=\"coerce\")\n",
    "df = df.sort_values(\"datetime\").reset_index(drop=True)\n",
    "\n",
    "# Identify coverage\n",
    "t_min = df[\"datetime\"].min()\n",
    "t_max = df[\"datetime\"].max()\n",
    "\n",
    "# Duplicated timestamps\n",
    "n_dup_rows = df.duplicated(subset=[\"datetime\"]).sum()\n",
    "\n",
    "# Build expected hourly range\n",
    "expected = pd.date_range(t_min, t_max, freq=\"h\")\n",
    "actual = pd.Index(df[\"datetime\"].unique())\n",
    "\n",
    "n_expected = len(expected)\n",
    "n_actual = len(actual)\n",
    "n_missing_hours = len(expected.difference(actual))\n",
    "\n",
    "print(\"=== Timeline quick check ===\")\n",
    "print(\"time_min:\", t_min)\n",
    "print(\"time_max:\", t_max)\n",
    "print(\"expected_hours:\", n_expected)\n",
    "print(\"present_unique_hours:\", n_actual)\n",
    "print(\"missing_hours:\", n_missing_hours)\n",
    "print(\"duplicate_rows:\", n_dup_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a3489a",
   "metadata": {},
   "source": [
    "As we can see, between January 2011 and December 2012, the dataset should contain **17 256 hourly rows**. In reality, it only contains **10 862 unique hours**, leaving **6 394 hours missing**. That's more than a third of the timeline absent — a major structural gap. We also see **24 duplicate rows**, meaning some hours are represented more than once.\n",
    "\n",
    "This is a critical insight: the raw data cannot be trusted as a continuous timeline. Large gaps undermine seasonal analysis, and duplicates risk double-counting demand. Before any modeling, we need to fix both problems. We will show how to do that in [Chapter 4](#4-data-cleaning-strategies-and-implementation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2de0dd",
   "metadata": {},
   "source": [
    "### 3.3. Outlier Detection\n",
    "\n",
    "Outlier detection identifies data points that deviate significantly from the expected pattern — values that are unusually high, unusually low, or inconsistent with the rest of the dataset.\n",
    "\n",
    "In transportation data, outliers can represent legitimate extreme events (like snowstorms causing demand drops), data collection errors (like negative bike rentals), or operational anomalies (like maintenance affecting normal patterns). Each type requires different treatment strategies.\n",
    "\n",
    "The goal is not just to detect outliers, but to classify them correctly so that legitimate events are preserved while errors are corrected. This ensures models learn from real patterns rather than data quality issues.\n",
    "\n",
    "Now that we understand what outliers are, let’s look at the different ways to detect them. Outlier detection methods generally fall into three categories:\n",
    "\n",
    "1. Statistical Methods\n",
    "2. Business Logic Methods\n",
    "3. Temporal Methods\n",
    "\n",
    "**1. Statistical Methods**\n",
    "\n",
    "Statistical methods use mathematical formulas to identify unusual values. They don’t require prior knowledge of the transportation system - they just look at how far a data point is from what is “normal” in the dataset.\n",
    "\n",
    "There are several statistical approaches, such as:\n",
    "\n",
    "- Z-Score Analysis\n",
    "- Interquartile Range (IQR) Method\n",
    "- Modified Z-Score\n",
    "\n",
    "In this lecture, we will focus on just one example: **Z-Score Analysis**.\n",
    "\n",
    "A **Z-score** tells us how many “standard steps” (standard deviations) a data point is away from the average.\n",
    "\n",
    "$$\n",
    "Z = \\frac{x - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $x$ = the value we’re checking\n",
    "- $\\mu$ = the mean (average) of the data\n",
    "- $\\sigma$ = the standard deviation (how spread out the data is)\n",
    "\n",
    "Think of the average bike rentals per day like the “center of gravity” of the data. Most days will be close to that average. The Z-score is like a distance meter: it tells us how far a particular day is from the typical pattern.\n",
    "\n",
    "- A Z-score of **0** → exactly average.\n",
    "- A Z-score of **+2** → two steps above average (busier than normal).\n",
    "- A Z-score of **–3** → three steps below average (quieter than normal).\n",
    "\n",
    "When a Z-score is bigger than 3 or smaller than –3, the value is far enough from the average that we should pause and ask: *Is this a real event, or is it an error?*\n",
    "\n",
    "We use Z-scores because they:\n",
    "\n",
    "- **Standardize values** so we can compare across variables.\n",
    "- **Give a simple rule of thumb**: beyond 3 = unusual.\n",
    "- **Provide a quick first filter** before applying more advanced techniques.\n",
    "\n",
    "Let’s see how this works in practice using the Washington D.C. bike-sharing dataset. We’ll calculate the Z-scores for daily demand (`count`) and flag potential outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3188fe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the Washington D.C. bike-sharing dataset\n",
    "data_path = \"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset-teaching-lec-04.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Convert datetime column to pandas datetime type\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "# Aggregate rentals by day\n",
    "daily_rentals = df.groupby(df['datetime'].dt.date)['count'].sum().reset_index()\n",
    "\n",
    "# Calculate mean and std\n",
    "mean = daily_rentals['count'].mean()\n",
    "std = daily_rentals['count'].std()\n",
    "\n",
    "# Compute Z-scores\n",
    "daily_rentals['z_score'] = (daily_rentals['count'] - mean) / std\n",
    "\n",
    "# Flag outliers (|Z| > 3)\n",
    "outliers = daily_rentals[daily_rentals['z_score'].abs() > 3]\n",
    "\n",
    "outliers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bed5871",
   "metadata": {},
   "source": [
    "Both May 15 and November 3 show exceptionally high rental counts, far beyond typical daily demand. These are unlikely to be ordinary fluctuations. They could represent special city-wide events or anomalies in how trips were logged. As consultants, we need to cross-check these dates with event calendars and system logs to confirm whether these spikes reflect genuine demand or possible data quality issues.\n",
    "\n",
    "**2. Business Logic Methods**\n",
    "\n",
    "While statistical methods rely purely on mathematical rules, **business logic methods** use knowledge of the system and its physical constraints to detect outliers. Instead of asking, “Does this number look statistically unusual?”, we ask, “Is this number even possible given how the transportation system works?”\n",
    "\n",
    "Business logic methods build rules like these based on:\n",
    "\n",
    "- **Physical constraints**: e.g., a bike station cannot have negative bikes, nor can it rent more bikes than its maximum capacity.\n",
    "- **Historical ranges**: e.g., demand has never exceeded 1,200 rentals in a day; a value above this threshold is suspicious.\n",
    "- **Cross-variable checks**: e.g., it shouldn’t be possible to record “heavy rain” alongside “record-high bike usage.”\n",
    "\n",
    "Let's apply business logic to detect outliers by checking for physical constraint violations in wind speed measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e843d9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Washington D.C. bike-sharing dataset\n",
    "data_path = \"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset-teaching-lec-04.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Convert datetime column to pandas datetime type\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "# Check for physical constraint violations in windspeed\n",
    "invalid_windspeed = df[(df['windspeed'] < 0) | (df['windspeed'] > 60)]\n",
    "\n",
    "invalid_windspeed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3e2154",
   "metadata": {},
   "source": [
    "The flagged records show windspeed values of `200.0`, which are far beyond any physically possible measurement for this system. These values clearly indicate sensor or recording errors rather than real-world weather conditions. If left uncorrected, they could distort downstream models, for example by falsely associating extreme winds with normal rental demand.\n",
    "\n",
    "**3. Temporal Methods**\n",
    "\n",
    "Transportation data is inherently tied to **time**. Unlike static datasets, values change depending on the hour, day, season, or long-term trends. **Temporal outlier detection methods** focus on identifying unusual data points that break these expected time-based patterns. For example:\n",
    "\n",
    "- A sudden drop in rentals during a weekday morning rush hour might indicate a system outage.\n",
    "- A sharp jump in rentals during winter could mean a special event.\n",
    "- A long-term shift in demand may signal that the system has grown or changed in some way.\n",
    "\n",
    "Some common temporal approaches include:\n",
    "\n",
    "- **Change Point Detection**: Identifying sudden structural shifts in the data (e.g., a new station or policy).\n",
    "- **Seasonal Anomaly Detection**: Checking if values align with expected seasonal patterns.\n",
    "- **Trend Deviation Analysis**: Comparing current values to long-term growth or decline.\n",
    "\n",
    "Let's see an example of a seasonal anomaly detection. We'll identify days where demand deviates significantly from seasonal expectations by comparing winter and summer rental patterns, then flagging winter days with unusually high demand and summer days with unusually low demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218f03f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Washington D.C. bike-sharing dataset\n",
    "data_path = \"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset-teaching-lec-04.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Convert datetime and build daily_rentals with a month column\n",
    "df[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\n",
    "daily_rentals = (\n",
    "    df.groupby(df[\"datetime\"].dt.date)[\"count\"]\n",
    "      .sum()\n",
    "      .reset_index()\n",
    "      .rename(columns={\"datetime\": \"date\"})\n",
    ")\n",
    "daily_rentals[\"date\"] = pd.to_datetime(daily_rentals[\"date\"])\n",
    "daily_rentals[\"month\"] = daily_rentals[\"date\"].dt.month\n",
    "\n",
    "# Define \"winter\" as Dec-Feb and \"summer\" as Jun-Aug\n",
    "winter_months = [12, 1, 2]\n",
    "summer_months = [6, 7, 8]\n",
    "\n",
    "winter_avg = daily_rentals[daily_rentals['month'].isin(winter_months)]['count'].mean()\n",
    "summer_avg = daily_rentals[daily_rentals['month'].isin(summer_months)]['count'].mean()\n",
    "\n",
    "# Flag unusual winter days (too close to summer levels)\n",
    "winter_anomalies = daily_rentals[\n",
    "    (daily_rentals['month'].isin(winter_months)) & \n",
    "    (daily_rentals['count'] > summer_avg * 0.8)\n",
    "]\n",
    "\n",
    "# Flag unusual summer days (too close to winter levels)\n",
    "summer_anomalies = daily_rentals[\n",
    "    (daily_rentals['month'].isin(summer_months)) & \n",
    "    (daily_rentals['count'] < winter_avg * 1.2)\n",
    "]\n",
    "\n",
    "seasonal_anomalies = pd.concat([winter_anomalies, summer_anomalies])\n",
    "seasonal_anomalies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5e4c1c",
   "metadata": {},
   "source": [
    "The detected anomalies highlight days in winter months (January and February) with demand levels much closer to what we’d expect in summer. For example, January 7 shows more than double the average rentals for that month. These could indicate unusually warm days that encouraged cycling, or they might reflect special events. In practice, such findings should be validated with weather data or event calendars. This illustrates how seasonal anomaly detection helps identify values that break expected seasonal patterns, providing valuable clues about real-world influences on demand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1b48a6",
   "metadata": {},
   "source": [
    "### 3.4. Missing Data Detection\n",
    "\n",
    "Missing data detection is the systematic analysis of gaps in your dataset to understand their patterns, causes, and potential impact on modeling and analysis.\n",
    "\n",
    "Unlike outliers which are individual problematic values, missing data represents systematic gaps that can undermine entire analyses. Missingness often follows patterns — clustering in certain periods, affecting groups of variables together, or reflecting underlying system failures like sensor outages.\n",
    "\n",
    "We analyze missing data patterns to design targeted cleaning strategies rather than applying generic fixes. Understanding where, when, and why data goes missing ensures our solutions address root causes and preserve data integrity.\n",
    "\n",
    "We'll explore missing data through three systematic analyses:\n",
    "\n",
    "- Quantitative assessment\n",
    "- Temporal pattern analysis\n",
    "- Cross-variable analysis\n",
    "\n",
    "**1. Quantitative Assessment of Missing Data**\n",
    "\n",
    "A **quantitative assessment** inventories missing values across all columns, showing both you and the client the scale and location of gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709116cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values and calculate percentages\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_percentages = (missing_counts / len(df)) * 100\n",
    "\n",
    "# Display side by side for clarity\n",
    "pd.DataFrame({\"Missing\": missing_counts, \"Percentage\": missing_percentages})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f140561",
   "metadata": {},
   "source": [
    "Our scan shows three main areas of concern:\n",
    "\n",
    "- First, the `holiday` column is missing in about **5% of rows**, which means we can't always tell whether a given day was a holiday — a potentially important driver of demand.\n",
    "- Second, weather-related variables (`temp`, `atemp`, `humidity`, `windspeed`) are missing in around **0.6% of cases each**. That may sound small, but if they go missing together, it likely reflects a sensor or reporting problem.\n",
    "- Finally, and most importantly, the **target variables** (`count`, `casual`, `registered`) are missing in **146 rows**, or about **1.3% of the dataset**.\n",
    "\n",
    "Any missing demand values directly reduce the amount of training data available for forecasting, which is bad.\n",
    "\n",
    "**2. Temporal Pattern Analysis**\n",
    "\n",
    "In transportation datasets, missing data often clusters in specific periods. This makes **temporal analysis** essential: by grouping missingness across months or seasons, we can check whether the gaps are random or systematically tied to certain time periods.\n",
    "\n",
    "This matters for forecasting because if missingness is concentrated in peak demand months, any model we build will be biased or incomplete in those periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c1ae2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year and month into a new column\n",
    "df['year_month'] = df['datetime'].dt.strftime('%Y-%m')\n",
    "\n",
    "# Group by the new 'year_month' column and calculate missing values\n",
    "missing_by_month = df.groupby(df['year_month'])['count'].apply(lambda x: x.isnull().sum())\n",
    "print(missing_by_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3df15a",
   "metadata": {},
   "source": [
    "The temporal scan reveals that missing demand values are not evenly spread across the dataset. Instead, they cluster heavily in **July 2011 and July 2012**. This points to a systematic issue, such as a recurring sensor outage or reporting gap during summer months. For the client, this has a clear implication: forecasts for peak-season demand may be less reliable unless these gaps are addressed. It’s not just random noise — it’s a structural weakness in the dataset that could distort decision-making during the busiest time of year.\n",
    "\n",
    "**3. Cross-Variable Analysis of Missing Data**\n",
    "\n",
    "Sometimes, missingness in one variable aligns with gaps in others. This is an important diagnostic step because it helps distinguish between isolated issues (e.g., a single column not recorded) and **system-wide failures** (e.g., a weather station outage affecting several variables at once).\n",
    "\n",
    "By checking which variables tend to go missing together, we can form a more realistic hypothesis about the underlying cause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afcd331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check rows where at least one weather-related variable is missing\n",
    "weather_missing = df[df[[\"temp\", \"humidity\", \"windspeed\"]].isnull().any(axis=1)]\n",
    "print(weather_missing.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757e795c",
   "metadata": {},
   "source": [
    "The cross-variable check confirms that missingness is not isolated — entire blocks of weather data (`temp`, `atemp`, `humidity`, `windspeed`) disappear at the same time. For example, on **February 11, 2011**, several consecutive hours show all weather variables missing together. This strongly suggests a **weather station outage or reporting failure**, not random gaps. Recognizing that these variables fail together allows us to design a coordinated cleaning strategy rather than treating each column as an independent problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15e7d87",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning Strategies and Implementation\n",
    "\n",
    "We've diagnosed the problems in our bike-sharing dataset: impossible values, extreme outliers, missing data blocks, and timeline gaps. **Now comes the treatment stage** — this is where clients see the real consulting value.\n",
    "\n",
    "Diagnosis impresses clients with your analytical rigor, but **cleaning delivers the reliable data foundation** they need for business decisions. While assessment shows what's wrong, cleaning demonstrates how you solve problems systematically and transparently.\n",
    "\n",
    "This transition from \"finding issues\" to \"fixing issues\" represents the shift from diagnostic consultant to solution provider. Clients pay for datasets they can trust, models they can deploy, and insights they can act on.\n",
    "\n",
    "**Data cleaning** is not cosmetic work to make datasets \"look nice.\" It's a structured process that:\n",
    "\n",
    "- Distinguishes **errors** from **real events**\n",
    "- Applies **consistent, rule-based fixes** where possible\n",
    "- Decides when to **impute or drop** values that cannot be fixed\n",
    "- Keeps every change **transparent and auditable**\n",
    "\n",
    "This systematic approach ensures that your cleaning decisions can be explained, defended, and replicated — critical requirements for professional consulting work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682bbf78",
   "metadata": {},
   "source": [
    "### 4.1. The Unified Cleaning Workflow\n",
    "\n",
    "Since we are working with time-series data, we will start by **standardizing the timeline** and fixing its structural problems — this is a critical first step that ensures we have a reliable temporal foundation.\n",
    "\n",
    "From there, we will follow a standard data cleaning workflow that, while always dependent on the specific dataset and business context, can be generalized into a systematic process that learners can apply to their own projects. Once the timeline is reliable, every suspicious value — whether it’s extreme, impossible, or missing — is treated with the same **three-step decision tree**:\n",
    "\n",
    "1. **Is this an event or an error?**\n",
    "\n",
    "   - *Event* → keep, but **flag** (e.g., snowstorm, festival).\n",
    "   - *Error* → continue.\n",
    "\n",
    "2. **If error: Can I fix it with a rule?**\n",
    "\n",
    "   - Examples: cap humidity to 100, relabel mis-coded weather categories, set negative rentals to `NaN`.\n",
    "   - If yes → **fix and flag**.\n",
    "\n",
    "3. **If cannot fix: Should I impute or drop?**\n",
    "\n",
    "   - **Predictors (features):** impute if valuable, drop if not.\n",
    "   - **Target (`count`):** never impute for modeling → drop missing rows.\n",
    "   - Always **flag** imputations or dropped ranges.\n",
    "\n",
    "> **Note 1:** **Predictors** are the input variables we use to make predictions (like weather conditions, time of day, or season), while the **target** is the outcome variable we're trying to predict (in our case, bike rental counts).\n",
    "\n",
    "> **Note 2:** Why flag? We flag because it ensures that every change is **visible, auditable, and explainable**. Flags allow you to:\n",
    "> \n",
    "> - Compare model performance with and without imputed values.\n",
    "> - Communicate risks to clients (*“July demand is less reliable: 20% of weather values were imputed.”*).\n",
    "> - Keep a record of what changed and why.\n",
    "\n",
    "This mindset — *event or error? fix, impute, or drop? always flag* — is the backbone of professional data cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0427372a",
   "metadata": {},
   "source": [
    "### 4.2. Standardizing the Timeline\n",
    "\n",
    "In the previous chapters, we discovered that our timeline is not fully reliable: some hours are missing, while others are duplicated. To move forward confidently, we need to standardize the timeline so that every hour has exactly one valid record.\n",
    "\n",
    "We will do so by:\n",
    "\n",
    "1. Collapsing duplicate rows\n",
    "2. Reindexing to a continuous hourly timeline\n",
    "\n",
    "**1. Collapsing Duplicate Rows**\n",
    "\n",
    "The bike-sharing system should record **exactly one entry per hour**. If we find multiple rows for the same timestamp, something has gone wrong. These duplicates can arise for several reasons:\n",
    "\n",
    "- **Data collection partitioning**: Different sensors or stations reporting separately for the same hour\n",
    "- **System processing delays**: Multiple data collection cycles within the same hour  \n",
    "- **Data pipeline issues**: ETL processes creating duplicate records\n",
    "\n",
    "In all of these situations, the extra rows are not separate time periods but **fragments of the same underlying hour**. This is why we adopt the working assumption that duplicates represent **partial data for a single hour**.\n",
    "\n",
    "Instead of discarding these rows, we will **combine them into one consolidated record**. This approach is practical for our project, but remember that in real consulting work you should always **validate the duplication patterns before applying aggregation**, to confirm that duplicates really are fragments and not a sign of a different data issue.\n",
    "\n",
    "Since we assume that duplicates are **partial fragments of the same hour**, we can combine them into a single record using the following aggregation policy:\n",
    "\n",
    "- For **targets** (`count`, `casual`, `registered`): use the **SUM**, because bike rentals accumulate across fragments. Two partial records for the same hour simply add up to the true total demand.\n",
    "- For **numeric predictors** (e.g., weather variables): take the **MEAN**, since these are measured as continuous conditions. Averaging across fragments best represents the overall hourly state.\n",
    "- For **categorical variables** (`holiday`, `weather`, `season`): take the **FIRST** value, because these codes should remain stable within an hour. A duplicated categorical code does not add new information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7300b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Identify duplicated timestamps\n",
    "rows_per_ts = df.groupby(\"datetime\").size().rename(\"n_rows_per_ts\")\n",
    "duplicated_ts = rows_per_ts[rows_per_ts > 1].index\n",
    "n_dup_timestamps = len(duplicated_ts)\n",
    "n_dup_rows = int((rows_per_ts[rows_per_ts > 1] - 1).sum())\n",
    "\n",
    "print(f\"Duplicated timestamps: {n_dup_timestamps} | Extra duplicate rows to collapse: {n_dup_rows}\")\n",
    "\n",
    "# Build aggregation policy\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "agg_map = {c: \"mean\" for c in numeric_cols}\n",
    "\n",
    "for c in [\"count\", \"casual\", \"registered\"]:\n",
    "    if c in agg_map:\n",
    "        agg_map[c] = \"sum\"\n",
    "\n",
    "for c in [\"holiday\", \"weather\", \"season\"]:\n",
    "    if c in df.columns:\n",
    "        agg_map[c] = \"first\"\n",
    "\n",
    "# Aggregate to one row per hour\n",
    "df = (\n",
    "    df.groupby(\"datetime\", as_index=False)\n",
    "      .agg(agg_map)\n",
    "      .sort_values(\"datetime\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Add a flag for hours that were collapsed from duplicates\n",
    "df[\"flag_collapsed_from_duplicates\"] = df[\"datetime\"].isin(duplicated_ts)\n",
    "\n",
    "print(\"Collapse complete → policy: SUM targets, MEAN numeric predictors, FIRST categoricals.\")\n",
    "print(\"Hours affected:\", int(df[\"flag_collapsed_from_duplicates\"].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e461f7a8",
   "metadata": {},
   "source": [
    "We collapsed **24 duplicated hours**, removing 24 extra rows from the dataset. The aggregation policy ensures that:\n",
    "\n",
    "- Demand counts remain correct (no double-counting).\n",
    "- Weather predictors reflect average conditions.\n",
    "- Categorical codes remain stable.\n",
    "\n",
    "The flag `flag_collapsed_from_duplicates` marks these hours so we can always trace which rows were affected. For transparency, this is important: if a client later asks why certain hours look unusual, we can point to the duplication issue.\n",
    "\n",
    "**2. Reindexing to a Continuous Hourly Timeline**\n",
    "\n",
    "Finally, we enforce a **continuous hourly index**. Right now, the dataset simply skips missing hours — they aren’t represented at all. This makes gaps invisible and impossible to handle systematically.\n",
    "\n",
    "By reindexing:\n",
    "\n",
    "- We insert a row for **every missing hour**.\n",
    "- Those rows will contain `NaN`s for predictors and/or target.\n",
    "- We add a flag to mark which rows were **inserted**.\n",
    "\n",
    "Next, we'll enforce a continuous hourly timeline by inserting rows for missing hours. This makes all gaps explicit and manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48940050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the full hourly index\n",
    "time_min = df[\"datetime\"].min()\n",
    "time_max = df[\"datetime\"].max()\n",
    "full_hours = pd.date_range(time_min, time_max, freq=\"h\")\n",
    "\n",
    "# Keep original set of hours\n",
    "original_hours = pd.Index(df[\"datetime\"])\n",
    "\n",
    "# Reindex and flag\n",
    "df = df.set_index(\"datetime\").reindex(full_hours)\n",
    "df.index.name = \"datetime\"\n",
    "df[\"flag_missing_timestamp\"] = ~df.index.isin(original_hours)\n",
    "\n",
    "# Audit\n",
    "inserted_hours = int(df[\"flag_missing_timestamp\"].sum())\n",
    "total_hours = len(df)\n",
    "present_hours = total_hours - inserted_hours\n",
    "\n",
    "print(\"=== Reindex audit ===\")\n",
    "print(\"time_min:\", time_min)\n",
    "print(\"time_max:\", time_max)\n",
    "print(\"total_hours_after_reindex:\", total_hours)\n",
    "print(\"present_hours_from_source:\", present_hours)\n",
    "print(\"inserted_missing_hours:\", inserted_hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db445579",
   "metadata": {},
   "source": [
    "After reindexing, the dataset now has **17,256 rows** — one for each expected hour. Of these, **6,394 rows were inserted** to represent missing hours. These rows currently contain `NaN`s, which is exactly what we want: the gaps are now explicit and can be handled in the cleaning workflow.\n",
    "\n",
    "From here:\n",
    "\n",
    "- For **predictors**, missing values can be imputed using interpolation or seasonal medians.\n",
    "- For the **target (`count`)**, rows with missing demand must be dropped before model training.\n",
    "- The flag `flag_missing_timestamp` allows us to communicate clearly to clients how much of the dataset is reconstructed rather than observed.\n",
    "\n",
    "With these two steps, we’ve established a **reliable timeline**. The dataset now has exactly one row per hour, duplicates resolved, and missing periods made explicit. This creates the solid foundation we need before applying the unified cleaning workflow to outliers and missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a3e445",
   "metadata": {},
   "source": [
    "### 4.3. Applying the Workflow to Outliers\n",
    "\n",
    "Earlier we learned to detect outliers; now we'll apply the unified workflow to treat them systematically. The key shift is from detection to decision-making.\n",
    "\n",
    "Let's work through the three-step workflow systematically with our detected outliers:\n",
    "\n",
    "**Step 1: Is this an event or an error?**\n",
    "\n",
    "First, we examine each outlier to determine its nature. This step requires both data analysis and business context.\n",
    "\n",
    "*Physically Impossible Values*\n",
    "\n",
    "Some outliers are easy to classify because they are **physically impossible**:\n",
    "\n",
    "- Negative rentals (`count < 0`) — cannot exist in any transportation system\n",
    "- Humidity above 100% — violates physical laws\n",
    "- Temperatures above 100°C in Washington, D.C. — impossible for the local climate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a37ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Identify impossible values across different variables\n",
    "print(\"=== Step 1: Identifying impossible values (clear errors) ===\")\n",
    "\n",
    "# Check negative rentals\n",
    "negative_rentals = (df[\"count\"] < 0).sum()\n",
    "print(f\"Negative rental counts: {negative_rentals}\")\n",
    "\n",
    "# Check invalid humidity\n",
    "invalid_humidity = ((df[\"humidity\"] < 0) | (df[\"humidity\"] > 100)).sum()\n",
    "print(f\"Humidity outside 0-100%: {invalid_humidity}\")\n",
    "\n",
    "# Check extreme temperatures (conservative cutoff for Washington climate)\n",
    "extreme_temps = (df[\"temp\"] > 60).sum()\n",
    "print(f\"Temperatures above 60°C: {extreme_temps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd1f410",
   "metadata": {},
   "source": [
    "**Decision:** These are **always errors** — they cannot represent real-world events. Move to Step 2.\n",
    "\n",
    "*Extreme-but-Possible Values*\n",
    "\n",
    "Other outliers are statistically extreme but could potentially be real events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ca4a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for extreme daily rental patterns using Z-scores\n",
    "daily_rentals = (\n",
    "    df[\"count\"]\n",
    "    .groupby(df.index.date)\n",
    "    .sum()\n",
    "    .rename(\"daily_count\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "mean = daily_rentals[\"daily_count\"].mean()\n",
    "std = daily_rentals[\"daily_count\"].std()\n",
    "daily_rentals[\"z_score\"] = (daily_rentals[\"daily_count\"] - mean) / std\n",
    "\n",
    "# Flag potential outliers (|Z| > 3)\n",
    "extreme_days = daily_rentals[daily_rentals[\"z_score\"].abs() > 3]\n",
    "\n",
    "print(f\"\\n=== Extreme daily rental patterns ===\")\n",
    "print(f\"Days with |Z-score| > 3: {len(extreme_days)}\")\n",
    "extreme_days.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ae3d08",
   "metadata": {},
   "source": [
    "**Decision:** These require investigation. Could be:\n",
    "\n",
    "- **Events**: Festival days, unusual weather patterns, special promotions\n",
    "- **Errors**: Duplicated logs, sensor malfunctions, data processing issues\n",
    "\n",
    "For this example, we'll assume days with extreme rentals could be legitimate events (holidays, festivals) and should be kept but flagged for transparency.\n",
    "\n",
    "**Step 2: If error, can I fix it with a rule?**\n",
    "\n",
    "For the values we've classified as errors, we apply rule-based fixes where possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93039ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Step 2: Applying rule-based fixes to errors ===\")\n",
    "\n",
    "# Fix impossible values using business rules\n",
    "\n",
    "# Negative rentals: Set to NaN (cannot be fixed with a rule)\n",
    "df[\"flag_negative_count\"] = df[\"count\"] < 0\n",
    "df.loc[df[\"flag_negative_count\"], \"count\"] = np.nan\n",
    "print(f\"Negative rentals set to NaN: {df['flag_negative_count'].sum()}\")\n",
    "\n",
    "# Humidity outside valid range: Set to NaN (cannot determine correct value)\n",
    "df[\"flag_humidity_invalid\"] = (df[\"humidity\"] < 0) | (df[\"humidity\"] > 100)\n",
    "df.loc[df[\"flag_humidity_invalid\"], \"humidity\"] = np.nan\n",
    "print(f\"Invalid humidity values set to NaN: {df['flag_humidity_invalid'].sum()}\")\n",
    "\n",
    "# Extreme temperatures: Set to NaN (cannot determine correct value)\n",
    "df[\"flag_temp_invalid\"] = df[\"temp\"] > 60\n",
    "df.loc[df[\"flag_temp_invalid\"], \"temp\"] = np.nan\n",
    "print(f\"Extreme temperatures set to NaN: {df['flag_temp_invalid'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2e1308",
   "metadata": {},
   "source": [
    "**Decision:** These impossible values cannot be fixed with business rules (we don't know what the correct values should be), so they move to Step 3.\n",
    "\n",
    "**Step 3: If cannot fix, should I impute or drop?**\n",
    "\n",
    "For values that cannot be rule-based fixed, we decide between imputation and dropping based on variable type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e7fbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Step 3: Impute or drop unfixable errors ===\")\n",
    "\n",
    "# For PREDICTORS (weather variables): These can be imputed\n",
    "print(\"Weather predictors with NaN values after error correction:\")\n",
    "weather_vars = [\"temp\", \"humidity\", \"windspeed\"]\n",
    "for var in weather_vars:\n",
    "    if var in df.columns:\n",
    "        nan_count = df[var].isna().sum()\n",
    "        print(f\"  {var}: {nan_count} NaN values → will be imputed later\")\n",
    "\n",
    "# For TARGET (count): Never impute for modeling - these rows will be dropped\n",
    "target_nan = df[\"count\"].isna().sum()\n",
    "print(f\"\\nTarget variable 'count': {target_nan} NaN values → rows will be dropped before modeling\")\n",
    "\n",
    "# Flag extreme events that we're keeping\n",
    "df[\"flag_extreme_but_kept\"] = False\n",
    "if len(extreme_days) > 0:\n",
    "    extreme_dates = extreme_days[\"datetime\"].dt.date\n",
    "    df[\"flag_extreme_but_kept\"] = df.index.date.isin(extreme_dates)\n",
    "    print(f\"Extreme-but-possible days flagged for transparency: {df['flag_extreme_but_kept'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9755e4",
   "metadata": {},
   "source": [
    "**Summary of workflow application:**\n",
    "\n",
    "1. **Step 1 (Event or Error):**\n",
    "   - Impossible values → Classified as errors\n",
    "   - Extreme-but-possible values → Classified as potential events, kept with flags\n",
    "\n",
    "2. **Step 2 (Fix with rule):**\n",
    "   - Impossible values → No business rule available, set to NaN and flagged\n",
    "\n",
    "3. **Step 3 (Impute or drop):**\n",
    "   - Predictor variables with NaN → Will be imputed in preprocessing\n",
    "   - Target variable with NaN → Rows will be dropped before modeling\n",
    "   - Extreme events → Kept with transparency flags\n",
    "\n",
    "Following the unified workflow ensures that every outlier treatment decision is systematic, auditable, and defensible. Impossible values were identified as clear errors and set to missing, extreme-but-possible values were preserved with transparency flags, and the treatment strategy differs appropriately between predictors (impute) and targets (drop). Every intervention is flagged, ensuring complete traceability for client communications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb77accb",
   "metadata": {},
   "source": [
    "### 4.4. Applying the Workflow to Missing Data\n",
    "\n",
    "Earlier we systematically detected missing data patterns; now we'll apply the unified workflow to treat them strategically. Missing data presents different challenges than outliers because absence itself can be meaningful information.\n",
    "\n",
    "The key distinction is understanding **why** data is missing: system failures during storms tell a different story than random sensor glitches, and each requires different treatment strategies.\n",
    "\n",
    "Let's work through the three-step workflow systematically with our detected missing data patterns:\n",
    "\n",
    "**Step 1: Is this an event or an error?**\n",
    "\n",
    "Missing data can result from legitimate operational events or system errors. Understanding the cause determines our treatment approach.\n",
    "\n",
    "*Systematic Operational Gaps*\n",
    "\n",
    "First, let's examine the temporal clustering we identified earlier — missing data concentrated in July periods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aacb370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze missing data patterns by year-month\n",
    "print(\"=== Step 1: Analyzing missingness patterns ===\")\n",
    "\n",
    "# Focus on the target variable and key predictors\n",
    "key_vars = [\"count\", \"temp\", \"humidity\", \"windspeed\", \"holiday\"]\n",
    "missing_analysis = {}\n",
    "\n",
    "for var in key_vars:\n",
    "    if var in df.columns:\n",
    "        # Monthly missingness pattern\n",
    "        monthly_missing = (\n",
    "            df.groupby(df.index.to_period('M'))[var]\n",
    "            .apply(lambda x: x.isna().sum())\n",
    "            .rename(f\"{var}_missing\")\n",
    "        )\n",
    "        missing_analysis[var] = monthly_missing\n",
    "\n",
    "# Display July patterns specifically\n",
    "july_months = [period for period in monthly_missing.index if period.month == 7]\n",
    "print(f\"\\nMissing data in July periods:\")\n",
    "for month in july_months:\n",
    "    print(f\"{month}:\")\n",
    "    for var in key_vars:\n",
    "        if var in missing_analysis:\n",
    "            count = missing_analysis[var].get(month, 0)\n",
    "            print(f\"  {var}: {count} missing hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939accd1",
   "metadata": {},
   "source": [
    "**Decision:** The July clustering suggests **systematic operational events** — likely planned maintenance windows or sensor replacement cycles. This is normal system operation, not an error.\n",
    "\n",
    "*Weather Station Outages*\n",
    "\n",
    "Next, examine simultaneous missingness across weather variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c481e6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for simultaneous weather variable failures\n",
    "weather_vars = [\"temp\", \"atemp\", \"humidity\", \"windspeed\"]\n",
    "available_weather_vars = [var for var in weather_vars if var in df.columns]\n",
    "\n",
    "# Count rows where multiple weather variables are missing together\n",
    "weather_missing_counts = df[available_weather_vars].isna().sum(axis=1)\n",
    "simultaneous_failures = (weather_missing_counts >= 2).sum()\n",
    "\n",
    "print(f\"\\nSimultaneous weather failures:\")\n",
    "print(f\"Hours with 2+ weather variables missing: {simultaneous_failures}\")\n",
    "\n",
    "# Sample some simultaneous failure periods\n",
    "simultaneous_missing_rows = df[weather_missing_counts >= 2].head()\n",
    "print(f\"\\nExample simultaneous failures:\")\n",
    "for idx, row in simultaneous_missing_rows.iterrows():\n",
    "    missing_vars = [var for var in available_weather_vars if pd.isna(row[var])]\n",
    "    print(f\"  {idx}: {missing_vars}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10426cb",
   "metadata": {},
   "source": [
    "**Decision:** Simultaneous weather failures indicate **weather station outages** — operational events, not errors. These represent real system limitations during specific periods.\n",
    "\n",
    "*Holiday Information Gaps*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad3fa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze holiday missingness pattern\n",
    "if \"holiday\" in df.columns:\n",
    "    holiday_missing = df[\"holiday\"].isna().sum()\n",
    "    total_rows = len(df)\n",
    "    holiday_missing_pct = (holiday_missing / total_rows) * 100\n",
    "    \n",
    "    print(f\"\\nHoliday information gaps:\")\n",
    "    print(f\"Missing holiday labels: {holiday_missing} ({holiday_missing_pct:.1f}%)\")\n",
    "    \n",
    "    # Check if missing holidays cluster around known holiday periods\n",
    "    holiday_missing_months = (\n",
    "        df[df[\"holiday\"].isna()]\n",
    "        .groupby(df.index.to_period('M'))\n",
    "        .size()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "    print(f\"Holiday missingness by month (top 5):\")\n",
    "    print(holiday_missing_months.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d679af1",
   "metadata": {},
   "source": [
    "**Decision:** Missing holiday labels are likely **data processing gaps** rather than operational events. These can potentially be filled using external holiday calendars.\n",
    "\n",
    "**Step 2: If error, can I fix it with a rule?**\n",
    "\n",
    "Since we classified July clustering and weather station outages as **operational events** (not errors) in Step 1, only the holiday information gaps require rule-based fixing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266d49a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Step 2: Applying rule-based fixes to errors ===\")\n",
    "\n",
    "# Fix holiday information using business rules\n",
    "if \"holiday\" in df.columns:\n",
    "    # Create a simple holiday calendar for major US federal holidays\n",
    "    # In practice, you'd use a comprehensive holiday library\n",
    "    known_holidays = {\n",
    "        '2011-01-01': 1, '2011-07-04': 1, '2011-12-25': 1,\n",
    "        '2012-01-01': 1, '2012-07-04': 1, '2012-12-25': 1,\n",
    "        # Add more holidays as needed\n",
    "    }\n",
    "    \n",
    "    # Flag original missing holidays\n",
    "    df[\"flag_holiday_originally_missing\"] = df[\"holiday\"].isna()\n",
    "    \n",
    "    # Fill known holidays\n",
    "    holiday_fixes = 0\n",
    "    for date_str, holiday_val in known_holidays.items():\n",
    "        date = pd.to_datetime(date_str)\n",
    "        if date in df.index and pd.isna(df.loc[date, \"holiday\"]):\n",
    "            df.loc[date, \"holiday\"] = holiday_val\n",
    "            holiday_fixes += 1\n",
    "    \n",
    "    print(f\"Holiday values filled using calendar rules: {holiday_fixes}\")\n",
    "    \n",
    "    # Remaining missing holidays default to 0 (non-holiday)\n",
    "    remaining_missing = df[\"holiday\"].isna().sum()\n",
    "    df[\"holiday\"] = df[\"holiday\"].fillna(0)\n",
    "    print(f\"Remaining missing holidays set to 0 (non-holiday): {remaining_missing}\")\n",
    "\n",
    "print(\"\\nNote: Weather variables and July patterns are operational events, not errors\")\n",
    "print(\"→ They proceed directly to Step 3 for imputation strategy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de1c742",
   "metadata": {},
   "source": [
    "**Decision:** Holiday information gaps were successfully addressed using external calendar rules. Weather station outages and maintenance periods are legitimate operational events that require imputation rather than rule-based fixes.\n",
    "\n",
    "**Step 3: If cannot fix, should I impute or drop?**\n",
    "\n",
    "For unfixable missing values, we decide based on variable type and modeling requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba61b66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Step 3: Impute or drop strategy ===\")\n",
    "\n",
    "# Strategy for PREDICTORS (weather and other features)\n",
    "print(\"Weather predictors with missing values after error correction:\")\n",
    "for var in available_weather_vars:\n",
    "    if var in df.columns:\n",
    "        missing_count = df[var].isna().sum()\n",
    "        if missing_count > 0:\n",
    "            # Create flag for transparency\n",
    "            flag_name = f\"flag_{var}_missing\"\n",
    "            df[flag_name] = df[var].isna()\n",
    "            print(f\"  {var}: {missing_count} missing values → will be imputed later\")\n",
    "\n",
    "# Strategy for TARGET variable\n",
    "print(f\"\\nMissing target variable - DROPPING strategy:\")\n",
    "target_missing = df[\"count\"].isna().sum()\n",
    "df[\"flag_target_missing\"] = df[\"count\"].isna()\n",
    "\n",
    "print(f\"  'count': {target_missing} rows with missing target → will be dropped before modeling\")\n",
    "print(f\"  Reason: Never impute target variables for supervised learning\")\n",
    "\n",
    "# Strategy for operational gaps (preserve context)\n",
    "print(f\"\\nOperational gaps - PRESERVE WITH CONTEXT:\")\n",
    "df[\"flag_operational_gap\"] = df[\"flag_missing_timestamp\"] | df[\"flag_collapsed_from_duplicates\"]\n",
    "operational_gaps = df[\"flag_operational_gap\"].sum()\n",
    "print(f\"  {operational_gaps} hours affected by operational events → preserved with context flags\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789305ae",
   "metadata": {},
   "source": [
    "**Summary of workflow application:**\n",
    "\n",
    "1. **Step 1 (Event or Error):**\n",
    "   - July clustering → Operational maintenance events, preserved with context\n",
    "   - Weather station outages → Operational limitations, imputation needed\n",
    "   - Holiday gaps → Data processing errors, fixable with rules\n",
    "\n",
    "2. **Step 2 (Fix with rule):**\n",
    "   - Holiday information → Partially fixed using calendar rules\n",
    "\n",
    "3. **Step 3 (Impute or drop):**\n",
    "   - Predictor variables → Flagged for later imputation during preprocessing\n",
    "   - Target variable → Rows with missing targets dropped (never impute)\n",
    "   - Operational gaps → Preserved with context for client transparency\n",
    "\n",
    "The missing data workflow ensures that every gap is handled appropriately based on its cause and variable type. Operational events are preserved with context, fixable gaps are corrected systematically, and remaining missingness is flagged for later preprocessing (predictors) or dropping (targets) with complete transparency. This approach maintains data integrity while deferring sophisticated imputation to the preprocessing phase where it belongs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2930566",
   "metadata": {},
   "source": [
    "### 4.5. Professional Data Cleaning Checklist\n",
    "\n",
    "Here's your consultant-ready workflow for any data cleaning project:\n",
    "\n",
    "**Phase 1: Foundation**\n",
    "- [ ] Standardize timeline (collapse duplicates, enforce continuity)\n",
    "- [ ] Add flags for all structural changes\n",
    "\n",
    "**Phase 2: The Three-Step Decision Process**\n",
    "For every suspicious value:\n",
    "- [ ] **Step 1:** Event or error? (Context check)\n",
    "- [ ] **Step 2:** If error, can I fix with a rule? (Apply fix + flag)\n",
    "- [ ] **Step 3:** If unfixable, impute or drop? (Predictors vs. targets)\n",
    "\n",
    "**Phase 3: Documentation**\n",
    "- [ ] Flag every intervention with clear labels\n",
    "- [ ] Document aggregation policies and business logic\n",
    "- [ ] Quantify impact (\"15% of weather data imputed\")\n",
    "- [ ] Prepare client communication on data limitations\n",
    "\n",
    "**Quality Gates:**\n",
    "- ✅ No impossible values remain\n",
    "- ✅ Timeline is continuous and complete\n",
    "- ✅ Every change is flagged and traceable\n",
    "- ✅ Client can understand what was done and why\n",
    "\n",
    "This checklist ensures your cleaning is not just thorough, but **defendable to clients and auditable by colleagues**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb81bb1",
   "metadata": {},
   "source": [
    "## Summary and Transition to Feature Engineering Implementation\n",
    "\n",
    "You've mastered essential data quality assessment and cleaning techniques: systematic missing data analysis, outlier detection, and validation workflows. These skills transform messy real-world datasets into reliable, analytical-grade information.\n",
    "\n",
    "Your expertise in data completeness assessment, systematic cleaning procedures, and quality validation creates the trustworthy foundation needed for all advanced analysis. This technical rigor combined with business judgment enables you to work confidently with complex transportation datasets while maintaining analytical integrity.\n",
    "\n",
    "In our next lecture, we'll build on this clean data foundation by learning advanced feature engineering and preprocessing techniques that prepare your data for machine learning models. You'll see how quality data translates into effective model-ready features for predictive analytics."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
