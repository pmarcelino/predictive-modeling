{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e92c57b",
   "metadata": {},
   "source": [
    "# Lecture 4: Data Quality & Cleaning Essentials - Professional Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0339f13c",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this lecture, you will be able to:\n",
    "- Identify and categorize different types of data quality issues in transportation datasets\n",
    "- Apply systematic approaches to detect missing data, outliers, and inconsistencies\n",
    "- Choose appropriate strategies for handling missing data (elimination vs. imputation)\n",
    "- Implement professional data cleaning workflows using pandas methods\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a78a106",
   "metadata": {},
   "source": [
    "## 1. The Reality of Real-World Transportation Data\n",
    "\n",
    "As your consulting work progresses, you'll quickly discover a fundamental truth: **real-world data is messy**. The clean datasets you see in textbooks don't exist in professional practice. Your bike-sharing client's data comes from sensors that malfunction, weather stations that go offline, and databases that occasionally corrupt records.\n",
    "\n",
    "This messiness isn't just a technical inconvenience - it's a business-critical challenge. Poor data quality can lead to incorrect demand predictions, resulting in empty bike stations when customers need them or overflow situations where returning bikes becomes impossible. These operational failures directly impact customer satisfaction and revenue.\n",
    "\n",
    "Your role as a professional consultant is to transform messy, incomplete data into reliable foundations for business decision-making. This requires systematic approaches, clear documentation, and transparent communication about data limitations and cleaning procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b45398f",
   "metadata": {},
   "source": [
    "## 2. Common Data Quality Issues in Transportation Systems\n",
    "\n",
    "Messiness isn’t a single problem—it comes in many forms, each affecting your analysis in different ways. As a consultant, your job is to **recognize these issues early** and decide how to handle them before they undermine your predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe951e26",
   "metadata": {},
   "source": [
    "### 2.1. Understanding Data Quality Dimensions\n",
    "\n",
    "Data quality is not simply “good” or “bad.” Instead, it exists along multiple dimensions, each of which can influence your analysis in different ways. Let’s break down the five most important ones and see how they appear in transportation data:\n",
    "\n",
    "1. **Completeness**\n",
    "   *Definition:* Whether all expected values are present in the dataset.\n",
    "   *Why it matters:* Missing values create blind spots, especially when the missingness is not random.\n",
    "   *Example:* If bike-sharing sensors fail during storms, you may lose exactly the data needed to understand weather impacts on demand.\n",
    "\n",
    "2. **Accuracy**\n",
    "   *Definition:* The extent to which recorded values reflect reality.\n",
    "   *Why it matters:* Inaccurate values can mislead both descriptive analysis and predictive models.\n",
    "   *Example:* A temperature of -50°C recorded in Washington D.C. in July is a clear sensor error that could confuse demand models.\n",
    "\n",
    "3. **Consistency**\n",
    "   *Definition:* Whether data follows uniform formats, units, and scales.\n",
    "   *Why it matters:* Inconsistent formats can corrupt calculations and comparisons.\n",
    "   *Example:* Mixing Celsius and Fahrenheit in the same column, or having timestamps in multiple formats, leads to corrupted analysis.\n",
    "\n",
    "4. **Validity**\n",
    "   *Definition:* Whether values fall within logical or physically possible ranges.\n",
    "   *Why it matters:* Invalid data points indicate measurement or collection errors.\n",
    "   *Example:* Negative bike counts or humidity above 100% are impossible values that reveal collection problems.\n",
    "\n",
    "5. **Uniqueness**\n",
    "   *Definition:* Whether each observation is recorded only once.\n",
    "   *Why it matters:* Duplicate records inflate usage counts and distort demand predictions.\n",
    "   *Example:* If the same rental transaction is logged twice, it looks like demand is higher than it really was.\n",
    "\n",
    "**Understanding the Five-Dimensional Framework**\n",
    "\n",
    "Together, these five dimensions reveal how data quality problems can undermine predictions in different ways. Completeness affects the scope of your analysis, accuracy determines whether your insights reflect reality, consistency ensures reliable calculations, validity catches measurement errors, and uniqueness prevents false inflation of patterns. Recognizing these distinct aspects helps you target specific problems rather than applying generic \"data cleaning\" approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b0b637",
   "metadata": {},
   "source": [
    "### 2.2. Missing Data Patterns and Business Implications\n",
    "\n",
    "Not all missing data is created equal. In transportation systems, gaps in the dataset often occur under very specific conditions—the very conditions you want to analyze. Understanding when and why data goes missing helps you choose the right handling strategies and communicate risks to clients.\n",
    "\n",
    "**Weather-Driven Data Loss**\n",
    "\n",
    "The most common pattern involves sensor failures during extreme weather events. Storms, heavy rain, and temperature extremes can knock out monitoring equipment precisely when weather has the strongest influence on bike usage. This creates a double problem: you lose data exactly when you need it most, and standard statistical assumptions about \"random\" missingness don't apply.\n",
    "\n",
    "**Operational and Maintenance Gaps**\n",
    "\n",
    "Planned maintenance creates predictable but significant gaps in transportation data. These periods often overlap with major infrastructure changes—like opening new stations or updating software systems—meaning that ignoring the operational context could hide important business insights about system growth and performance.\n",
    "\n",
    "**Network Effects and Cascade Failures**\n",
    "\n",
    "Transportation systems are interconnected. When a high-demand station goes offline, neighboring stations typically experience unusual demand spikes. Without data from the offline station, it becomes difficult to distinguish between genuine demand growth and temporary displacement of riders to nearby locations.\n",
    "\n",
    "**Peak Period Vulnerabilities**\n",
    "\n",
    "Finally, data failures during rush hours pose special challenges because peak-period predictions drive critical business decisions. Missing data during these high-stakes periods requires more sophisticated handling strategies than gaps occurring during quiet off-peak hours when the business impact is minimal.\n",
    "\n",
    "**The Business Impact of Missing Data Patterns**\n",
    "\n",
    "These four patterns demonstrate why missing data analysis goes beyond simple counts and percentages. Each pattern creates specific risks for demand forecasting and requires tailored handling strategies. By understanding when and why data disappears, you can make informed decisions about imputation, communicate limitations clearly to clients, and avoid building models on unreliable foundations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599c3baa",
   "metadata": {},
   "source": [
    "### 2.3. Systematic Inconsistencies\n",
    "\n",
    "Now that we've examined missing data, let's shift to inconsistencies that are harder to detect. Beyond obvious errors like negative values, transportation datasets often contain **systematic inconsistencies** that masquerade as genuine patterns. These subtle problems are particularly dangerous because they can mislead analysis while appearing completely normal. As a consultant, actively searching for these hidden inconsistencies protects both your models and your credibility.\n",
    "\n",
    "The most common systematic inconsistencies in transportation data fall into four categories:\n",
    "\n",
    "* **Temporal Zone Confusion**\n",
    "  Mixing standard time and daylight saving time can create sudden “demand anomalies” around clock changes that have nothing to do with rider behavior.\n",
    "\n",
    "* **Geographic Coordinate Systems**\n",
    "  If different coordinate systems are used to record station locations, spatial analysis may reveal patterns that are purely artifacts of mismatched formats.\n",
    "\n",
    "* **Operational Status Mixing**\n",
    "  True zero demand (no rides) is very different from a station being unavailable due to maintenance or full capacity. Mixing these cases together creates artificial demand patterns that models mistakenly learn.\n",
    "\n",
    "* **Unit Inconsistencies**\n",
    "  Data collected in multiple units—such as Fahrenheit vs. Celsius, or miles vs. kilometers—corrupts correlations and model training if not standardized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25c5d15",
   "metadata": {},
   "source": [
    "## 3. Data Quality Assessment Process\n",
    "\n",
    "As a consultant, clients will expect you to follow a systematic, defensible process for data quality assessment — not ad-hoc checking. The 4-step process we'll use gives you a professional framework that you can explain and justify to any client:\n",
    "\n",
    "1. **Quick Data Quality Checks** – your first diagnostic scan to flag obvious issues.\n",
    "2. **Time Series Integrity Check** – systematic analysis of temporal continuity.\n",
    "3. **Outlier Detection** – comprehensive detection of anomalies.\n",
    "4. **Missing Data Detection** – detailed analysis of completeness.\n",
    "\n",
    "This systematic approach demonstrates expertise and builds client confidence from day one. Steps 1, 3, and 4 apply universally across domains. Step 2 becomes critical for time-series data, like the one we usually see in transportation problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56847b7e",
   "metadata": {},
   "source": [
    "### 3.1. Quick Data Quality Checks\n",
    "\n",
    "**Definition:** Quick data quality checks are rapid diagnostic scans that assess whether a dataset is fundamentally sound before investing time in detailed analysis or modeling.\n",
    "\n",
    "**Explanation:** Think of this like a consultant's \"triage\" — in just a few minutes, you want to know whether the dataset looks broadly reliable, where the biggest risks lie, and which areas deserve closer investigation. When you receive a new dataset from a client, this scan is your first step, not modeling.\n",
    "\n",
    "**Purpose:** These lightweight checks flag obvious issues across structure, value ranges, and cross-variable plausibility. We won't yet explain problems in depth or attempt fixes — that comes later. The goal is rapid risk assessment.\n",
    "\n",
    "**Structural Snapshot**\n",
    "\n",
    "The first thing we do is take a **structural snapshot** of the dataset: how many rows and columns it has, and whether the variables are of the expected type. This step sounds simple, but it’s one of the fastest ways to detect import errors, unexpected row counts, or inconsistencies in data types — all of which can indicate bigger problems lurking beneath the surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe5231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Washington D.C. bike-sharing dataset (intentionally messy version)\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset-teaching-lec-04.csv\")\n",
    "\n",
    "# Check dataset dimensions and data types\n",
    "print(f\"Rows: {df.shape[0]}, Columns: {df.shape[1]}\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18b93bd",
   "metadata": {},
   "source": [
    "> **Note:** You may have noticed something unusual here: the results don't look exactly like what we saw in the previous lecture. That's intentional. From this point forward, we'll sometimes work with a slightly modified version of the Washington D.C. dataset. We've made it \"messy\" on purpose so you can practice handling real-world problems. As a consultant, you'll rarely get a dataset that's perfectly clean — each phase of the course will bring new challenges for you to detect and resolve.\n",
    "\n",
    "When we run this quick check, a few important concerns stand out immediately:\n",
    "\n",
    "* The **`holiday` column has missing values**. This means that not every day is properly labeled as a holiday or not — a detail that could easily distort demand forecasts, since holiday patterns differ sharply from regular weekdays.\n",
    "* The **weather-related variables** such as `temp`, `humidity`, and `windspeed` also contain gaps. Because these are some of the most important explanatory variables in our forecasting model, missingness here reduces our ability to explain variation in bike rentals.\n",
    "* Most critically, the **`count` column — our target variable — is missing in several rows**. This is a red flag: every missing entry in `count` means lost training data, and the reliability of our model hinges on how much usable demand history we have.\n",
    "\n",
    "This single, simple scan already tells us that the dataset cannot be used “as is” for modeling. More importantly, it shows why **structural checks are powerful**: with just one command, we’ve uncovered problems in both our explanatory variables and our target.\n",
    "\n",
    "This is exactly the kind of insight to highlight at the project start: *\"Before we can move into forecasting, we've already identified major gaps in the dataset that could affect both explanatory power and prediction accuracy.\"*\n",
    "\n",
    "**Value Sanity Checks**\n",
    "\n",
    "After confirming the dataset’s structure, the next step is to ask: *“Do the values themselves make sense?”*\n",
    "\n",
    "Every variable has **natural boundaries** defined by either business rules or physical limits:\n",
    "\n",
    "* Bike rentals cannot be negative.\n",
    "* Humidity must fall between 0% and 100%.\n",
    "* Local temperatures should stay within climate-appropriate ranges.\n",
    "\n",
    "Values that fall outside these ranges are not just unusual — they are almost always errors caused by faulty sensors, bad data entry, or processing mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d1aaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic range checks\n",
    "print(\"Negative rentals:\", (df[\"count\"] < 0).sum())\n",
    "print(\"Humidity out of range:\", ((df[\"humidity\"] < 0) | (df[\"humidity\"] > 100)).sum())\n",
    "print(\"Temperature extremes:\", df[\"temp\"].describe()[[\"min\", \"max\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624007aa",
   "metadata": {},
   "source": [
    "Running these quick checks reveals three immediate red flags:\n",
    "\n",
    "* We find **6 cases of negative rentals**, which is logically impossible — you can’t rent fewer than zero bikes. This is most likely a logging or entry error.\n",
    "* We spot **10 humidity values above 100%**, which is physically impossible. This usually points to a faulty sensor reading or an ingestion problem.\n",
    "* Finally, the **temperature maximum is close to 100°C**. While summers in Washington D.C. can be hot, they certainly don’t reach boiling point! This extreme value is almost certainly an error that could distort averages or mislead a forecasting model.\n",
    "\n",
    "Together, these findings show why **range validation is essential**. With just a few simple checks, we can identify values that clearly break real-world rules — and if left undetected, they could slip into analysis, biasing results and damaging credibility with clients.\n",
    "\n",
    "This step builds trust: you demonstrate that you're not just running models blindly, but verifying whether the data itself reflects reality.\n",
    "\n",
    "**Cross-Variable Plausibility**\n",
    "\n",
    "Numbers can look fine in isolation but make no sense once you compare them across variables. That’s why a good quick check also includes a **plausibility scan across related variables**. In transportation data, the most important relationship to test is usually between **demand** and **context variables** like weather.\n",
    "\n",
    "For example, common sense (and business experience) tells us that **bike rentals should fall when weather conditions worsen**. If the dataset shows the opposite, that’s a red flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f933a6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick plausibility check: average rentals by weather condition\n",
    "avg_rentals_by_weather = df.groupby(\"weather\")[\"count\"].mean()\n",
    "print(avg_rentals_by_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727a080e",
   "metadata": {},
   "source": [
    "At first glance, the results follow expectations:\n",
    "\n",
    "- Rentals are **highest on clear days (weather = 1)**.\n",
    "- They gradually decrease as conditions worsen to misty or light rain.\n",
    "\n",
    "But the final category is suspicious. Under **heavy rain or storms (weather = 4)**, the dataset shows an **average of more than 600 rentals per hour** — even higher than on sunny days.\n",
    "\n",
    "This makes little business sense: real-world demand should drop sharply in severe weather, not skyrocket. Such an inconsistency usually points to:\n",
    "\n",
    "- **Mislabelled weather codes**, or\n",
    "- A **misalignment between weather feeds and rental logs**.\n",
    "\n",
    "Left uncorrected, such errors could lead to false conclusions like *\"bike demand is resilient during storms\"* — which in turn could drive poor operational decisions, such as overstocking bikes or overscheduling staff during extreme weather events.\n",
    "\n",
    "This kind of cross-variable check is a reminder: **some errors only appear when you look at relationships, not just single columns**. That’s why consultants always test whether the data’s “story” matches real-world logic.\n",
    "\n",
    "**Concluding Reflection on Quick Checks**\n",
    "\n",
    "Our quick scan uncovered multiple issues:\n",
    "- **Structural problems** in explanatory variables and target `count`\n",
    "- **Impossible values** like negative rentals and extreme temperatures  \n",
    "- **Cross-variable inconsistencies** where weather and demand defy business logic\n",
    "\n",
    "This initial scan shows the dataset is not ready for modeling. We need deeper investigation of missing data and outliers before drawing conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98100ce2",
   "metadata": {},
   "source": [
    "### 3.2. Temporal Continuity Check\n",
    "\n",
    "**Definition:** Temporal continuity refers to whether your time-series data has a complete, consistent timeline without gaps, duplicates, or misaligned sequences.\n",
    "\n",
    "**Explanation:** Transportation data is inherently time-based. If the timeline itself is broken, then any further cleaning, imputation, or modeling will rest on shaky foundations. Missing hours create false patterns, duplicates skew averages, and misaligned sequences break seasonal analysis.\n",
    "\n",
    "**Purpose:** Before we tackle outliers or missing values, we must verify the timeline's integrity to ensure reliable foundations for all subsequent work.\n",
    "\n",
    "To do so, we run a timeline diagnostic. This tells us:\n",
    "\n",
    "- The first and last timestamp in the dataset.\n",
    "- How many hours should exist in that range.\n",
    "- How many unique hours actually exist.\n",
    "- How many are missing.\n",
    "- How many duplicate rows we have for the same hour.\n",
    "\n",
    "This gives us a quick sense of whether the dataset is complete and well-aligned, or whether we’re missing entire blocks of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11b85be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ensure datetime is properly parsed\n",
    "data_path = \"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset-teaching-lec-04.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "df[\"datetime\"] = pd.to_datetime(df[\"datetime\"], errors=\"coerce\")\n",
    "df = df.sort_values(\"datetime\").reset_index(drop=True)\n",
    "\n",
    "# Identify coverage\n",
    "t_min = df[\"datetime\"].min()\n",
    "t_max = df[\"datetime\"].max()\n",
    "\n",
    "# Duplicated timestamps\n",
    "n_dup_rows = df.duplicated(subset=[\"datetime\"]).sum()\n",
    "\n",
    "# Build expected hourly range\n",
    "expected = pd.date_range(t_min, t_max, freq=\"h\")\n",
    "actual = pd.Index(df[\"datetime\"].unique())\n",
    "\n",
    "n_expected = len(expected)\n",
    "n_actual = len(actual)\n",
    "n_missing_hours = len(expected.difference(actual))\n",
    "\n",
    "print(\"=== Timeline quick check ===\")\n",
    "print(\"time_min:\", t_min)\n",
    "print(\"time_max:\", t_max)\n",
    "print(\"expected_hours:\", n_expected)\n",
    "print(\"present_unique_hours:\", n_actual)\n",
    "print(\"missing_hours:\", n_missing_hours)\n",
    "print(\"duplicate_rows:\", n_dup_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b7010b",
   "metadata": {},
   "source": [
    "Between January 2011 and December 2012, the dataset should contain **17,256 hourly rows**. In reality, it only contains **10,862 unique hours**, leaving **6,394 hours missing**. That's more than a third of the timeline absent — a major structural gap. We also see **24 duplicate rows**, meaning some hours are represented more than once.\n",
    "\n",
    "This is a critical insight: the raw data cannot be trusted as a continuous timeline. Large gaps undermine seasonal analysis, and duplicates risk double-counting demand. Before any modeling, we need to fix both problems. We will show how to do that in Chapter 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e06cd44",
   "metadata": {},
   "source": [
    "### 3.3. Outlier Detection\n",
    "\n",
    "**Definition:** Outlier detection identifies data points that deviate significantly from the expected pattern — values that are unusually high, unusually low, or inconsistent with the rest of the dataset.\n",
    "\n",
    "**Explanation:** In transportation data, outliers can represent legitimate extreme events (like snowstorms causing demand drops), data collection errors (like negative bike rentals), or operational anomalies (like maintenance affecting normal patterns). Each type requires different treatment strategies.\n",
    "\n",
    "**Purpose:** The goal is not just to detect outliers, but to classify them correctly so that legitimate events are preserved while errors are corrected. This ensures models learn from real patterns rather than data quality issues.\n",
    "\n",
    "Outliers matter in urban mobility because they can carry very different meanings:\n",
    "\n",
    "- **Legitimate Extreme Events**: Real extraordinary conditions like snowstorms or city-wide festivals that models should account for\n",
    "- **Data Collection Errors**: Sensor malfunctions or impossible values that need correction or removal\n",
    "- **Operational Anomalies**: Maintenance activities or system changes that create artificial patterns\n",
    "\n",
    "Now that we understand what outliers are, let’s look at the different ways to detect them. Outlier detection methods generally fall into three categories:\n",
    "\n",
    "1. **Statistical Methods**: Mathematical rules based on the distribution of the data.\n",
    "2. **Business Logic Methods**: Checks based on domain knowledge and system constraints.\n",
    "3. **Temporal Methods**: Techniques that consider the time-based nature of transportation data.\n",
    "\n",
    "**Statistical Methods**\n",
    "\n",
    "Statistical methods use mathematical formulas to identify unusual values. They don’t require prior knowledge of the transportation system - they just look at how far a data point is from what is “normal” in the dataset.\n",
    "\n",
    "There are several statistical approaches, such as:\n",
    "\n",
    "- **Z-Score Analysis**\n",
    "- **Interquartile Range (IQR) Method**\n",
    "- **Modified Z-Score**\n",
    "\n",
    "In this lecture, we will focus on just one example: **Z-Score Analysis**. A **Z-score** tells us how many “standard steps” (standard deviations) a data point is away from the average.\n",
    "\n",
    "$$\n",
    "Z = \\frac{x - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $x$ = the value we’re checking\n",
    "- $\\mu$ = the mean (average) of the data\n",
    "- $\\sigma$ = the standard deviation (how spread out the data is)\n",
    "\n",
    "Think of the average bike rentals per day like the “center of gravity” of the data. Most days will be close to that average. The Z-score is like a distance meter: it tells us how far a particular day is from the typical pattern.\n",
    "\n",
    "- A Z-score of **0** → exactly average.\n",
    "- A Z-score of **+2** → two steps above average (busier than normal).\n",
    "- A Z-score of **–3** → three steps below average (quieter than normal).\n",
    "\n",
    "When a Z-score is bigger than 3 or smaller than –3, the value is far enough from the average that we should pause and ask: *Is this a real event, or is it an error?*\n",
    "\n",
    "We use Z-scores because they:\n",
    "\n",
    "- **Standardize values** so we can compare across variables.\n",
    "- **Give a simple rule of thumb**: beyond 3 = unusual.\n",
    "- **Provide a quick first filter** before applying more advanced techniques.\n",
    "\n",
    "Let’s see how this works in practice using the Washington D.C. bike-sharing dataset. We’ll calculate the Z-scores for daily demand (`count`) and flag potential outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2514b243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the Washington D.C. bike-sharing dataset\n",
    "data_path = \"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset-teaching-lec-04.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Convert datetime column to pandas datetime type\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "# Aggregate rentals by day\n",
    "daily_rentals = df.groupby(df['datetime'].dt.date)['count'].sum().reset_index()\n",
    "\n",
    "# Calculate mean and std\n",
    "mean = daily_rentals['count'].mean()\n",
    "std = daily_rentals['count'].std()\n",
    "\n",
    "# Compute Z-scores\n",
    "daily_rentals['z_score'] = (daily_rentals['count'] - mean) / std\n",
    "\n",
    "# Flag outliers (|Z| > 3)\n",
    "outliers = daily_rentals[daily_rentals['z_score'].abs() > 3]\n",
    "\n",
    "outliers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc8f2a9",
   "metadata": {},
   "source": [
    "Both May 15 and November 3 show exceptionally high rental counts, far beyond typical daily demand. These are unlikely to be ordinary fluctuations. They could represent special city-wide events or anomalies in how trips were logged. As consultants, we need to cross-check these dates with event calendars and system logs to confirm whether these spikes reflect genuine demand or possible data quality issues.\n",
    "\n",
    "**Business Logic Methods**\n",
    "\n",
    "While statistical methods rely purely on mathematical rules, **business logic methods** use knowledge of the system and its physical constraints to detect outliers. Instead of asking, “Does this number look statistically unusual?”, we ask, “Is this number even possible given how the transportation system works?”\n",
    "\n",
    "Business logic methods build rules like these based on:\n",
    "\n",
    "- **Physical constraints**: e.g., a bike station cannot have negative bikes, nor can it rent more bikes than its maximum capacity.\n",
    "- **Historical ranges**: e.g., demand has never exceeded 1,200 rentals in a day; a value above this threshold is suspicious.\n",
    "- **Cross-variable checks**: e.g., it shouldn’t be possible to record “heavy rain” alongside “record-high bike usage.”\n",
    "\n",
    "Let's see an example of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca50d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Washington D.C. bike-sharing dataset\n",
    "data_path = \"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset-teaching-lec-04.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Convert datetime column to pandas datetime type\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "# Check for physical constraint violations in windspeed\n",
    "invalid_windspeed = df[(df['windspeed'] < 0) | (df['windspeed'] > 60)]\n",
    "\n",
    "invalid_windspeed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7342db",
   "metadata": {},
   "source": [
    "The flagged records show windspeed values of `200.0`, which are far beyond any physically possible measurement for this system. These values clearly indicate sensor or recording errors rather than real-world weather conditions. If left uncorrected, they could distort downstream models, for example by falsely associating extreme winds with normal rental demand.\n",
    "\n",
    "**Temporal Methods**\n",
    "\n",
    "Transportation data is inherently tied to **time**. Unlike static datasets, values change depending on the hour, day, season, or long-term trends. **Temporal outlier detection methods** focus on identifying unusual data points that break these expected time-based patterns. For example:\n",
    "\n",
    "- A sudden drop in rentals during a weekday morning rush hour might indicate a system outage.\n",
    "- A sharp jump in rentals during winter could mean a special event.\n",
    "- A long-term shift in demand may signal that the system has grown or changed in some way.\n",
    "\n",
    "Some common temporal approaches include:\n",
    "\n",
    "- **Change Point Detection**: Identifying sudden structural shifts in the data (e.g., a new station or policy).\n",
    "- **Seasonal Anomaly Detection**: Checking if values align with expected seasonal patterns.\n",
    "- **Trend Deviation Analysis**: Comparing current values to long-term growth or decline.\n",
    "\n",
    "Let's see an example of a seasonal anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7f27d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Washington D.C. bike-sharing dataset\n",
    "data_path = \"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset-teaching-lec-04.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Convert datetime and build daily_rentals with a month column\n",
    "df[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\n",
    "daily_rentals = (\n",
    "    df.groupby(df[\"datetime\"].dt.date)[\"count\"]\n",
    "      .sum()\n",
    "      .reset_index()\n",
    "      .rename(columns={\"datetime\": \"date\"})\n",
    ")\n",
    "daily_rentals[\"date\"] = pd.to_datetime(daily_rentals[\"date\"])\n",
    "daily_rentals[\"month\"] = daily_rentals[\"date\"].dt.month\n",
    "\n",
    "# Define \"winter\" as Dec-Feb and \"summer\" as Jun-Aug\n",
    "winter_months = [12, 1, 2]\n",
    "summer_months = [6, 7, 8]\n",
    "\n",
    "winter_avg = daily_rentals[daily_rentals['month'].isin(winter_months)]['count'].mean()\n",
    "summer_avg = daily_rentals[daily_rentals['month'].isin(summer_months)]['count'].mean()\n",
    "\n",
    "# Flag unusual winter days (too close to summer levels)\n",
    "winter_anomalies = daily_rentals[\n",
    "    (daily_rentals['month'].isin(winter_months)) & \n",
    "    (daily_rentals['count'] > summer_avg * 0.8)\n",
    "]\n",
    "\n",
    "# Flag unusual summer days (too close to winter levels)\n",
    "summer_anomalies = daily_rentals[\n",
    "    (daily_rentals['month'].isin(summer_months)) & \n",
    "    (daily_rentals['count'] < winter_avg * 1.2)\n",
    "]\n",
    "\n",
    "seasonal_anomalies = pd.concat([winter_anomalies, summer_anomalies])\n",
    "seasonal_anomalies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e85ee6f",
   "metadata": {},
   "source": [
    "The detected anomalies highlight days in winter months (January and February) with demand levels much closer to what we’d expect in summer. For example, January 7 shows more than double the average rentals for that month. These could indicate unusually warm days that encouraged cycling, or they might reflect special events. In practice, such findings should be validated with weather data or event calendars. This illustrates how seasonal anomaly detection helps identify values that break expected seasonal patterns, providing valuable clues about real-world influences on demand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3450e82b",
   "metadata": {},
   "source": [
    "### 3.4. Missing Data Detection\n",
    "\n",
    "**Definition:** Missing data detection is the systematic analysis of gaps in your dataset to understand their patterns, causes, and potential impact on modeling and analysis.\n",
    "\n",
    "**Explanation:** Unlike outliers which are individual problematic values, missing data represents systematic gaps that can undermine entire analyses. Missingness often follows patterns — clustering in certain periods, affecting groups of variables together, or reflecting underlying system failures like sensor outages.\n",
    "\n",
    "**Purpose:** We analyze missing data patterns to design targeted cleaning strategies rather than applying generic fixes. Understanding where, when, and why data goes missing ensures our solutions address root causes and preserve data integrity.\n",
    "\n",
    "We'll explore missing data through three systematic analyses:\n",
    "- **Quantitative assessment** — count and percentage of missing values per variable\n",
    "- **Temporal pattern analysis** — check if gaps cluster in specific time periods  \n",
    "- **Cross-variable analysis** — identify which variables go missing together\n",
    "\n",
    "**Quantitative Assessment of Missing Data**\n",
    "\n",
    "A **quantitative assessment** inventories missing values across all columns, showing both you and the client the scale and location of gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dae8d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values and calculate percentages\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_percentages = (missing_counts / len(df)) * 100\n",
    "\n",
    "# Display side by side for clarity\n",
    "pd.DataFrame({\"Missing\": missing_counts, \"Percentage\": missing_percentages})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67986277",
   "metadata": {},
   "source": [
    "Our scan shows three main areas of concern:\n",
    "\n",
    "- First, the `holiday` column is missing in about **5% of rows**, which means we can't always tell whether a given day was a holiday — a potentially important driver of demand.\n",
    "- Second, weather-related variables (`temp`, `atemp`, `humidity`, `windspeed`) are missing in around **0.6% of cases each**. That may sound small, but because they often go missing together, it likely reflects a sensor or reporting problem.\n",
    "- Finally, and most importantly, the **target variables** (`casual`, `registered`, and `count`) are missing in **146 rows**, or about **1.3% of the dataset**.\n",
    "\n",
    "Any missing demand values directly reduce the amount of training data available for forecasting, which is bad.\n",
    "\n",
    "**Temporal Pattern Analysis**\n",
    "\n",
    "In transportation datasets, missing data often clusters in specific periods. This makes **temporal analysis** essential: by grouping missingness across months or seasons, we can check whether the gaps are random or systematically tied to certain time periods.\n",
    "\n",
    "This matters for forecasting because if missingness is concentrated in peak demand months, any model we build will be biased or incomplete in those periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20970fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year and month into a new column\n",
    "df['year_month'] = df['datetime'].dt.strftime('%Y-%m')\n",
    "\n",
    "# Group by the new 'year_month' column and calculate missing values\n",
    "missing_by_month = df.groupby(df['year_month'])['count'].apply(lambda x: x.isnull().sum())\n",
    "print(missing_by_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28471259",
   "metadata": {},
   "source": [
    "The temporal scan reveals that missing demand values are not evenly spread across the dataset. Instead, they cluster heavily in **July 2011 and July 2012**. This points to a systematic issue, such as a recurring sensor outage or reporting gap during summer months. For the client, this has a clear implication: forecasts for peak-season demand may be less reliable unless these gaps are addressed. It’s not just random noise — it’s a structural weakness in the dataset that could distort decision-making during the busiest time of year.\n",
    "\n",
    "**Cross-Variable Analysis of Missing Data**\n",
    "\n",
    "Sometimes, missingness in one variable aligns with gaps in others. This is a crucial diagnostic step: it helps distinguish between isolated issues (e.g., a single column not recorded) and **system-wide failures** (e.g., a weather station outage affecting several variables at once).\n",
    "\n",
    "By checking which variables tend to go missing together, we can form a more realistic hypothesis about the underlying cause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c264c04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check rows where at least one weather-related variable is missing\n",
    "weather_missing = df[df[[\"temp\", \"humidity\", \"windspeed\"]].isnull().any(axis=1)]\n",
    "print(weather_missing.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1030743",
   "metadata": {},
   "source": [
    "The cross-variable check confirms that missingness is not isolated — entire blocks of weather data (`temp`, `atemp`, `humidity`, `windspeed`) disappear at the same time. For example, on **February 11, 2011**, several consecutive hours show all weather variables missing together. This strongly suggests a **weather station outage or reporting failure**, not random gaps. Recognizing that these variables fail together allows us to design a coordinated cleaning strategy rather than treating each column as an independent problem.\n",
    "\n",
    "**Concluding Reflection on Missing Data**\n",
    "\n",
    "Our analysis shows that missingness is not random noise — it's a **systematic problem** with serious implications:\n",
    "\n",
    "- **Quantitative scan:** Notable gaps in `holiday`, weather variables, and target `count`\n",
    "- **Temporal analysis:** Gaps cluster in specific months, particularly July\n",
    "- **Cross-variable analysis:** Weather variables often fail together, indicating sensor outages\n",
    "\n",
    "This structured diagnosis transforms vague concerns into actionable insights. Instead of simply saying *\"the dataset is incomplete\"*, we can now explain precisely **where, when, and why** missingness occurs.\n",
    "\n",
    "This prepares us for the next step. In **Data Cleaning Techniques**, we'll move from diagnosis to **treatment**: applying targeted strategies to address missingness (alongside outliers and other issues), ensuring the dataset is ready for reliable modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2514225a",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning Strategies and Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf6c963",
   "metadata": {},
   "source": [
    "### 4.1. From Assessment to Treatment: Where Clients See Value\n",
    "\n",
    "We've diagnosed the problems in our bike-sharing dataset: impossible values, extreme outliers, missing data blocks, and timeline gaps. **Now comes the treatment stage** — this is where clients see the real consulting value.\n",
    "\n",
    "Diagnosis impresses clients with your analytical rigor, but **cleaning delivers the reliable data foundation** they need for business decisions. While assessment shows what's wrong, cleaning demonstrates how you solve problems systematically and transparently.\n",
    "\n",
    "This transition from \"finding issues\" to \"fixing issues\" represents the shift from diagnostic consultant to solution provider. Clients pay for datasets they can trust, models they can deploy, and insights they can act on.\n",
    "\n",
    "**Data cleaning** is not cosmetic work to make datasets \"look nice.\" It's a structured process that:\n",
    "\n",
    "- Distinguishes **errors** from **real events**\n",
    "- Applies **consistent, rule-based fixes** where possible\n",
    "- Decides when to **impute or drop** values that cannot be fixed\n",
    "- Keeps every change **transparent and auditable**\n",
    "\n",
    "This systematic approach ensures that your cleaning decisions can be explained, defended, and replicated — critical requirements for professional consulting work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7017863b",
   "metadata": {},
   "source": [
    "### 4.2. The Unified Cleaning Workflow\n",
    "\n",
    "Since we are working with time-series data, we will start by **standardizing the timeline** and fixing its structural problems — this is a critical first step that ensures we have a reliable temporal foundation.\n",
    "\n",
    "From there, we will follow a standard data cleaning workflow that, while always dependent on the specific dataset and business context, can be generalized into a systematic process that learners can apply to their own projects. Once the timeline is reliable, every suspicious value — whether it’s extreme, impossible, or missing — is treated with the same **three-step decision tree**:\n",
    "\n",
    "1. **Is this an event or an error?**\n",
    "\n",
    "   - *Event* → keep, but **flag** (e.g., snowstorm, festival).\n",
    "   - *Error* → continue.\n",
    "\n",
    "2. **If error: Can I fix it with a rule?**\n",
    "\n",
    "   - Examples: cap humidity to 100, relabel mis-coded weather categories, set negative rentals to `NaN`.\n",
    "   - If yes → **fix and flag**.\n",
    "\n",
    "3. **If cannot fix: Should I impute or drop?**\n",
    "\n",
    "   - **Predictors (features):** impute if valuable, drop if not.\n",
    "   - **Target (`count`):** never impute for modeling → drop missing rows.\n",
    "   - Always **flag** imputations or dropped ranges.\n",
    "\n",
    "👉 **Why flag?**\n",
    "Flagging ensures that every change is **visible, auditable, and explainable**. Flags allow you to:\n",
    "\n",
    "- Compare model performance with and without imputed values.\n",
    "- Communicate risks to clients (*“July demand is less reliable: 20% of weather values were imputed.”*).\n",
    "- Keep a record of what changed and why.\n",
    "\n",
    "This mindset — *event or error? fix, impute, or drop? always flag* — is the backbone of professional data cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e8763b",
   "metadata": {},
   "source": [
    "### 4.3. Standardizing the Timeline\n",
    "\n",
    "As we saw in Chapter 4, the timeline is not reliable: we have missing hours and duplicate rows. To fix this, we will:\n",
    "\n",
    "- Collapse duplicate rows.\n",
    "- Reindex to a continuous hourly timeline and flag inserted hours.\n",
    "\n",
    "**Collapse Duplicate Rows**\n",
    "\n",
    "Let's start by collapsing duplicate rows. Having more than one row per hour is inconsistent with how the system should work: the bike-sharing dataset should have exactly one record for each hour.\n",
    "\n",
    "But how do we combine duplicates? We use a **clear, rule-based aggregation policy**:\n",
    "\n",
    "- For **targets** (`count`, `casual`, `registered`): **SUM** the values, since demand is additive.\n",
    "- For **numeric predictors** (e.g., weather variables): take the **MEAN**, since conditions are measured as averages.\n",
    "- For **categorical variables** (`holiday`, `weather`, `season`): take the **FIRST** value, as codes should be stable within an hour.\n",
    "\n",
    "First, we'll identify and collapse duplicate rows using a rule-based aggregation strategy. This ensures consistent handling while preserving the underlying data meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16dba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Identify duplicated timestamps\n",
    "rows_per_ts = df.groupby(\"datetime\").size().rename(\"n_rows_per_ts\")\n",
    "duplicated_ts = rows_per_ts[rows_per_ts > 1].index\n",
    "n_dup_timestamps = len(duplicated_ts)\n",
    "n_dup_rows = int((rows_per_ts[rows_per_ts > 1] - 1).sum())\n",
    "\n",
    "print(f\"Duplicated timestamps: {n_dup_timestamps} | Extra duplicate rows to collapse: {n_dup_rows}\")\n",
    "\n",
    "# Build aggregation policy\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "agg_map = {c: \"mean\" for c in numeric_cols}\n",
    "\n",
    "for c in [\"count\", \"casual\", \"registered\"]:\n",
    "    if c in agg_map:\n",
    "        agg_map[c] = \"sum\"\n",
    "\n",
    "for c in [\"holiday\", \"weather\", \"season\"]:\n",
    "    if c in df.columns:\n",
    "        agg_map[c] = \"first\"\n",
    "\n",
    "# Aggregate to one row per hour\n",
    "df = (\n",
    "    df.groupby(\"datetime\", as_index=False)\n",
    "      .agg(agg_map)\n",
    "      .sort_values(\"datetime\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Add a flag for hours that were collapsed from duplicates\n",
    "df[\"flag_collapsed_from_duplicates\"] = df[\"datetime\"].isin(duplicated_ts)\n",
    "\n",
    "print(\"Collapse complete → policy: SUM targets, MEAN numeric predictors, FIRST categoricals.\")\n",
    "print(\"Hours affected:\", int(df[\"flag_collapsed_from_duplicates\"].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcc6d80",
   "metadata": {},
   "source": [
    "We collapsed **24 duplicated hours**, removing 24 extra rows from the dataset. The aggregation policy ensures that:\n",
    "\n",
    "- Demand counts remain correct (no double-counting).\n",
    "- Weather predictors reflect average conditions.\n",
    "- Categorical codes remain stable.\n",
    "\n",
    "The flag `flag_collapsed_from_duplicates` marks these hours so we can always trace which rows were affected. For transparency, this is important: if a client later asks why certain hours look unusual, we can point to the duplication issue.\n",
    "\n",
    "**Reindex to a Continuous Hourly Timeline**\n",
    "\n",
    "Finally, we enforce a **continuous hourly index**. Right now, the dataset simply skips missing hours — they aren’t represented at all. This makes gaps invisible and impossible to handle systematically.\n",
    "\n",
    "By reindexing:\n",
    "\n",
    "- We insert a row for **every missing hour**.\n",
    "- Those rows will contain `NaN`s for predictors and/or target.\n",
    "- We add a flag to mark which rows were **inserted**.\n",
    "\n",
    "Next, we'll enforce a continuous hourly timeline by inserting rows for missing hours. This makes all gaps explicit and manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d7ba58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the full hourly index\n",
    "time_min = df[\"datetime\"].min()\n",
    "time_max = df[\"datetime\"].max()\n",
    "full_hours = pd.date_range(time_min, time_max, freq=\"h\")\n",
    "\n",
    "# Keep original set of hours\n",
    "original_hours = pd.Index(df[\"datetime\"])\n",
    "\n",
    "# Reindex and flag\n",
    "df = df.set_index(\"datetime\").reindex(full_hours)\n",
    "df.index.name = \"datetime\"\n",
    "df[\"flag_missing_timestamp\"] = ~df.index.isin(original_hours)\n",
    "\n",
    "# Audit\n",
    "inserted_hours = int(df[\"flag_missing_timestamp\"].sum())\n",
    "total_hours = len(df)\n",
    "present_hours = total_hours - inserted_hours\n",
    "\n",
    "print(\"=== Reindex audit ===\")\n",
    "print(\"time_min:\", time_min)\n",
    "print(\"time_max:\", time_max)\n",
    "print(\"total_hours_after_reindex:\", total_hours)\n",
    "print(\"present_hours_from_source:\", present_hours)\n",
    "print(\"inserted_missing_hours:\", inserted_hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5444f6f",
   "metadata": {},
   "source": [
    "After reindexing, the dataset now has **17,256 rows** — one for each expected hour. Of these, **6,394 rows were inserted** to represent missing hours. These rows currently contain `NaN`s, which is exactly what we want: the gaps are now explicit and can be handled in the cleaning workflow.\n",
    "\n",
    "From here:\n",
    "\n",
    "- For **predictors**, missing values can be imputed using interpolation or seasonal medians.\n",
    "- For the **target (`count`)**, rows with missing demand must be dropped before model training.\n",
    "- The flag `flag_missing_timestamp` allows us to communicate clearly to clients how much of the dataset is reconstructed rather than observed.\n",
    "\n",
    "With these two steps, we’ve established a **reliable timeline**. The dataset now has exactly one row per hour, duplicates resolved, and missing periods made explicit. This creates the solid foundation we need before applying the unified cleaning workflow to outliers and missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd14355",
   "metadata": {},
   "source": [
    "### 4.4. Applying the Workflow to Outliers\n",
    "\n",
    "Earlier we learned to detect outliers; now we'll apply the unified workflow to treat them systematically. The key shift is from detection to decision-making.\n",
    "\n",
    "We apply the three-step workflow to each outlier:\n",
    "\n",
    "1. **Is this an event or an error?**\n",
    "\n",
    "   - *Event* → keep, but **flag** for transparency.\n",
    "   - *Error* → continue.\n",
    "\n",
    "2. **If error: Can I fix it with a rule?**\n",
    "\n",
    "   - If yes → **fix and flag**.\n",
    "   - If no → move to step 3.\n",
    "\n",
    "3. **If cannot fix: Should I impute or drop?**\n",
    "\n",
    "   - **Predictors** → impute if valuable, drop if not.\n",
    "   - **Target (`count`)** → drop rows (never impute for modeling).\n",
    "   - Always **flag** the treatment.\n",
    "\n",
    "In this section, we’ll look at three kinds of outliers in our dataset: **impossible values, extreme-but-possible values, and flagged unusual demand days.**\n",
    "\n",
    "**Impossible Values (Clear Errors)**\n",
    "\n",
    "Some values are not just unusual — they are **physically impossible**. For example:\n",
    "\n",
    "* Negative rentals (`count < 0`).\n",
    "* Humidity above 100%.\n",
    "* Temperatures above 100°C in Washington, D.C.\n",
    "\n",
    "These values can never represent real-world events, so they are **always errors**. Our workflow says: *if error, fix if possible; if not, set to missing (`NaN`) so it can be imputed or dropped later.*\n",
    "\n",
    "Now we apply rule-based fixes to impossible values and flag each intervention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc99f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Negative rentals (cannot exist)\n",
    "df[\"flag_negative_count\"] = df[\"count\"] < 0\n",
    "df.loc[df[\"flag_negative_count\"], \"count\"] = np.nan\n",
    "\n",
    "# Humidity outside [0,100]\n",
    "df[\"flag_humidity_invalid\"] = (df[\"humidity\"] < 0) | (df[\"humidity\"] > 100)\n",
    "df.loc[df[\"flag_humidity_invalid\"], \"humidity\"] = np.nan\n",
    "\n",
    "# Temperature above plausible range\n",
    "df[\"flag_temp_invalid\"] = df[\"temp\"] > 60  # conservative cutoff for Washington climate\n",
    "df.loc[df[\"flag_temp_invalid\"], \"temp\"] = np.nan\n",
    "\n",
    "print(\"Impossible values flagged and set to NaN where needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4977b912",
   "metadata": {},
   "source": [
    "**Consultant reflection:**\n",
    "Impossible values were found and corrected: negative rentals, humidity outside 0–100%, and extreme temperatures above 60°C. Each was set to `NaN` so that predictors can later be imputed and targets dropped if necessary. Flags preserve full transparency, letting us show clients exactly which rows were affected.\n",
    "\n",
    "**Extreme-but-Possible Values (Events or Errors)**\n",
    "\n",
    "Other values may look suspiciously large or small but could still be real. For example:\n",
    "\n",
    "- A sudden spike in rentals on a festival day.\n",
    "- A sharp drop in rentals during a blizzard.\n",
    "\n",
    "Our task is to distinguish **events** (valid, keep+flag) from **errors** (sensor glitches, logging bugs). We'll use Z-scores to flag candidates for further investigation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb03d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Z-scores for daily rentals\n",
    "daily_rentals = (\n",
    "    df[\"count\"]\n",
    "    .groupby(df.index.date)\n",
    "    .sum()\n",
    "    .rename(\"daily_count\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "mean = daily_rentals[\"daily_count\"].mean()\n",
    "std = daily_rentals[\"daily_count\"].std()\n",
    "daily_rentals[\"z_score\"] = (daily_rentals[\"daily_count\"] - mean) / std\n",
    "\n",
    "# Flag potential outliers (|Z| > 3)\n",
    "daily_rentals[\"flag_daily_outlier\"] = daily_rentals[\"z_score\"].abs() > 3\n",
    "outliers = daily_rentals[daily_rentals[\"flag_daily_outlier\"]]\n",
    "\n",
    "outliers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9446713c",
   "metadata": {},
   "source": [
    "The Z-score scan highlights several days where total rentals are far beyond normal levels. These could represent genuine events (such as public holidays or festivals) or errors (like duplicated logs). Our workflow requires further investigation before making a decision:\n",
    "\n",
    "- If the spike matches an event calendar → keep as **event**, but flag.\n",
    "- If no event explains it → treat as **error**, and decide whether to drop or impute.\n",
    "\n",
    "This step demonstrates the consultant’s value: not just deleting data, but connecting patterns to real-world context.\n",
    "\n",
    "Outlier treatment is not about “removing unusual values.” It’s about applying a **consistent, auditable decision process**:\n",
    "\n",
    "- **Impossible values** → always errors → set to missing, flag.\n",
    "- **Extreme-but-possible values** → could be events or errors.\n",
    "\n",
    "  - If event → keep, flag.\n",
    "  - If error → fix if rule-based, else impute or drop.\n",
    "\n",
    "This way, nothing disappears silently. Every change is recorded, every uncertainty is visible, and every decision can be defended to a client."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3e1af6",
   "metadata": {},
   "source": [
    "### 4.5. Professional Data Cleaning Checklist\n",
    "\n",
    "Here's your consultant-ready workflow for any data cleaning project:\n",
    "\n",
    "**Phase 1: Foundation**\n",
    "- [ ] Standardize timeline (collapse duplicates, enforce continuity)\n",
    "- [ ] Add flags for all structural changes\n",
    "\n",
    "**Phase 2: The Three-Step Decision Process**\n",
    "For every suspicious value:\n",
    "- [ ] **Step 1:** Event or error? (Context check)\n",
    "- [ ] **Step 2:** If error, can I fix with a rule? (Apply fix + flag)\n",
    "- [ ] **Step 3:** If unfixable, impute or drop? (Predictors vs. targets)\n",
    "\n",
    "**Phase 3: Documentation**\n",
    "- [ ] Flag every intervention with clear labels\n",
    "- [ ] Document aggregation policies and business logic\n",
    "- [ ] Quantify impact (\"15% of weather data imputed\")\n",
    "- [ ] Prepare client communication on data limitations\n",
    "\n",
    "**Quality Gates:**\n",
    "- ✅ No impossible values remain\n",
    "- ✅ Timeline is continuous and complete\n",
    "- ✅ Every change is flagged and traceable\n",
    "- ✅ Client can understand what was done and why\n",
    "\n",
    "This checklist ensures your cleaning is not just thorough, but **defendable to clients and auditable by colleagues**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71078c9a",
   "metadata": {},
   "source": [
    "## Summary and Transition to Feature Engineering Implementation\n",
    "\n",
    "Your mastery of data quality assessment and cleaning establishes the reliability foundation essential for professional transportation consulting. Understanding missing data patterns, outlier detection techniques, and systematic cleaning workflows provides the data integrity infrastructure necessary for sophisticated business applications.\n",
    "\n",
    "The data quality expertise you've developed transforms messy real-world datasets into analytical-grade information. Your ability to assess data completeness, handle missing values systematically, and validate cleaning procedures creates the trustworthy data foundation for all advanced analysis you'll perform as a transportation consultant.\n",
    "\n",
    "Professional data quality proficiency distinguishes competent consultants who combine technical rigor with business judgment. Your expertise enables engagement with complex transportation datasets while maintaining analytical integrity essential for generating reliable business insights and strategic recommendations.\n",
    "\n",
    "Your next challenge involves implementing advanced feature engineering and preprocessing techniques that transform clean data into model-ready formats. The feature engineering implementation will demonstrate how data quality foundations translate to working solutions that prepare transportation data for sophisticated predictive modeling.\n",
    "\n",
    "The integration of data quality mastery with feature engineering practices creates comprehensive data preparation capability essential for professional transportation consulting success. Your data reliability foundation combined with systematic feature development expertise enables sophisticated business applications that drive strategic value creation and competitive advantage in urban mobility markets.\n",
    "\n",
    "The systematic approach to data quality assessment and cleaning that you've learned in this lecture transforms raw, messy data into reliable foundations for business analysis. In our next lecture, we'll build on this cleaned data foundation by learning advanced preprocessing and feature engineering techniques that will prepare your data for sophisticated machine learning models."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
