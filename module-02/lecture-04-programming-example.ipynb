{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a538cdbd",
   "metadata": {},
   "source": [
    "# Lecture 4: Programming Example - Data Quality Assessment and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5d41e8",
   "metadata": {},
   "source": [
    "## Introduction: Cleaning Real Transportation Data\n",
    "\n",
    "Welcome back, junior data consultant! Your client was impressed with your initial work and now they need you to ensure their Washington D.C. bike-sharing dataset is ready for business predictions. Today, you'll learn to be a data detective - identifying problems, applying fixes, and validating your work.\n",
    "\n",
    "Think of data cleaning like being a mechanic inspecting a car before a long journey. You need to check every component, fix what's broken, and ensure everything runs smoothly. Your client is counting on clean, reliable data for their million-dollar bike expansion decisions.\n",
    "\n",
    "> **üöÄ Interactive Learning Alert**\n",
    "> \n",
    "> This is a hands-on data cleaning tutorial with detective work and problem-solving. For the best experience:\n",
    ">\n",
    "> - **Click \"Open in Colab\"** at the bottom to run code interactively\n",
    "> - **Execute each code cell** by pressing **Shift + Enter**\n",
    "> - **Complete the challenges** to practice your data cleaning skills\n",
    "> - **Think like a consultant** - every decision impacts client trust\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91745224",
   "metadata": {},
   "source": [
    "## Step 1: Loading Your Dataset and Getting the First Look\n",
    "\n",
    "Just like a mechanic needs the right tools before inspecting a car, you need to set up your data cleaning environment and load your client's data. Let's start by importing essential libraries and loading the bike-sharing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18182279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas - your primary data manipulation tool\n",
    "import pandas as pd\n",
    "# Import numpy - for mathematical operations and handling special values\n",
    "import numpy as np\n",
    "\n",
    "# Load the Washington D.C. bike-sharing dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset-teaching-lec-04.csv\")\n",
    "\n",
    "# Get your first look at what you're working with\n",
    "print(\"Dataset Overview:\")\n",
    "print(f\"Shape: {df.shape[0]} rows √ó {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6d1527",
   "metadata": {},
   "source": [
    "**What this does:**\n",
    "- **Libraries**: `pandas` (as `pd`) is your Swiss Army knife for data manipulation, `numpy` (as `np`) helps with mathematical operations and missing data handling\n",
    "- **Dataset loading**: Brings your client's bike-sharing data into your workspace for analysis\n",
    "- **Shape overview**: Shows how many hours of data you have (rows) and how many variables you need to check (columns)\n",
    "\n",
    "More data means more reliable insights, but also more potential quality issues to find and fix.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652c021b",
   "metadata": {},
   "source": [
    "### Challenge 1: Get the Complete Data Health Report\n",
    "Your client wants to know: \"What exactly is in this dataset?\" Use `df.info()` to create a comprehensive health report showing data types and completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f355e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and load data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset-teaching-lec-04.csv\")\n",
    "\n",
    "# Your code here - generate the complete data health report\n",
    "print(\"Dataset Health Report:\")\n",
    "print(_____.info())  # Fill in the DataFrame name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2074d563",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° <strong>Tip</strong> (click to expand)</summary>\n",
    "\n",
    "\n",
    "The `df.info()` method is like getting a complete medical checkup for your data. Look for:\n",
    "- **Non-null counts**: Should match total rows if data is complete\n",
    "- **Data types**: Numbers should be `int64` or `float64`, not `object`\n",
    "- **Memory usage**: Helps you know if your computer can handle the data\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c84c0df",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ü§´ <strong>Solution</strong> (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "# Import libraries and load data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset-teaching-lec-04.csv\")\n",
    "\n",
    "# Your code here - generate the complete data health report\n",
    "print(\"Dataset Health Report:\")\n",
    "print(df.info())  # Fill in the DataFrame name\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bc7c82",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3258a54",
   "metadata": {},
   "source": [
    "## Step 2: Checking for Out-of-Range Values\n",
    "\n",
    "Think of this like a car inspection - you need to know if the readings make sense. A thermometer can't read 200¬∞C in Washington D.C., and you can't have negative bike rentals! We'll define reasonable ranges that represent **physical limits** (temperature and humidity can't exceed natural boundaries), **business limits** (bike stations have capacity constraints), and **sensor limits** (equipment has measurement ranges). Values outside these ranges might indicate broken sensors, data entry errors, or extraordinary events that need investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7606942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's see what our data actually looks like\n",
    "print(\"Current data ranges:\")\n",
    "print(df[['temp', 'humidity', 'windspeed', 'count']].describe())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Define what \"normal\" looks like for Washington D.C. bike data\n",
    "reasonable_ranges = {\n",
    "    'temp': (-20, 45),      # Temperature in Celsius (D.C. winter to summer)\n",
    "    'atemp': (-20, 50),     # How temperature feels to humans\n",
    "    'humidity': (0, 100),   # Humidity percentage (0% = desert, 100% = fog)\n",
    "    'windspeed': (0, 50),   # Wind speed km/h (50+ is severe storm)\n",
    "    'count': (0, 1000),     # Total bike rentals per hour\n",
    "    'casual': (0, 500),     # Casual user rentals per hour\n",
    "    'registered': (0, 900)   # Registered user rentals per hour\n",
    "}\n",
    "\n",
    "print(\"Reasonable Range Definitions:\")\n",
    "for variable, (min_val, max_val) in reasonable_ranges.items():\n",
    "    print(f\"{variable}: {min_val} to {max_val}\")\n",
    "\n",
    "# Now check if any actual values fall outside our reasonable ranges\n",
    "print(\"\\nRange Validation Results:\")\n",
    "for column, (min_val, max_val) in reasonable_ranges.items():\n",
    "    if column in df.columns:\n",
    "        # Get the actual data range\n",
    "        actual_min = df[column].min()\n",
    "        actual_max = df[column].max()\n",
    "        \n",
    "        # Count violations (values outside reasonable range)\n",
    "        below_range = (df[column] < min_val).sum()\n",
    "        above_range = (df[column] > max_val).sum()\n",
    "        \n",
    "        print(f\"\\n{column}:\")\n",
    "        print(f\"  Expected: {min_val} to {max_val}\")\n",
    "        print(f\"  Actual: {actual_min:.2f} to {actual_max:.2f}\")\n",
    "        \n",
    "        # Report any problems found\n",
    "        if below_range > 0:\n",
    "            print(f\"  ‚ö†Ô∏è WARNING: {below_range} values too low\")\n",
    "        if above_range > 0:\n",
    "            print(f\"  ‚ö†Ô∏è WARNING: {above_range} values too high\")\n",
    "        if below_range == 0 and above_range == 0:\n",
    "            print(f\"  ‚úÖ All values look reasonable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a6985b",
   "metadata": {},
   "source": [
    "**What this does:**\n",
    "- **Validates reality**: Data should match what's physically possible\n",
    "- **Identifies errors**: Impossible values suggest data collection problems\n",
    "- **Builds trust**: Your client knows the data has been thoroughly checked\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ac0291",
   "metadata": {},
   "source": [
    "### Challenge 2: Investigate Extreme Values\n",
    "Your client asks: \"Were those really busy hours normal, or were they data errors?\" Find the top 5 highest bike rental hours and examine the conditions to see if they make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b83b3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and load data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset-teaching-lec-04.csv\")\n",
    "\n",
    "# Your code here - find the top 5 busiest hours\n",
    "top_rentals = _____.nlargest(_____, _____)  # Fill in DataFrame, number, and column\n",
    "print(\"Top 5 busiest bike rental hours:\")\n",
    "print(top_rentals[['datetime', 'temp', 'humidity', 'weather', 'count']])\n",
    "\n",
    "# Make datetime more readable\n",
    "top_rentals_copy = top_rentals.copy()\n",
    "top_rentals_copy['datetime'] = pd.to_datetime(top_rentals_copy['datetime'])\n",
    "print(\"\\nWith readable dates:\")\n",
    "print(top_rentals_copy[['datetime', 'temp', 'weather', 'count']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133be2c6",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° <strong>Tip</strong> (click to expand)</summary>\n",
    "\n",
    "\n",
    "Use `df.nlargest(n, 'column')` to efficiently find the top N values in any column. Think of it as asking \"show me the highest scores on the test.\" Look at the weather and temperature during these busy times - do they explain why so many people rented bikes?\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07641638",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ü§´ <strong>Solution</strong> (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "# Import libraries and load data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset-teaching-lec-04.csv\")\n",
    "\n",
    "# Your code here - find the top 5 busiest hours\n",
    "top_rentals = df.nlargest(5, 'count')  # Fill in DataFrame, number, and column\n",
    "print(\"Top 5 busiest bike rental hours:\")\n",
    "print(top_rentals[['datetime', 'temp', 'humidity', 'weather', 'count']])\n",
    "\n",
    "# Make datetime more readable\n",
    "top_rentals_copy = top_rentals.copy()\n",
    "top_rentals_copy['datetime'] = pd.to_datetime(top_rentals_copy['datetime'])\n",
    "print(\"\\nWith readable dates:\")\n",
    "print(top_rentals_copy[['datetime', 'temp', 'weather', 'count']])\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ce1cae",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea658222",
   "metadata": {},
   "source": [
    "## Step 3: Outlier Detection and Treatment - Finding the Unusual\n",
    "\n",
    "Now, let's identify outliers - data points that are unusually high or low compared to the rest. Think of outliers like finding a person who is 8 feet tall in a crowd - they're not necessarily wrong, but they're different enough to deserve special attention.\n",
    "\n",
    "In this example, we will use the IQR (Interquartile Range) method to detect outliers. The IQR method is like creating a \"normal zone\" for your data. Here's how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64a0908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect outliers using IQR method with detailed explanation\n",
    "def detect_outliers_iqr(series, multiplier=1.5):\n",
    "    \"\"\"\n",
    "    Detect outliers using the Interquartile Range (IQR) method.\n",
    "    \n",
    "    The IQR method works by:\n",
    "    1. Finding Q1 (25th percentile) - 25% of data is below this value\n",
    "    2. Finding Q3 (75th percentile) - 75% of data is below this value  \n",
    "    3. Calculating IQR = Q3 - Q1 (the middle 50% range)\n",
    "    4. Setting boundaries at Q1 - 1.5*IQR and Q3 + 1.5*IQR\n",
    "    \n",
    "    Data points outside these boundaries are considered outliers.\n",
    "    \"\"\"\n",
    "    Q1 = series.quantile(0.25)  # 25th percentile\n",
    "    Q3 = series.quantile(0.75)  # 75th percentile\n",
    "    IQR = Q3 - Q1               # Interquartile range (middle 50%)\n",
    "    \n",
    "    # Calculate outlier boundaries\n",
    "    lower_bound = Q1 - multiplier * IQR\n",
    "    upper_bound = Q3 + multiplier * IQR\n",
    "    \n",
    "    # Identify outliers (values outside boundaries)\n",
    "    outliers = (series < lower_bound) | (series > upper_bound)\n",
    "    \n",
    "    print(f\"IQR Analysis for {series.name}:\")\n",
    "    print(f\"  Q1 (25th percentile): {Q1:.2f}\")\n",
    "    print(f\"  Q3 (75th percentile): {Q3:.2f}\")\n",
    "    print(f\"  IQR (Q3 - Q1): {IQR:.2f}\")\n",
    "    print(f\"  Lower boundary: {lower_bound:.2f}\")\n",
    "    print(f\"  Upper boundary: {upper_bound:.2f}\")\n",
    "    \n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# Detect outliers in bike count\n",
    "print(\"Analyzing bike rental outliers using IQR method:\")\n",
    "outliers, lower, upper = detect_outliers_iqr(df['count'])\n",
    "print(f\"  Number of outliers: {outliers.sum()}\")\n",
    "print(f\"  Percentage of data: {outliers.sum()/len(df)*100:.1f}%\")\n",
    "\n",
    "# Examine extreme outliers\n",
    "if outliers.sum() > 0:\n",
    "    extreme_outliers = df[outliers].nlargest(5, 'count')\n",
    "    print(\"\\nTop 5 outlier periods:\")\n",
    "    print(extreme_outliers[['datetime', 'temp', 'weather', 'workingday', 'count']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa075ba5",
   "metadata": {},
   "source": [
    "> **Note**: Why IQR method is robust:\n",
    "> - **Not affected by extreme values**: Unlike mean/standard deviation, IQR uses percentiles\n",
    "> - **Works with skewed data**: Doesn't assume normal distribution\n",
    "> - **Interpretable boundaries**: Clear definition of \"normal\" vs \"unusual\"\n",
    "\n",
    "When you run this code, you'll see the number of outliers detected in the bike count data using the IQR method. Rather than automatically removing outliers, we should investigate them first because they might indicate:\n",
    "\n",
    "- Special events (festivals, parades)\n",
    "- Perfect weather conditions  \n",
    "- System promotions or changes\n",
    "- Data collection errors\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9855ca6",
   "metadata": {},
   "source": [
    "### Challenge 3: Alternative Outlier Detection with Z-Score Method\n",
    "Your client asks: \"Can we use a different statistical approach to validate our outlier findings?\" Implement the Z-score method and compare results with the IQR approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f169e45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and load data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset-teaching-lec-04.csv\")\n",
    "\n",
    "# Your code here - implement Z-score outlier detection\n",
    "def detect_outliers_zscore(series, threshold=3):\n",
    "    \"\"\"\n",
    "    Detect outliers using the Z-score method.\n",
    "    \n",
    "    Z-score measures how many standard deviations a value is from the mean.\n",
    "    Values with |Z-score| > threshold are considered outliers.\n",
    "    Common threshold: 3 (99.7% of normal data falls within ¬±3 standard deviations)\n",
    "    \"\"\"\n",
    "    mean = _____._____()  # Fill in series and method\n",
    "    std = _____._____()   # Fill in series and method\n",
    "    \n",
    "    # Calculate Z-scores\n",
    "    z_scores = (_____ - _____) / _____  # Fill in: (series - mean) / std\n",
    "    \n",
    "    # Identify outliers\n",
    "    outliers = np.abs(_____) > _____  # Fill in z_scores and threshold\n",
    "    \n",
    "    print(f\"Z-score Analysis for {series.name}:\")\n",
    "    print(f\"  Mean: {_____:.2f}\")  # Fill in variable\n",
    "    print(f\"  Standard deviation: {_____:.2f}\")  # Fill in variable\n",
    "    print(f\"  Threshold: ¬±{_____}\")  # Fill in threshold\n",
    "    \n",
    "    return outliers, z_scores\n",
    "\n",
    "# Apply Z-score method to bike count\n",
    "print(\"Analyzing bike rental outliers using Z-score method:\")\n",
    "zscore_outliers, z_scores = detect_outliers_zscore(df['_____'])  # Fill in column name\n",
    "print(f\"  Number of outliers: {zscore_outliers.sum()}\")\n",
    "print(f\"  Percentage of data: {zscore_outliers.sum()/len(df)*100:.1f}%\")\n",
    "\n",
    "# Compare with IQR method\n",
    "iqr_outliers, _, _ = detect_outliers_iqr(df['_____'])  # Fill in column name\n",
    "print(f\"\\nMethod Comparison:\")\n",
    "print(f\"  IQR outliers: {iqr_outliers.sum()}\")\n",
    "print(f\"  Z-score outliers: {zscore_outliers.sum()}\")\n",
    "\n",
    "# Find overlapping outliers\n",
    "overlap = (iqr_outliers & zscore_outliers).sum()\n",
    "print(f\"  Overlapping outliers: {overlap}\")\n",
    "print(f\"  Agreement rate: {overlap/max(iqr_outliers.sum(), zscore_outliers.sum())*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5842ac",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° <strong>Tip</strong> (click to expand)</summary>\n",
    "\n",
    "\n",
    "The Z-score method assumes your data follows a normal distribution. Use `np.abs()` to get absolute values (distance from mean regardless of direction). Compare the two methods: Z-score is more sensitive to extreme values, while IQR is more robust. Look for patterns in which outliers each method identifies!\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f7d257",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ü§´ <strong>Solution</strong> (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "# Import libraries and load data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset-teaching-lec-04.csv\")\n",
    "\n",
    "# Your code here - implement Z-score outlier detection\n",
    "def detect_outliers_zscore(series, threshold=3):\n",
    "    \"\"\"\n",
    "    Detect outliers using the Z-score method.\n",
    "    \n",
    "    Z-score measures how many standard deviations a value is from the mean.\n",
    "    Values with |Z-score| > threshold are considered outliers.\n",
    "    Common threshold: 3 (99.7% of normal data falls within ¬±3 standard deviations)\n",
    "    \"\"\"\n",
    "    mean = series.mean()  # Fill in series and method\n",
    "    std = series.std()   # Fill in series and method\n",
    "    \n",
    "    # Calculate Z-scores\n",
    "    z_scores = (series - mean) / std  # Fill in: (series - mean) / std\n",
    "    \n",
    "    # Identify outliers\n",
    "    outliers = np.abs(z_scores) > threshold  # Fill in z_scores and threshold\n",
    "    \n",
    "    print(f\"Z-score Analysis for {series.name}:\")\n",
    "    print(f\"  Mean: {mean:.2f}\")  # Fill in variable\n",
    "    print(f\"  Standard deviation: {std:.2f}\")  # Fill in variable\n",
    "    print(f\"  Threshold: ¬±{threshold}\")  # Fill in threshold\n",
    "    \n",
    "    return outliers, z_scores\n",
    "\n",
    "# Apply Z-score method to bike count\n",
    "print(\"Analyzing bike rental outliers using Z-score method:\")\n",
    "zscore_outliers, z_scores = detect_outliers_zscore(df['count'])  # Fill in column name\n",
    "print(f\"  Number of outliers: {zscore_outliers.sum()}\")\n",
    "print(f\"  Percentage of data: {zscore_outliers.sum()/len(df)*100:.1f}%\")\n",
    "\n",
    "# Compare with IQR method\n",
    "iqr_outliers, _, _ = detect_outliers_iqr(df['count'])  # Fill in column name\n",
    "print(f\"\\nMethod Comparison:\")\n",
    "print(f\"  IQR outliers: {iqr_outliers.sum()}\")\n",
    "print(f\"  Z-score outliers: {zscore_outliers.sum()}\")\n",
    "\n",
    "# Find overlapping outliers\n",
    "overlap = (iqr_outliers & zscore_outliers).sum()\n",
    "print(f\"  Overlapping outliers: {overlap}\")\n",
    "print(f\"  Agreement rate: {overlap/max(iqr_outliers.sum(), zscore_outliers.sum())*100:.1f}%\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd6763e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00135d1a",
   "metadata": {},
   "source": [
    "## Step 4: Creating a Missing Data Report Card\n",
    "\n",
    "Missing data is like having pieces of a puzzle scattered around. Before you can solve the puzzle (make predictions), you need to know which pieces are missing and how serious the problem is. Let's create a comprehensive missing data report card:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cc5894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values (like counting empty seats in a theater)\n",
    "missing_counts = df.isnull().sum()\n",
    "print(\"Missing Data Count:\")\n",
    "print(missing_counts)\n",
    "\n",
    "# Calculate percentages (like getting a grade out of 100%)\n",
    "missing_percentages = (missing_counts / len(df)) * 100\n",
    "\n",
    "# Create a professional missing data report\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Missing_Count': missing_counts,\n",
    "    'Missing_Percentage': missing_percentages\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\nMissing Data Report Card:\")\n",
    "print(missing_summary[missing_summary['Missing_Count'] > 0])\n",
    "\n",
    "# Identify the \"A+ students\" - columns with perfect attendance\n",
    "complete_columns = missing_summary[missing_summary['Missing_Count'] == 0].index.tolist()\n",
    "print(f\"\\nPerfect attendance (no missing data): {len(complete_columns)} columns\")\n",
    "for col in complete_columns:\n",
    "    print(f\"  ‚úì {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c706e2",
   "metadata": {},
   "source": [
    "**What `.isnull()` does:**\n",
    "- Creates a True/False map of your data\n",
    "- `True` means \"this cell is empty\"\n",
    "- `.sum()` counts all the `True` values (empty cells)\n",
    "\n",
    "**What this reveals:**\n",
    "- **Missing counts**: How many empty cells each variable has\n",
    "- **Missing percentages**: How reliable each variable is (0% = completely reliable)\n",
    "- **Complete columns**: Variables you can trust for all calculations\n",
    "\n",
    "Variables with 0% missing data are gold - you can use them confidently. Variables with missing data need special handling before making business decisions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239f46b9",
   "metadata": {},
   "source": [
    "### Challenge 4: Investigate Missing Data Patterns\n",
    "Be a data detective! Your client wants to know: \"Are the missing windspeed readings random, or is there a pattern?\" Examine missing windspeed data to see when it occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778c07fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and load data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset-teaching-lec-04.csv\")\n",
    "\n",
    "# Ensure proper datetime handling for pattern analysis\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df = df.sort_values('datetime').reset_index(drop=True)\n",
    "\n",
    "# Your code here - investigate missing windspeed patterns\n",
    "if 'windspeed' in _____.columns:  # Fill in DataFrame name\n",
    "    missing_windspeed = _____[_____['windspeed'].isnull()]  # Fill in DataFrame names\n",
    "    if len(missing_windspeed) > 0:\n",
    "        print(\"First 10 periods with missing windspeed:\")\n",
    "        print(missing_windspeed[['datetime', 'temp', 'humidity', 'windspeed']].head(_____))  # Fill in number\n",
    "        print(\"\\nLast 10 periods with missing windspeed:\")\n",
    "        print(missing_windspeed[['datetime', 'temp', 'humidity', 'windspeed']].tail(_____))  # Fill in number\n",
    "    else:\n",
    "        print(\"Great news! No missing windspeed data found\")\n",
    "else:\n",
    "    print(\"Windspeed column not found in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501b3630",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° <strong>Tip</strong> (click to expand)</summary>\n",
    "\n",
    "\n",
    "When investigating missing data patterns, think like a detective:\n",
    "- Use `df[df['column'].isnull()]` to filter rows where data is missing\n",
    "- Look at timestamps: are missing values clustered in time periods?\n",
    "- Check other variables: do missing values happen during specific weather conditions?\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929db4a4",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ü§´ <strong>Solution</strong> (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "# Import libraries and load data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset-teaching-lec-04.csv\")\n",
    "\n",
    "# Ensure proper datetime handling for pattern analysis\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df = df.sort_values('datetime').reset_index(drop=True)\n",
    "\n",
    "# Your code here - investigate missing windspeed patterns\n",
    "if 'windspeed' in df.columns:  # Fill in DataFrame name\n",
    "    missing_windspeed = df[df['windspeed'].isnull()]  # Fill in DataFrame names\n",
    "    if len(missing_windspeed) > 0:\n",
    "        print(\"First 10 periods with missing windspeed:\")\n",
    "        print(missing_windspeed[['datetime', 'temp', 'humidity', 'windspeed']].head(10))  # Fill in number\n",
    "        print(\"\\nLast 10 periods with missing windspeed:\")\n",
    "        print(missing_windspeed[['datetime', 'temp', 'humidity', 'windspeed']].tail(10))  # Fill in number\n",
    "    else:\n",
    "        print(\"Great news! No missing windspeed data found\")\n",
    "else:\n",
    "    print(\"Windspeed column not found in dataset\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184a5e5a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653c0af3",
   "metadata": {},
   "source": [
    "## Step 5: Standardizing the Timeline - Fixing Duplicate Hours\n",
    "\n",
    "Before we can trust our time-series data, we need to ensure each hour appears exactly once. Think of this like organizing a photo album - if you have multiple copies of the same photo, you need to decide which one to keep or how to combine them. We'll first identify duplicates, then collapse them using smart aggregation rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b07b38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# First, let's check how many duplicate timestamps we have\n",
    "rows_per_ts = df.groupby(\"datetime\").size().rename(\"n_rows_per_ts\")\n",
    "duplicated_ts = rows_per_ts[rows_per_ts > 1].index\n",
    "n_dup_timestamps = len(duplicated_ts)\n",
    "n_dup_rows = int((rows_per_ts[rows_per_ts > 1] - 1).sum())\n",
    "\n",
    "print(f\"Found {n_dup_timestamps} timestamps with duplicates\")\n",
    "print(f\"Total extra duplicate rows to collapse: {n_dup_rows}\")\n",
    "\n",
    "# Show an example of duplicated data\n",
    "if n_dup_timestamps > 0:\n",
    "    sample_duplicate = duplicated_ts[0]\n",
    "    print(f\"\\nExample - rows for {sample_duplicate}:\")\n",
    "    print(df[df[\"datetime\"] == sample_duplicate][[\"datetime\", \"count\", \"temp\", \"weather\"]].head())\n",
    "\n",
    "# Build our aggregation policy - different rules for different types of data\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "agg_map = {c: \"mean\" for c in numeric_cols}\n",
    "\n",
    "# For bike counts (targets), we SUM because rentals accumulate\n",
    "for c in [\"count\", \"casual\", \"registered\"]:\n",
    "    if c in agg_map:\n",
    "        agg_map[c] = \"sum\"\n",
    "\n",
    "# For categorical variables, we take the FIRST value\n",
    "for c in [\"holiday\", \"weather\", \"season\"]:\n",
    "    if c in df.columns:\n",
    "        agg_map[c] = \"first\"\n",
    "\n",
    "print(\"\\nAggregation policy:\")\n",
    "print(\"- Bike counts (count, casual, registered): SUM\")\n",
    "print(\"- Weather variables (temp, humidity, etc.): MEAN\") \n",
    "print(\"- Categories (holiday, weather, season): FIRST\")\n",
    "\n",
    "# Handle duplicates based on what we found\n",
    "if n_dup_timestamps == 0:\n",
    "    print(\"\\n‚úÖ No duplicate timestamps found - data is already clean!\")\n",
    "    df_clean = df.copy()\n",
    "else:\n",
    "    # Collapse the duplicates\n",
    "    print(f\"\\nCollapsing {n_dup_timestamps} duplicate timestamps...\")\n",
    "    df_clean = (\n",
    "        df.groupby(\"datetime\", as_index=False)\n",
    "          .agg(agg_map)\n",
    "          .sort_values(\"datetime\")\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    # Add a flag to track which hours were collapsed\n",
    "    df_clean[\"flag_collapsed_from_duplicates\"] = df_clean[\"datetime\"].isin(duplicated_ts)\n",
    "    \n",
    "    print(f\"‚úÖ Collapse complete!\")\n",
    "    print(f\"Hours that were collapsed: {df_clean['flag_collapsed_from_duplicates'].sum()}\")\n",
    "    print(f\"Total rows now: {len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ca66a1",
   "metadata": {},
   "source": [
    "**What this does:**\n",
    "We successfully identify and collapse duplicate timestamps using intelligent aggregation rules. Bike rental counts are summed (because rentals accumulate), weather variables are averaged (representing hourly conditions), and categorical variables keep their first values (maintaining consistency). We create a clean dataset (`df_clean`) and add a transparency flag to track which hours were affected by this process.\n",
    "\n",
    "**Why this matters:**\n",
    "This standardization is critical for reliable time-series analysis. Duplicate rows would inflate rental counts and create false demand patterns, potentially leading to poor business decisions. Now each hour has exactly one record with properly combined data, creating a trustworthy foundation for forecasting models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44b479e",
   "metadata": {},
   "source": [
    "### Challenge 5: Complete Timeline - Filling the Missing Hours\n",
    "Your client asks: \"Are we missing any hours in our dataset? Can we have a complete timeline?\" Create a continuous hourly timeline by inserting rows for missing hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f25846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and load data (and apply Step 5 cleaning first)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load and prepare the dataset with collapsed duplicates\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset-teaching-lec-04.csv\")\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "# First collapse duplicates (from Step 5)\n",
    "rows_per_ts = df.groupby(\"datetime\").size()\n",
    "duplicated_ts = rows_per_ts[rows_per_ts > 1].index\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "agg_map = {c: \"mean\" for c in numeric_cols}\n",
    "for c in [\"count\", \"casual\", \"registered\"]:\n",
    "    if c in agg_map:\n",
    "        agg_map[c] = \"sum\"\n",
    "for c in [\"holiday\", \"weather\", \"season\"]:\n",
    "    if c in df.columns:\n",
    "        agg_map[c] = \"first\"\n",
    "\n",
    "df = df.groupby(\"datetime\", as_index=False).agg(agg_map).sort_values(\"datetime\").reset_index(drop=True)\n",
    "\n",
    "# Your challenge: Create a complete hourly timeline\n",
    "time_min = _____[\"datetime\"].min()  # Fill in DataFrame name\n",
    "time_max = _____[\"datetime\"].max()  # Fill in DataFrame name\n",
    "\n",
    "# Build the full hourly index from min to max\n",
    "full_hours = pd.date_range(_____, _____, freq=\"H\")  # Fill in time_min and time_max\n",
    "\n",
    "print(f\"Original hours: {len(df)}\")\n",
    "print(f\"Expected hours: {len(full_hours)}\")\n",
    "print(f\"Missing hours: {len(full_hours) - len(df)}\")\n",
    "\n",
    "# Your code here - reindex to the full timeline and add a flag\n",
    "original_hours = pd.Index(df[\"datetime\"])\n",
    "df = df.set_index(\"datetime\").reindex(_____)  # Fill in the full_hours variable\n",
    "df.index.name = \"datetime\"\n",
    "df = df.sort_index()  # Ensure chronological order\n",
    "\n",
    "# Add flag for inserted missing hours\n",
    "df[\"flag_missing_timestamp\"] = ~df.index.isin(_____)  # Fill in original_hours variable\n",
    "\n",
    "# Check results\n",
    "inserted_hours = int(df[\"flag_missing_timestamp\"].sum())\n",
    "print(f\"‚úÖ Timeline completed!\")\n",
    "print(f\"Total hours now: {len(df)}\")\n",
    "print(f\"Inserted missing hours: {inserted_hours}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca4e653",
   "metadata": {},
   "source": [
    "**What you should achieve:**\n",
    "- A complete hourly timeline from first to last timestamp\n",
    "- Identification of how many hours were missing\n",
    "- A flag marking which rows were inserted vs. original data\n",
    "\n",
    "**Why this matters:**\n",
    "A continuous timeline is essential for time-series analysis. Missing hours can break seasonal patterns and forecasting models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dc35c5",
   "metadata": {},
   "source": [
    "## Step 6: Forward Fill for Known Missing Data\n",
    "\n",
    "From our data quality assessment, we identified missing values in windspeed and potentially other variables. Before applying complex imputation strategies, let's start with forward fill for variables that change slowly over time.\n",
    "\n",
    "This method assumes that if a value is missing, the most recent valid observation is our best estimate. For example, if temperature at 2 PM is missing but we recorded 25¬∞C at 1 PM, forward fill uses 25¬∞C for the 2 PM gap. This works because many weather variables have **temporal continuity** - they don't jump dramatically from hour to hour.\n",
    "\n",
    "The beauty of this method is that it's simple, preserves actual measurements (rather than creating artificial averages), and works well for variables with natural persistence. Think of it as saying \"conditions probably stayed similar to the last known measurement\" rather than guessing with statistical averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8c061f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule #1: Never modify original data! Always work on a copy\n",
    "df_clean = df.copy()\n",
    "print(\"Created a backup copy of original data\")\n",
    "print(\"Now we can safely apply fixes without losing the original\")\n",
    "\n",
    "# Check which variables have missing data (from our previous analysis)\n",
    "missing_summary = df_clean.isnull().sum()\n",
    "print(\"\\nVariables with missing data:\")\n",
    "for col in missing_summary[missing_summary > 0].index:\n",
    "    count = missing_summary[col]\n",
    "    percentage = (count / len(df_clean)) * 100\n",
    "    print(f\"  {col}: {count} missing ({percentage:.1f}%)\")\n",
    "\n",
    "# Apply forward fill to slowly-changing weather variables\n",
    "weather_vars_for_ffill = ['atemp', 'humidity', 'temp']\n",
    "filled_count = 0\n",
    "\n",
    "for var in weather_vars_for_ffill:\n",
    "    if var in df_clean.columns and df_clean[var].isnull().sum() > 0:\n",
    "        missing_before = df_clean[var].isnull().sum()\n",
    "        df_clean[var] = df_clean[var].fillna(method='ffill')\n",
    "        missing_after = df_clean[var].isnull().sum()\n",
    "        filled = missing_before - missing_after\n",
    "        filled_count += filled\n",
    "        print(f\"‚úÖ Forward-filled {filled} missing values in {var}\")\n",
    "\n",
    "if filled_count == 0:\n",
    "    print(\"No weather variables needed forward fill\")\n",
    "else:\n",
    "    print(f\"\\nTotal values filled with forward fill: {filled_count}\")\n",
    "\n",
    "# Show remaining missing data after forward fill\n",
    "remaining_missing = df_clean.isnull().sum()\n",
    "print(\"\\nMissing data remaining after forward fill:\")\n",
    "if remaining_missing.sum() == 0:\n",
    "    print(\"üéâ All missing data has been handled!\")\n",
    "else:\n",
    "    for col in remaining_missing[remaining_missing > 0].index:\n",
    "        print(f\"  {col}: {remaining_missing[col]} still missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee10098",
   "metadata": {},
   "source": [
    "**When forward fill works well:**\n",
    "- **Temperature variables**: Change gradually over hours\n",
    "- **Atmospheric pressure**: Varies slowly throughout the day\n",
    "- **Humidity**: Generally stable unless weather system changes\n",
    "\n",
    "**When NOT to use forward fill:**\n",
    "- **Windspeed**: Can change rapidly with weather fronts\n",
    "- **Bike counts**: Demand varies dramatically by hour\n",
    "- **Precipitation**: Rain can start/stop suddenly\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aad1e2",
   "metadata": {},
   "source": [
    "### Challenge 6: Smart Grouping for Weather-Related Variables\n",
    "\n",
    "Your client asks: \"Can we handle the remaining missing windspeed data more intelligently?\" Implement season-aware weather-based imputation for windspeed since wind patterns vary significantly between weather conditions AND seasons (winter storms vs summer breezes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456a2647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and load data (building on Step 6)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load and apply our Step 6 cleaning first\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset-teaching-lec-04.csv\")\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Apply forward fill to slowly-changing variables (from Step 6)\n",
    "weather_vars_for_ffill = ['atemp', 'humidity', 'temp']\n",
    "for var in weather_vars_for_ffill:\n",
    "    if var in df_clean.columns and df_clean[var].isnull().sum() > 0:\n",
    "        df_clean[var] = df_clean[var].fillna(method='ffill')\n",
    "\n",
    "# Your code here - implement smart grouping for windspeed\n",
    "if 'windspeed' in df_clean.columns and df_clean['windspeed'].isnull().sum() > 0:\n",
    "    print(\"Found missing windspeed data - implementing season-aware weather-based imputation\")\n",
    "    \n",
    "    # Step 1: Calculate typical windspeed for each season-weather combination\n",
    "    seasonal_weather_windspeed = df_clean.groupby(['_____', '_____'])['windspeed']._____()  # Fill in groupby columns and aggregation method\n",
    "    print(\"Typical windspeed by season and weather condition:\")\n",
    "    print(seasonal_weather_windspeed)\n",
    "    \n",
    "    # Step 2: Fill missing values based on season-weather combinations\n",
    "    missing_filled = 0\n",
    "    for season in df_clean['season'].unique():\n",
    "        for weather_code in df_clean['weather'].unique():\n",
    "            # Find rows with this season-weather combination AND missing windspeed\n",
    "            mask = (df_clean['season'] == season) & (df_clean['_____'] == weather_code) & (df_clean['windspeed']._____())  # Fill in column and missing check\n",
    "            if mask.sum() > 0:\n",
    "                # Get the fill value for this season-weather combination\n",
    "                if (season, weather_code) in seasonal_weather_windspeed.index:\n",
    "                    fill_value = seasonal_weather_windspeed[(season, weather_code)]\n",
    "                    # Check if the median value is valid (not NaN)\n",
    "                    if pd.isna(fill_value):\n",
    "                        # Fallback to season-only median\n",
    "                        fill_value = df_clean[df_clean['season'] == season]['windspeed'].median()\n",
    "                else:\n",
    "                    # Fallback to season-only median if combination doesn't exist\n",
    "                    fill_value = df_clean[df_clean['season'] == season]['windspeed'].median()\n",
    "                \n",
    "                # Apply the fill value\n",
    "                df_clean.loc[mask, 'windspeed'] = fill_value\n",
    "                missing_filled += mask.sum()\n",
    "    \n",
    "    print(f\"‚úÖ Filled {missing_filled} missing windspeed values using season-aware weather logic\")\n",
    "    \n",
    "    # Step 3: Validate our smart imputation\n",
    "    print(\"\\nValidation - windspeed distribution by season and weather:\")\n",
    "    validation = df_clean.groupby(['season', 'weather'])['windspeed'].agg(['count', 'mean', 'std']).round(2)\n",
    "    print(_____)  # Fill in variable name\n",
    "    \n",
    "    # Step 4: Additional seasonal validation\n",
    "    print(\"\\nSeasonal windspeed patterns:\")\n",
    "    seasonal_summary = df_clean.groupby('season')['windspeed'].agg(['mean', 'std']).round(3)\n",
    "    print(seasonal_summary)\n",
    "else:\n",
    "    print(\"No missing windspeed data found\")\n",
    "\n",
    "# Final check - did we handle all missing data?\n",
    "remaining_missing = df_clean.isnull().sum()\n",
    "print(\"\\nFinal missing data status:\")\n",
    "if remaining_missing.sum() == 0:\n",
    "    print(\"üéâ All missing data has been successfully handled!\")\n",
    "else:\n",
    "    print(\"Variables still with missing data:\")\n",
    "    for col in remaining_missing[remaining_missing > 0].index:\n",
    "        print(f\"  {col}: {remaining_missing[col]} missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eee9131",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° <strong>Tip</strong> (click to expand)</summary>\n",
    "\n",
    "\n",
    "Season-aware weather imputation is more meteorologically accurate because wind patterns depend on both weather conditions AND seasonal factors. Use `groupby(['season', 'weather'])['windspeed'].median()` to calculate typical windspeed for each season-weather combination. Winter storms typically have higher winds than summer storms, while spring/fall weather systems show different patterns. The median remains more robust than mean for this type of imputation. Validate that your results show realistic seasonal progression: winter (season=1) and fall (season=4) generally have higher wind speeds than summer (season=3).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5d5fc7",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ü§´ <strong>Solution</strong> (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "# Import libraries and load data (building on Step 6)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load and apply our Step 6 cleaning first\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset-teaching-lec-04.csv\")\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Apply forward fill to slowly-changing variables (from Step 6)\n",
    "weather_vars_for_ffill = ['atemp', 'humidity', 'temp']\n",
    "for var in weather_vars_for_ffill:\n",
    "    if var in df_clean.columns and df_clean[var].isnull().sum() > 0:\n",
    "        df_clean[var] = df_clean[var].fillna(method='ffill')\n",
    "\n",
    "# Your code here - implement smart grouping for windspeed\n",
    "if 'windspeed' in df_clean.columns and df_clean['windspeed'].isnull().sum() > 0:\n",
    "    print(\"Found missing windspeed data - implementing season-aware weather-based imputation\")\n",
    "    \n",
    "    # Step 1: Calculate typical windspeed for each season-weather combination\n",
    "    seasonal_weather_windspeed = df_clean.groupby(['season', 'weather'])['windspeed'].median()  # Fill in groupby columns and aggregation method\n",
    "    print(\"Typical windspeed by season and weather condition:\")\n",
    "    print(seasonal_weather_windspeed)\n",
    "    \n",
    "    # Step 2: Fill missing values based on season-weather combinations\n",
    "    missing_filled = 0\n",
    "    for season in df_clean['season'].unique():\n",
    "        for weather_code in df_clean['weather'].unique():\n",
    "            # Find rows with this season-weather combination AND missing windspeed\n",
    "            mask = (df_clean['season'] == season) & (df_clean['weather'] == weather_code) & (df_clean['windspeed'].isnull())  # Fill in column and missing check\n",
    "            if mask.sum() > 0:\n",
    "                # Get the fill value for this season-weather combination\n",
    "                if (season, weather_code) in seasonal_weather_windspeed.index:\n",
    "                    fill_value = seasonal_weather_windspeed[(season, weather_code)]\n",
    "                    # Check if the median value is valid (not NaN)\n",
    "                    if pd.isna(fill_value):\n",
    "                        # Fallback to season-only median\n",
    "                        fill_value = df_clean[df_clean['season'] == season]['windspeed'].median()\n",
    "                else:\n",
    "                    # Fallback to season-only median if combination doesn't exist\n",
    "                    fill_value = df_clean[df_clean['season'] == season]['windspeed'].median()\n",
    "                \n",
    "                # Apply the fill value\n",
    "                df_clean.loc[mask, 'windspeed'] = fill_value\n",
    "                missing_filled += mask.sum()\n",
    "    \n",
    "    print(f\"‚úÖ Filled {missing_filled} missing windspeed values using season-aware weather logic\")\n",
    "    \n",
    "    # Step 3: Validate our smart imputation\n",
    "    print(\"\\nValidation - windspeed distribution by season and weather:\")\n",
    "    validation = df_clean.groupby(['season', 'weather'])['windspeed'].agg(['count', 'mean', 'std']).round(2)\n",
    "    print(validation)  # Fill in variable name\n",
    "    \n",
    "    # Step 4: Additional seasonal validation\n",
    "    print(\"\\nSeasonal windspeed patterns:\")\n",
    "    seasonal_summary = df_clean.groupby('season')['windspeed'].agg(['mean', 'std']).round(3)\n",
    "    print(seasonal_summary)\n",
    "else:\n",
    "    print(\"No missing windspeed data found\")\n",
    "\n",
    "# Final check - did we handle all missing data?\n",
    "remaining_missing = df_clean.isnull().sum()\n",
    "print(\"\\nFinal missing data status:\")\n",
    "if remaining_missing.sum() == 0:\n",
    "    print(\"üéâ All missing data has been successfully handled!\")\n",
    "else:\n",
    "    print(\"Variables still with missing data:\")\n",
    "    for col in remaining_missing[remaining_missing > 0].index:\n",
    "        print(f\"  {col}: {remaining_missing[col]} missing\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee866cc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3685700e",
   "metadata": {},
   "source": [
    "## Summary: Professional Data Quality Assessment and Cleaning\n",
    "\n",
    "**What We've Accomplished**:\n",
    "- Implemented comprehensive data quality assessment protocols for real transportation datasets\n",
    "- Applied advanced statistical outlier detection methodologies using IQR and Z-score techniques\n",
    "- Developed timeline standardization procedures through intelligent duplicate handling and missing timestamp resolution\n",
    "- Executed sophisticated imputation strategies including forward-fill and season-aware weather-based approaches\n",
    "- Established data validation frameworks ensuring cleaning operations preserved essential dataset characteristics\n",
    "- Created systematic approaches to range validation and impossible value detection for business contexts\n",
    "\n",
    "**Key Technical Skills Mastered**:\n",
    "- Range validation and boundary checking for transportation data quality control\n",
    "- Statistical outlier identification and investigation using multiple detection methodologies\n",
    "- Time-series data standardization with intelligent aggregation rule implementation\n",
    "- Missing data pattern analysis and targeted imputation strategy development\n",
    "- Business logic integration for contextual data cleaning and validation protocols\n",
    "- Quality assurance verification through before-and-after statistical comparison frameworks\n",
    "\n",
    "**Next Steps**: Next, we'll advance to feature engineering and preprocessing techniques, transforming cleaned datasets into optimized variable structures and formats required for sophisticated machine learning model development and deployment in transportation demand forecasting applications.\n",
    "\n",
    "Your bike-sharing client now possesses thoroughly validated, production-ready datasets that demonstrate professional data cleaning methodologies and systematic quality control processes - the essential foundation that consulting firms require for reliable predictive modeling and strategic business intelligence initiatives."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
