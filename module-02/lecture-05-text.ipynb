{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d00a0ee",
   "metadata": {},
   "source": [
    "# Lecture 5: Advanced Preprocessing & Feature Engineering - Optimizing Data for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315bf85b",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this lecture, you will be able to:\n",
    "\n",
    "- Apply categorical encoding techniques for machine learning compatibility\n",
    "- Implement data scaling and normalization strategies for optimal model performance\n",
    "- Design and implement time-based features for transportation demand prediction\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18c57a6",
   "metadata": {},
   "source": [
    "## 1. From Clean Data to Machine Learning Ready Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3e97b9",
   "metadata": {},
   "source": [
    "### 1.1. The Bridge Between Data and Models\n",
    "\n",
    "You now have **clean, reliable data** from your bike-sharing client - but **clean data isn't the same as machine learning ready data**. Raw data variables often need **transformation, combination, and optimization** before machine learning algorithms can use them effectively.\n",
    "\n",
    "In the previous lecture, you learned to identify and handle data quality issues - missing values, outliers, and inconsistencies. That cleaning work ensures you're starting with reliable, trustworthy data. Now, we move to the next essential step: **transforming that clean data into optimized features for machine learning**.\n",
    "\n",
    "Think of this stage like preparing ingredients for a sophisticated recipe. Having fresh, quality ingredients (clean data) is essential, but you still need to chop, season, and combine them in specific ways to create the final dish (predictive model). This preparation involves two complementary activities:\n",
    "\n",
    "**Data preprocessing** adjusts existing variables to formats and scales that algorithms can process efficiently - ensuring numerical features exist on compatible scales so algorithm mechanics don't introduce artificial biases. **Feature engineering** creates new variables that capture patterns and relationships hidden in raw data - transforming simple measurements into sophisticated representations that expose the underlying structure driving predictions.\n",
    "\n",
    "The boundaries between **data cleaning**, **preprocessing**, and **feature engineering** are not always clear-cut. In practice, these steps often overlap, and the exact labels matter less than the outcome: **data that is trustworthy, ready for algorithms, and enriched with predictive signal**.\n",
    "\n",
    "In this lecture, we will focus on three essential transformation techniques:\n",
    "\n",
    "1. **Categorical encoding**, which converts qualitative categories into numerical representations,\n",
    "2. **Numerical scaling and normalization**, which adjust numerical values either to standardized distributions or to a fixed range,\n",
    "3. **Temporal feature creation**, which extracts meaningful time-based patterns such as seasonality, trends, or hour-of-day effects.\n",
    "\n",
    "These methods form the backbone of most machine learning workflows, especially in time-sensitive transportation contexts like bike-sharing demand. While many other transformation strategies exist in the broader ML toolkit, mastering these three gives you a strong foundation for preparing real-world datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c5368f",
   "metadata": {},
   "source": [
    "### 1.2. Understanding Data Preprocessing: Making Data Algorithm-Ready\n",
    "\n",
    "Machine learning algorithms operate on numerical matrices through mathematical operations like multiplication, addition, and distance calculations. However, transportation data arrives in formats that algorithms cannot process directly - text labels like \"Rainy\" or \"Clear,\" and numerical values spanning wildly different scales. Data preprocessing transforms this raw data into algorithm-compatible formats while preserving the information needed for accurate predictions.\n",
    "\n",
    "Transportation datasets present specific preprocessing challenges that require careful handling:\n",
    "\n",
    "- **Mixed Data Types**: Transportation systems generate both categorical variables (weather conditions, day types, station locations) and continuous numerical measurements (temperature, humidity, bike counts). Each type requires different preprocessing strategies to become machine learning compatible.\n",
    "- **Categorical Relationships**: Not all categories are equal. Weather severity has natural ordering (Clear < Misty < Light Rain < Heavy Rain), while seasons are cyclical labels without inherent hierarchy. Choosing the wrong encoding strategy can introduce false relationships or miss important orderings.\n",
    "- **Scale Disparities**: Raw features exist on incompatible numerical scales - temperature ranges from -10°C to 40°C, humidity spans 0-100%, while hourly bike rentals can vary from 1 to 1000+. Without normalization, algorithms incorrectly prioritize features with larger numerical ranges over potentially more predictive features with smaller scales.\n",
    "\n",
    "Professional preprocessing ensures that algorithm limitations don't distort the transportation patterns you're trying to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35d60f7",
   "metadata": {},
   "source": [
    "### 1.3. Understanding Time-Based Feature Engineering in Transportation\n",
    "\n",
    "Once data is preprocessed into algorithm-compatible formats, feature engineering creates new variables that capture hidden patterns and relationships in the raw data. While preprocessing ensures algorithms can process your data, feature engineering determines what insights they can extract from it. For transportation demand prediction, time is one of the most critical dimensions requiring sophisticated feature engineering.\n",
    "\n",
    "Transportation demand exhibits temporal complexities that raw timestamps cannot capture directly:\n",
    "\n",
    "- **Cyclical Time Patterns**: Transportation operates on repeating cycles - hourly rush patterns, daily commute rhythms, weekly work schedules, and seasonal vacation periods. Raw timestamps treat these cycles as linear sequences, failing to recognize that 11 PM and midnight are adjacent hours, or that December and January are consecutive months. Machine learning algorithms need explicit encoding to understand temporal continuity.\n",
    "- **Multi-Scale Temporal Dependencies**: Demand at any moment depends on patterns at multiple time scales simultaneously. Current hour demand relates to yesterday's same hour (daily cycle), last week's same hour (weekly cycle), and recent hours (momentum). Simple timestamp features cannot represent these layered temporal relationships that drive transportation behavior.\n",
    "- **Temporal Context and Transitions**: The meaning of any time point depends on its position within larger temporal structures. Friday evening demand differs fundamentally from Monday evening demand despite identical clock times. Similarly, demand evolves systematically as the work week progresses, requiring features that capture position within weekly and seasonal cycles.\n",
    "- **Sequential Momentum Effects**: Transportation demand exhibits inertia - high demand periods tend to persist, and transitions between demand states follow predictable patterns. Raw timestamps provide no information about recent demand history or emerging trends that influence near-future predictions.\n",
    "\n",
    "Professional time-based feature engineering transforms simple timestamps into sophisticated temporal features that capture these cyclical, multi-scale, contextual, and sequential patterns essential for accurate transportation demand forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc2ded2",
   "metadata": {},
   "source": [
    "## 2. Categorical Encoding Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6d2041",
   "metadata": {},
   "source": [
    "### 2.1. Understanding Categorical Variables in Transportation\n",
    "\n",
    "We will start by exploring why categorical variables matter in transportation datasets and how to transform them into useful numerical representations.\n",
    "\n",
    "Most machine learning algorithms rely on numerical operations such as addition, multiplication, and comparison. But transportation data often contains text-based categories like *“Rainy”* or *“Clear.”* Algorithms cannot process these directly — we need to **convert them into numbers** while preserving their meaning for prediction.\n",
    "\n",
    "The challenge lies in doing this transformation in a way that **respects the type of category** and the relationships it carries. Not all categorical variables behave the same way: some have natural orderings, while others are just labels.\n",
    "\n",
    "**Examples of Categorical Variables in Bike-Sharing:**\n",
    "\n",
    "* **Weather Conditions**: Clear, misty, light rain, heavy rain\n",
    "* **Day Types**: Weekday, weekend, holiday\n",
    "* **Seasons**: Spring, summer, fall, winter\n",
    "* **Time Periods**: Rush hour, off-peak, late night\n",
    "* **Events**: Normal, special event, maintenance period\n",
    "\n",
    "Each requires a different encoding strategy depending on whether categories are *unordered labels*, *ranked scales*, or *business-specific conditions*.\n",
    "\n",
    "Here we introduce three widely used approaches to categorical encoding: one-hot encoding, ordinal encoding, and binary encoding. Each method fits different types of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a410eed6",
   "metadata": {},
   "source": [
    "### 2.2. One-Hot Encoding for Nominal Categories**\n",
    "\n",
    "**Definition**: One-hot encoding creates binary columns for each category, with values 1 (present) or 0 (absent).\n",
    "\n",
    "**When to use**: For categories with **no inherent order** where all options should be treated equally.\n",
    "\n",
    "**Python Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d248d0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original weather column (as categories):\n",
      "weather_condition\n",
      "Clear         7192\n",
      "Misty         2834\n",
      "Light Rain     859\n",
      "Heavy Rain       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "One-hot encoded columns:\n",
      "   weather_Clear  weather_Heavy Rain  weather_Light Rain  weather_Misty\n",
      "0           True               False               False          False\n",
      "1           True               False               False          False\n",
      "2           True               False               False          False\n",
      "3           True               False               False          False\n",
      "4           True               False               False          False\n",
      "\n",
      "Original 1 column → 4 binary columns\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = \"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\"\n",
    "df = pd.read_csv(data_path, parse_dates=['datetime'])\n",
    "\n",
    "# Ensure 'weather_condition' exists; derive it from numeric 'weather' if needed\n",
    "if 'weather_condition' not in df.columns:\n",
    "    if 'weather' in df.columns:\n",
    "        _weather_map = {1: 'Clear', 2: 'Misty', 3: 'Light Rain', 4: 'Heavy Rain'}\n",
    "        df['weather_condition'] = df['weather'].map(_weather_map).astype('category')\n",
    "    else:\n",
    "        raise KeyError(\"Neither 'weather_condition' nor 'weather' columns are present in df.\")\n",
    "\n",
    "print(\"Original weather column (as categories):\")\n",
    "print(df['weather_condition'].value_counts())\n",
    "\n",
    "# One-hot encode weather conditions\n",
    "weather_encoded = pd.get_dummies(df['weather_condition'], prefix='weather')\n",
    "print(\"\\nOne-hot encoded columns:\")\n",
    "print(weather_encoded.head())\n",
    "print(f\"\\nOriginal 1 column → {len(weather_encoded.columns)} binary columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ef3571",
   "metadata": {},
   "source": [
    "This transformation creates separate binary columns for each weather condition. Where we previously had one column with text values ('Clear', 'Cloudy', 'Light Rain', 'Heavy Rain'), we now have four binary columns (weather_Clear, weather_Cloudy, weather_Light Rain, weather_Heavy Rain) that machine learning algorithms can process directly.\n",
    "\n",
    "Each row has exactly one '1' (`True`) and the rest '0s' (`False`), preserving the original information in a numerical format. For bike-sharing demand prediction, this allows the model to learn different demand patterns for each weather type - for example, Clear days might show high recreational usage while Light Rain days might see reduced casual ridership but maintained commuter patterns.\n",
    "\n",
    "The key advantage: \"Clear\" and \"Cloudy\" are treated as equally valid categories without implying any ordering or hierarchy between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8dc71e",
   "metadata": {},
   "source": [
    "### 2.3. Ordinal Encoding for Ordered Categories\n",
    "\n",
    "**Definition**: Ordinal encoding maps categories to numbers that reflect their natural order.\n",
    "\n",
    "**When to use**: For categories where **order matters**, such as severity or ranking.\n",
    "\n",
    "**Python Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d068b29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather severity column (as ordered numbers):\n",
      "weather_severity\n",
      "1    7192\n",
      "2    2834\n",
      "3     859\n",
      "4       1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = \"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\"\n",
    "df = pd.read_csv(data_path, parse_dates=['datetime'])\n",
    "\n",
    "# Ensure 'weather_condition' exists; derive it from numeric 'weather' if needed\n",
    "if 'weather_condition' not in df.columns:\n",
    "    if 'weather' in df.columns:\n",
    "        _weather_map = {1: 'Clear', 2: 'Misty', 3: 'Light Rain', 4: 'Heavy Rain'}\n",
    "        df['weather_condition'] = df['weather'].map(_weather_map).astype('category')\n",
    "    else:\n",
    "        raise KeyError(\"Neither 'weather_condition' nor 'weather' columns are present in df.\")\n",
    "\n",
    "# Encoding weather severity by order\n",
    "weather_severity_map = {'Clear': 1, 'Misty': 2, 'Light Rain': 3, 'Heavy Rain': 4}\n",
    "df['weather_severity'] = df['weather_condition'].map(weather_severity_map)\n",
    "print(\"Weather severity column (as ordered numbers):\")\n",
    "print(df['weather_severity'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6b5a58",
   "metadata": {},
   "source": [
    "This transformation converts text weather conditions into ordered numerical values that preserve their severity relationship. Where we previously had one column with text values ('Clear', 'Misty', 'Light Rain', 'Heavy Rain'), we now have a single numerical column with ordered values (1, 2, 3, 4) that machine learning algorithms can process directly while understanding the inherent ordering.\n",
    "\n",
    "Each weather condition maps to a specific number representing its severity level: Clear conditions (most favorable, severity 1) appear 7,192 times in the dataset, Misty conditions (severity 2) appear 2,834 times, Light Rain (severity 3) occurs 859 times, and Heavy Rain (most severe, severity 4) is rare with only 1 occurrence. For bike-sharing demand prediction, this allows the model to understand that deteriorating weather conditions progressively reduce ridership - a unit increase in weather severity (e.g., from Clear to Misty) represents a consistent step toward worse conditions.\n",
    "\n",
    "The key advantage: the model learns that Heavy Rain > Light Rain > Misty > Clear in terms of severity, capturing the natural ordering that affects transportation behavior, unlike one-hot encoding which would treat these conditions as unrelated categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087ba423",
   "metadata": {},
   "source": [
    "### 2.4. Binary Encoding for Business-Specific Conditions\n",
    "\n",
    "**Definition**: Creates simple 0/1 features for key conditions.\n",
    "\n",
    "**When to use**: When certain conditions are **especially important for the business**.\n",
    "\n",
    "Examples in bike-sharing:\n",
    "\n",
    "* `is_holiday`: 1 if holiday, else 0\n",
    "* `is_weekend`: 1 if Saturday/Sunday, else 0\n",
    "* `is_rush_hour`: 1 if within rush hour, else 0\n",
    "* `is_good_weather`: 1 if clear or mild conditions, else 0\n",
    "\n",
    "**Python Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b0863cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample binary business-condition features (first 10 rows):\n",
      "           datetime  is_holiday  is_weekend  is_rush_hour  is_good_weather\n",
      "2011-01-01 00:00:00           0           1             0                1\n",
      "2011-01-01 01:00:00           0           1             0                1\n",
      "2011-01-01 02:00:00           0           1             0                1\n",
      "2011-01-01 03:00:00           0           1             0                1\n",
      "2011-01-01 04:00:00           0           1             0                1\n",
      "2011-01-01 05:00:00           0           1             0                1\n",
      "2011-01-01 06:00:00           0           1             0                1\n",
      "2011-01-01 07:00:00           0           1             1                1\n",
      "2011-01-01 08:00:00           0           1             1                1\n",
      "2011-01-01 09:00:00           0           1             1                1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_path = \"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\"\n",
    "df = pd.read_csv(data_path, parse_dates=['datetime'])\n",
    "\n",
    "# Binary indicators for business-relevant conditions\n",
    "df['is_holiday'] = (df['holiday'] == 1).astype(int)\n",
    "\n",
    "# Weekend: Saturday=5, Sunday=6\n",
    "df['is_weekend'] = df['datetime'].dt.dayofweek.isin([5, 6]).astype(int)\n",
    "\n",
    "# Rush hour windows (commuting peaks)\n",
    "rush_hours = [7, 8, 9, 17, 18, 19]\n",
    "df['is_rush_hour'] = df['datetime'].dt.hour.isin(rush_hours).astype(int)\n",
    "\n",
    "# Good weather: Clear/Misty considered favorable; support either numeric or text weather columns\n",
    "if 'weather' in df.columns:\n",
    "    df['is_good_weather'] = df['weather'].isin([1, 2]).astype(int)\n",
    "elif 'weather_condition' in df.columns:\n",
    "    df['is_good_weather'] = df['weather_condition'].isin(['Clear', 'Misty']).astype(int)\n",
    "else:\n",
    "    df['is_good_weather'] = 0  # fallback if weather info is unavailable\n",
    "\n",
    "print(\"\\nSample binary business-condition features (first 10 rows):\")\n",
    "print(df[['datetime', 'is_holiday', 'is_weekend', 'is_rush_hour', 'is_good_weather']].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25871ffd",
   "metadata": {},
   "source": [
    "This transformation creates focused binary indicators that flag business-critical conditions. Where we previously needed to check multiple columns and apply business logic (e.g., \"Is this Saturday or Sunday?\" or \"Does the hour fall in 7-9 or 17-19?\"), we now have simple 0/1 flags (is_weekend, is_rush_hour, is_good_weather, is_holiday) that machine learning algorithms can process directly.\n",
    "\n",
    "Examining January 1, 2011 (the dataset start): this Saturday is correctly flagged as weekend (is_weekend=1) throughout the day, not a holiday (is_holiday=0), and shows good weather conditions (is_good_weather=1). The rush hour indicator activates appropriately at 7 AM, 8 AM, and 9 AM (is_rush_hour=1), even on this weekend day - capturing that some commuters work weekends and morning activity patterns persist. For bike-sharing demand prediction, this allows the model to learn distinct patterns for each condition combination - for example, weekend mornings with good weather drive recreational usage, while weekday rush hours with poor weather concentrate demand around transit stations.\n",
    "\n",
    "The key advantage: binary flags make complex business rules immediately accessible to the model without requiring it to rediscover these domain-specific condition definitions, allowing faster learning of operationally relevant patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17927771",
   "metadata": {},
   "source": [
    "## 3. Scaling and Normalization for Optimal Performance\n",
    "\n",
    "You may have sophisticated engineered features from your bike-sharing data - but having great features isn't enough if they can't work together effectively. Raw feature values often exist on completely different scales, creating a hidden problem that can sabotage your machine learning models.\n",
    "\n",
    "Consider the magnitude differences in your Washington D.C. bike-sharing dataset:\n",
    "\n",
    "- **Temperature**: -10 to 40°C (range of 50 units)\n",
    "- **Humidity**: 0 to 100% (range of 100 units)\n",
    "- **Bike demand**: 1 to 1000+ rentals per hour (range of 1000+ units)\n",
    "- **Hour of day**: 0 to 23 (range of 24 units)\n",
    "\n",
    "Without proper scaling, machine learning algorithms naturally give more weight to features with larger numerical ranges. In this case, bike count values in the hundreds will overshadow temperature changes in the tens - even when temperature might be more predictive of future demand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930a4165",
   "metadata": {},
   "source": [
    "### 3.1. Scaling and Normalization Fundamentals\n",
    "\n",
    "Before exploring specific techniques, let's establish the main definitions:\n",
    "\n",
    "- **Normalization** reshapes distributions to fit standardized ranges or statistical properties  \n",
    "- **Scaling** adjusts feature value ranges to enable fair comparison across variables\n",
    "\n",
    "A key principle here is that **these transformations preserve underlying data patterns while standardizing numerical representation**.\n",
    "\n",
    "Professional scaling ensures features contribute based on *predictive importance* rather than arbitrary numerical scale, keeping your models aligned with real transportation dynamics. Consider a Tuesday 3 PM demand prediction request from your client. Without proper scaling, your model might overweight historical bike counts (values in the hundreds) while underweighting a forecasted 15-degree temperature drop (numerically smaller change). The result: inflated demand predictions and misallocated bike fleet resources.\n",
    "\n",
    "Now that you know *why* scaling matters, let's explore the two core methods every transportation consultant should master: StandardScaler and MinMaxScaler. Each one transforms the data differently and is best suited for particular feature types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca94a33",
   "metadata": {},
   "source": [
    "### 3.2. StandardScaler: Statistical Normalization\n",
    "\n",
    "**Definition:** StandardScaler (Z-score normalization) standardizes features so they have mean 0 and standard deviation 1. This preserves the distribution shape but makes features comparable on a common statistical scale.\n",
    "\n",
    "**Formula:**\n",
    "$$scaled\\_value = \\frac{original\\_value - mean}{standard\\_deviation}$$\n",
    "\n",
    "**Python Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1aeb1c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original values (first 5 rows):\n",
      "   temp   atemp  humidity  windspeed  casual  registered\n",
      "0  9.84  14.395        81        0.0       3          13\n",
      "1  9.02  13.635        80        0.0       8          32\n",
      "2  9.02  13.635        80        0.0       5          27\n",
      "3  9.84  14.395        75        0.0       3          10\n",
      "4  9.84  14.395        75        0.0       0           1\n",
      "\n",
      "Scaled values (first 5 rows):\n",
      "       temp     atemp  humidity  windspeed    casual  registered\n",
      "0 -1.333661 -1.092737  0.993213  -1.567754 -0.660992   -0.943854\n",
      "1 -1.438907 -1.182421  0.941249  -1.567754 -0.560908   -0.818052\n",
      "2 -1.438907 -1.182421  0.941249  -1.567754 -0.620958   -0.851158\n",
      "3 -1.333661 -1.092737  0.681430  -1.567754 -0.660992   -0.963717\n",
      "4 -1.333661 -1.092737  0.681430  -1.567754 -0.721042   -1.023307\n",
      "\n",
      "Scaled statistics:\n",
      "Mean: [-0.  0.  0. -0.  0. -0.]\n",
      "Std: [1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data_path = \"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\"\n",
    "df = pd.read_csv(data_path, parse_dates=['datetime'])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "numerical_columns = ['temp', 'atemp', 'humidity', 'windspeed', 'casual', 'registered']\n",
    "\n",
    "# Show before/after comparison\n",
    "print(\"Original values (first 5 rows):\")\n",
    "print(df[numerical_columns].head())\n",
    "\n",
    "df_scaled = scaler.fit_transform(df[numerical_columns])\n",
    "df_scaled_display = pd.DataFrame(df_scaled, columns=numerical_columns)\n",
    "\n",
    "print(\"\\nScaled values (first 5 rows):\")\n",
    "print(df_scaled_display.head())\n",
    "\n",
    "print(\"\\nScaled statistics:\")\n",
    "print(f\"Mean: {df_scaled.mean(axis=0).round(10)}\")  # Should be ~0\n",
    "print(f\"Std: {df_scaled.std(axis=0).round(3)}\")     # Should be ~1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360da4fc",
   "metadata": {},
   "source": [
    "Notice how StandardScaler transforms the features to have mean ≈ 0 and standard deviation = 1. Temperature values that ranged from 0-40°C now center around 0, with most values falling between -2 and +2. This puts temperature, humidity, and bike counts on the same statistical scale, ensuring the model treats them fairly rather than over-weighting variables with larger raw values.\n",
    "\n",
    "The transformation preserves relationships within each feature while making them directly comparable. For bike-sharing prediction, this ensures that a 10-point change in humidity has a similar numerical weight as a 10-degree change in temperature, allowing the model to learn which features are truly most predictive.\n",
    "\n",
    "**When to Use:**\n",
    "\n",
    "* Weather variables with near-normal distributions\n",
    "* Linear regression or neural networks\n",
    "* Situations without extreme outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e392475",
   "metadata": {},
   "source": [
    "### 3.3. MinMaxScaler: Bounded Range Normalization\n",
    "\n",
    "**Definition:** MinMaxScaler rescales values into a defined range, usually 0–1. This ensures all values fit within a predictable bound.\n",
    "\n",
    "**Formula:**\n",
    "$$scaled\\_value = \\frac{original\\_value - min}{max - min}$$\n",
    "\n",
    "**Python Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fa427c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original min/max (per column):\n",
      "             min       max\n",
      "temp        0.82   41.0000\n",
      "atemp       0.76   45.4550\n",
      "humidity    0.00  100.0000\n",
      "windspeed   0.00   56.9969\n",
      "casual      0.00  367.0000\n",
      "registered  0.00  886.0000\n",
      "\n",
      "Scaled min/max (should be ~0 and ~1):\n",
      "            min  max\n",
      "temp        0.0  1.0\n",
      "atemp       0.0  1.0\n",
      "humidity    0.0  1.0\n",
      "windspeed   0.0  1.0\n",
      "casual      0.0  1.0\n",
      "registered  0.0  1.0\n",
      "\n",
      "Inverse-transformed (first 5 rows) matches original scale:\n",
      "   temp   atemp  humidity  windspeed  casual  registered\n",
      "0  9.84  14.395      81.0        0.0     3.0        13.0\n",
      "1  9.02  13.635      80.0        0.0     8.0        32.0\n",
      "2  9.02  13.635      80.0        0.0     5.0        27.0\n",
      "3  9.84  14.395      75.0        0.0     3.0        10.0\n",
      "4  9.84  14.395      75.0        0.0     0.0         1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data_path = \"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\"\n",
    "df = pd.read_csv(data_path, parse_dates=['datetime'])\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "numerical_columns = ['temp', 'atemp', 'humidity', 'windspeed', 'casual', 'registered']\n",
    "\n",
    "# Show before\n",
    "print(\"Original min/max (per column):\")\n",
    "print(df[numerical_columns].agg(['min','max']).T)\n",
    "\n",
    "# Fit & transform\n",
    "mm_scaled = scaler.fit_transform(df[numerical_columns])\n",
    "mm_scaled_df = pd.DataFrame(mm_scaled, columns=numerical_columns)\n",
    "\n",
    "# Show after\n",
    "print(\"\\nScaled min/max (should be ~0 and ~1):\")\n",
    "print(mm_scaled_df.agg(['min','max']).round(4).T)\n",
    "\n",
    "# Optional: invert back to original scale (sanity check)\n",
    "mm_inverted = scaler.inverse_transform(mm_scaled_df.head())\n",
    "print(\"\\nInverse-transformed (first 5 rows) matches original scale:\")\n",
    "print(pd.DataFrame(mm_inverted, columns=numerical_columns).round(3).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c26853a",
   "metadata": {},
   "source": [
    "Notice how MinMaxScaler compresses all values into the 0–1 range while preserving the relative distances between data points. Temperature values that originally ranged from 0-40°C are now squeezed between 0 and 1, with the minimum temperature mapping to 0 and the maximum to 1. This bounded transformation is particularly useful when you need predictable value ranges.\n",
    "\n",
    "The transformation maintains the original distribution shape and outlier patterns, simply rescaling them to fit the target range. For bike-sharing prediction, this ensures that features with different natural scales—like temperature (0-40°C) and humidity (0-100%)—are all mapped to the same 0-1 range, preventing any single feature from dominating due to its larger numerical scale. The inverse_transform function allows you to convert predictions back to the original scale for interpretation.\n",
    "\n",
    "**When to Use:**\n",
    "\n",
    "* Time-based features (e.g., hour of day)\n",
    "* Variables with natural bounds (e.g., percentages)\n",
    "* Algorithms sensitive to bounded inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cbf3fe",
   "metadata": {},
   "source": [
    "## 4. Time-Based Feature Engineering for Transportation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a946e5b",
   "metadata": {},
   "source": [
    "### 4.1. Extracting Temporal Intelligence from Timestamps\n",
    "\n",
    "Time is the most important dimension in transportation data, but raw timestamps contain hidden patterns that must be extracted and transformed to be useful for machine learning.\n",
    "\n",
    "Think about how a clock works - after 11 PM comes midnight (12 AM), but to a computer, these look like completely different numbers (23 and 0). This creates a problem: the computer thinks 11 PM and midnight are far apart, when they're actually next to each other on the clock. Similarly, transportation demand operates simultaneously at multiple temporal scales - hourly rush patterns, weekly commute cycles, and seasonal variations - all requiring specialized feature engineering approaches.\n",
    "\n",
    "In this section, we explore four essential temporal feature engineering techniques that transform raw timestamps into predictive features for bike-sharing demand forecasting:\n",
    "\n",
    "1. **Cyclical encoding for continuous time**\n",
    "2. **Time-since features**\n",
    "3. **Temporal aggregation features**\n",
    "4. **Lag features for sequential patterns**\n",
    "\n",
    "Each technique captures different temporal patterns that drive transportation demand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9dd2d4",
   "metadata": {},
   "source": [
    "### 4.2. Cyclical Encoding for Continuous Time\n",
    "\n",
    "**Definition**: Cyclical encoding transforms linear time values (hours, days, months) into circular representations using sine and cosine functions, ensuring that adjacent time points remain close in the feature space.\n",
    "\n",
    "**Purpose**: This technique prevents artificial breaks in temporal data - hour 23 and hour 0 are neighbors on a clock, and models should treat them as such.\n",
    "\n",
    "**Python Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d0bba649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hour 23 encoding: sin=-0.259, cos=0.966\n",
      "Hour 0 encoding:  sin=0.000, cos=1.000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data_path = \"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\"\n",
    "df = pd.read_csv(data_path, parse_dates=['datetime'])\n",
    "\n",
    "# Extract hour from timestamp\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "\n",
    "# Create cyclical encoding for 24-hour cycle\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "\n",
    "# Compare hour 23 vs hour 0\n",
    "print(\"Hour 23 encoding:\", f\"sin={df[df['hour']==23]['hour_sin'].iloc[0]:.3f}, cos={df[df['hour']==23]['hour_cos'].iloc[0]:.3f}\")\n",
    "print(\"Hour 0 encoding: \", f\"sin={df[df['hour']==0]['hour_sin'].iloc[0]:.3f}, cos={df[df['hour']==0]['hour_cos'].iloc[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c05ffe3",
   "metadata": {},
   "source": [
    "The formula `2 * π * hour / 24` maps the 24-hour cycle onto a circle. We use both sine and cosine because together they uniquely identify any point on the circle. Hour 0 maps to (sin=0, cos=1), hour 6 to (sin=1, cos=0), hour 12 to (sin=0, cos=-1), and hour 18 to (sin=-1, cos=0). Critically, hour 23 maps to approximately (sin=-0.259, cos=0.966), very close to hour 0's position.\n",
    "\n",
    "For bike-sharing demand prediction, this encoding allows the model to learn that late-night hours (22-23) and early-morning hours (0-1) share similar low-demand patterns. Without cyclical encoding, a model would incorrectly assume hour 23 is as different from hour 0 as hour 0 is from hour 23, missing the continuous flow of nighttime demand patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfc0ba6",
   "metadata": {},
   "source": [
    "### 4.3. Time-Since Features\n",
    "\n",
    "**Definition**: Time-since features measure the elapsed time between the current observation and a meaningful reference point, such as the last weekend, holiday, or weather event.\n",
    "\n",
    "**Purpose**: These features capture recovery patterns and transition effects that influence transportation demand after significant events.\n",
    "\n",
    "**Python Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4702e6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average demand by days since weekend:\n",
      "days_since_weekend\n",
      "0    188.8\n",
      "1    190.4\n",
      "2    189.7\n",
      "3    188.4\n",
      "4    197.3\n",
      "5    197.8\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = \"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\"\n",
    "df = pd.read_csv(data_path, parse_dates=['datetime'])\n",
    "df = df.sort_values('datetime')\n",
    "\n",
    "# Calculate days since last weekend (Monday=0, Sunday=6)\n",
    "df['day_of_week'] = df['datetime'].dt.dayofweek\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Simple calculation: Monday=1, Tuesday=2, ..., Friday=5, Sat/Sun=0\n",
    "df['days_since_weekend'] = df['day_of_week'].apply(\n",
    "    lambda x: x + 1 if x < 5 else 0\n",
    ")\n",
    "\n",
    "# Show demand pattern by days since weekend\n",
    "print(\"\\nAverage demand by days since weekend:\")\n",
    "print(df.groupby('days_since_weekend')['count'].mean().round(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e8f7a5",
   "metadata": {},
   "source": [
    "We created a feature that tracks how many days have passed since the last weekend ended. Monday gets value 1, Tuesday gets 2, and so on through Friday (value 5), while weekends themselves get 0. This captures the weekly rhythm where demand patterns evolve as the work week progresses.\n",
    "\n",
    "Analysis reveals that bike-sharing demand builds throughout the work week. Monday through Wednesday (1-3 days since weekend) show relatively stable baseline demand around 189-190 bikes per hour. Demand increases notably on Thursday and Friday (4-5 days), reaching nearly 198 bikes per hour - about 5% higher than mid-week. This pattern suggests people may make more trips toward the end of the work week, possibly combining commutes with after-work activities or weekend preparation. Operators should ensure higher bike availability at stations on Thursday-Friday afternoons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1753717b",
   "metadata": {},
   "source": [
    "### 4.4. Temporal Aggregation Features\n",
    "\n",
    "**Definition**: Temporal aggregation features summarize past values of a variable over a defined time window, providing context about recent trends and stability.\n",
    "\n",
    "**Purpose**: They help models understand whether demand has been rising or falling, and whether conditions have been stable or volatile.\n",
    "\n",
    "**Python Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7c2080eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample temporal aggregation features:\n",
      "           datetime  count  demand_3h_avg  demand_24h_avg  demand_momentum\n",
      "2011-01-01 00:00:00     16      16.000000       16.000000         0.000000\n",
      "2011-01-01 01:00:00     40      28.000000       28.000000        12.000000\n",
      "2011-01-01 02:00:00     32      29.333333       29.333333         2.666667\n",
      "2011-01-01 03:00:00     13      28.333333       25.250000       -12.250000\n",
      "2011-01-01 04:00:00      1      15.333333       20.400000       -19.400000\n",
      "2011-01-01 05:00:00      1       5.000000       17.166667       -16.166667\n",
      "2011-01-01 06:00:00      2       1.333333       15.000000       -13.000000\n",
      "2011-01-01 07:00:00      3       2.000000       13.500000       -10.500000\n",
      "2011-01-01 08:00:00      8       4.333333       12.888889        -4.888889\n",
      "2011-01-01 09:00:00     14       8.333333       13.000000         1.000000\n",
      "2011-01-01 10:00:00     36      19.333333       15.090909        20.909091\n",
      "2011-01-01 11:00:00     56      35.333333       18.500000        37.500000\n",
      "2011-01-01 12:00:00     84      58.666667       23.538462        60.461538\n",
      "2011-01-01 13:00:00     94      78.000000       28.571429        65.428571\n",
      "2011-01-01 14:00:00    106      94.666667       33.733333        72.266667\n",
      "2011-01-01 15:00:00    110     103.333333       38.500000        71.500000\n",
      "2011-01-01 16:00:00     93     103.000000       41.705882        51.294118\n",
      "2011-01-01 17:00:00     67      90.000000       43.111111        23.888889\n",
      "2011-01-01 18:00:00     35      65.000000       42.684211        -7.684211\n",
      "2011-01-01 19:00:00     37      46.333333       42.400000        -5.400000\n",
      "2011-01-01 20:00:00     36      36.000000       42.095238        -6.095238\n",
      "2011-01-01 21:00:00     34      35.666667       41.727273        -7.727273\n",
      "2011-01-01 22:00:00     28      32.666667       41.130435       -13.130435\n",
      "2011-01-01 23:00:00     39      33.666667       41.041667        -2.041667\n",
      "2011-01-02 00:00:00     17      28.000000       41.083333       -24.083333\n",
      "2011-01-02 01:00:00     17      24.333333       40.125000       -23.125000\n",
      "2011-01-02 02:00:00      9      14.333333       39.166667       -30.166667\n",
      "2011-01-02 03:00:00      6      10.666667       38.875000       -32.875000\n",
      "2011-01-02 04:00:00      3       6.000000       38.958333       -35.958333\n",
      "2011-01-02 06:00:00      2       3.666667       39.000000       -37.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = \"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\"\n",
    "df = pd.read_csv(data_path, parse_dates=['datetime'])\n",
    "df = df.sort_values('datetime')\n",
    "\n",
    "# Create 3-hour rolling average demand\n",
    "df['demand_3h_avg'] = df['count'].rolling(window=3, min_periods=1).mean()\n",
    "\n",
    "# Create 24-hour rolling average demand\n",
    "df['demand_24h_avg'] = df['count'].rolling(window=24, min_periods=1).mean()\n",
    "\n",
    "# Compare current demand to 24-hour average (momentum indicator)\n",
    "df['demand_momentum'] = df['count'] - df['demand_24h_avg']\n",
    "\n",
    "# Show example\n",
    "print(\"\\nSample temporal aggregation features:\")\n",
    "print(df[['datetime', 'count', 'demand_3h_avg', 'demand_24h_avg', 'demand_momentum']].head(30).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bf104c",
   "metadata": {},
   "source": [
    "We created rolling window features that summarize recent demand history. The 3-hour average captures immediate trends - if demand has been building over the past few hours, this average will be rising. The 24-hour average provides daily context, smoothing out hourly fluctuations. The momentum indicator (current - 24h average) shows whether current demand is above or below typical levels for this time of day.\n",
    "\n",
    "These aggregation features help distinguish between sustained demand trends and temporary spikes. For example, if current demand is 200 bikes/hour but the 3-hour average is 150, this suggests demand is accelerating - perhaps weather improved or an event started. Conversely, if current demand is 100 but the 3-hour average is 150, demand is declining and bike rebalancing can be delayed. The 24-hour average is particularly valuable for detecting anomalies: when current demand deviates significantly from the 24-hour norm (momentum > ±50), it signals special conditions requiring operational attention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5c5298",
   "metadata": {},
   "source": [
    "### 4.5. Lag Features for Sequential Patterns\n",
    "\n",
    "**Definition**: Lag features use values from previous time steps as predictors for the current observation, explicitly introducing historical patterns into the model.\n",
    "\n",
    "**Purpose**: They capture the sequential dependencies and recurring cycles that characterize transportation demand.\n",
    "\n",
    "**Python Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bb183b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lag feature correlations with current demand:\n",
      "1-hour lag:  0.842\n",
      "24-hour lag: 0.811\n",
      "7-day lag:   0.786\n",
      "\n",
      "Saturday 7 AM demand patterns:\n",
      "           datetime  count  demand_lag_1h  demand_lag_24h  demand_lag_7d\n",
      "2011-01-01 07:00:00      3            2.0             NaN            NaN\n",
      "2011-01-08 07:00:00      9            2.0            84.0           16.0\n",
      "2011-01-15 07:00:00     10            3.0            70.0           16.0\n",
      "2011-02-05 07:00:00      4            4.0            87.0          113.0\n",
      "2011-02-12 07:00:00     11            2.0            74.0           11.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = \"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\"\n",
    "df = pd.read_csv(data_path, parse_dates=['datetime'])\n",
    "df = df.sort_values('datetime')\n",
    "\n",
    "# Create lag features at different time scales\n",
    "df['demand_lag_1h'] = df['count'].shift(1)      # 1 hour ago\n",
    "df['demand_lag_24h'] = df['count'].shift(24)    # Same hour yesterday\n",
    "df['demand_lag_7d'] = df['count'].shift(24 * 7) # Same hour last week\n",
    "\n",
    "# Calculate correlations to understand predictive power\n",
    "print(\"Lag feature correlations with current demand:\")\n",
    "print(f\"1-hour lag:  {df['count'].corr(df['demand_lag_1h']):.3f}\")\n",
    "print(f\"24-hour lag: {df['count'].corr(df['demand_lag_24h']):.3f}\")\n",
    "print(f\"7-day lag:   {df['count'].corr(df['demand_lag_7d']):.3f}\")\n",
    "\n",
    "# Show example for Saturday morning\n",
    "print(\"\\nSaturday 7 AM demand patterns:\")\n",
    "saturday_7am = df[(df['datetime'].dt.dayofweek == 5) & (df['datetime'].dt.hour == 7)]\n",
    "print(saturday_7am[['datetime', 'count', 'demand_lag_1h', 'demand_lag_24h', 'demand_lag_7d']].head(5).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cdee69",
   "metadata": {},
   "source": [
    "We created three lag features at different time scales. The 1-hour lag captures immediate momentum - high demand often persists for several hours. The 24-hour lag captures daily repetition - 8 AM today resembles 8 AM yesterday. The 7-day lag captures weekly cycles - Monday patterns repeat week after week. The `.shift()` function moves values backward in time, making historical demand available as features for prediction.\n",
    "\n",
    "The correlation analysis reveals that 1-hour lags (correlation ≈ 0.84) are the strongest predictors for bike-sharing demand, as demand in consecutive hours tends to be very similar. The 24-hour lag (correlation ≈ 0.81) is nearly as strong, reflecting the dominant daily cycle in urban transportation. The Saturday 7 AM example demonstrates the value of the 7-day lag (correlation ≈ 0.79): demand on one Saturday (9 bikes) is much closer to the previous Saturday (16 bikes) than to Friday 7 AM (84 bikes), showing that weekly patterns are crucial for weekend predictions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9828f2",
   "metadata": {},
   "source": [
    "## Summary and Transition to Exploratory Data Analysis\n",
    "\n",
    "You've mastered advanced preprocessing and feature engineering techniques: categorical encoding strategies, scaling methods, cyclical time encoding, and lag features. These skills transform clean transportation data into machine learning-ready inputs.\n",
    "\n",
    "Your ability to create new features from raw data prepares you to work with complex transportation prediction challenges while maintaining the data quality essential for accurate models.\n",
    "\n",
    "In the next module, you'll learn how to explore and visualize these engineered features to generate business insights and validate that your preprocessing pipeline creates data that reflects real-world transportation patterns."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
