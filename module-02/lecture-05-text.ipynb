{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28101be7",
   "metadata": {},
   "source": [
    "# Lecture 5: Advanced Preprocessing & Feature Engineering - Optimizing Data for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d03b13e",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this lecture, you will be able to:\n",
    "- Design and implement time-based features for transportation demand prediction\n",
    "- Apply categorical encoding techniques for machine learning compatibility\n",
    "- Implement data scaling and normalization strategies for optimal model performance\n",
    "- Create domain-specific features that capture transportation business logic\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84924b40",
   "metadata": {},
   "source": [
    "## 1. From Clean Data to Machine Learning Ready Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601a016b",
   "metadata": {},
   "source": [
    "### The Bridge Between Data and Models\n",
    "\n",
    "You now have clean, reliable data from your bike-sharing client - but clean data isn't the same as machine learning ready data. Raw data variables often need transformation, combination, and optimization before machine learning algorithms can use them effectively.\n",
    "\n",
    "In Lecture 4, you learned to identify and handle data quality issues - missing values, outliers, and inconsistencies. That cleaning work ensures you're starting with reliable, trustworthy data. Now, we move to the next essential step: transforming that clean data into optimized features for machine learning.\n",
    "\n",
    "Think of this stage like preparing ingredients for a sophisticated recipe. Having fresh, quality ingredients (clean data) is essential, but you still need to chop, season, and combine them in specific ways to create the final dish (predictive model). Feature engineering is this crucial preparation step that transforms your clean data into optimized inputs for machine learning algorithms.\n",
    "\n",
    "For transportation consulting, this means creating features that capture the complex temporal, spatial, and operational patterns that drive demand. Simple variables like \"temperature\" and \"hour\" become sophisticated features like \"temperature deviation from seasonal average\" and \"rush hour intensity\" that enable more accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11d7bc8",
   "metadata": {},
   "source": [
    "### Understanding Feature Engineering in Transportation Context\n",
    "\n",
    "Transportation systems exhibit complex patterns that require specialized feature engineering approaches:\n",
    "\n",
    "**Temporal Complexity**: Transportation demand follows nested temporal cycles (hourly, daily, weekly, seasonal) that interact in sophisticated ways. Rush hour patterns differ between weekdays and weekends, seasonal effects vary by time of day, and special events create temporary pattern disruptions.\n",
    "\n",
    "**Environmental Sensitivity**: Weather affects transportation differently depending on trip purpose, time of day, and seasonal context. A 25°C day feels warm in January but cool in July, creating different demand responses that simple temperature variables can't capture.\n",
    "\n",
    "**Network Effects**: Transportation systems are networks where demand at one location affects nearby locations through user behavior, capacity constraints, and operational interventions like bike rebalancing.\n",
    "\n",
    "**Operational Interdependencies**: User types (casual vs. registered) respond differently to environmental and temporal factors, requiring features that capture these interaction effects.\n",
    "\n",
    "Professional feature engineering transforms these complexities into variables that machine learning algorithms can effectively use to predict demand patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebed89ec",
   "metadata": {},
   "source": [
    "## 2. Time-Based Feature Engineering for Transportation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc1e878",
   "metadata": {},
   "source": [
    "### Extracting Temporal Intelligence from Timestamps\n",
    "\n",
    "Time is the most important dimension in transportation data, but raw timestamps contain hidden patterns that must be extracted and transformed to be useful for machine learning.\n",
    "\n",
    "**Cyclical Time Components**:\n",
    "Raw time values (like hour 23 followed by hour 0) create artificial discontinuities for machine learning algorithms. Professional time-based feature engineering transforms these linear representations into cyclical features that reflect the true cyclical nature of time.\n",
    "\n",
    "**Multi-Scale Temporal Patterns**:\n",
    "Transportation demand operates simultaneously at multiple temporal scales:\n",
    "- **Intraday cycles**: Rush hours, lunch periods, evening entertainment\n",
    "- **Weekly cycles**: Weekday vs. weekend patterns, Monday effects, Friday departures\n",
    "- **Seasonal cycles**: Weather-driven variations, holiday periods, academic calendars\n",
    "- **Annual cycles**: Long-term growth trends, infrastructure changes, demographic shifts\n",
    "\n",
    "**Temporal Interaction Effects**:\n",
    "Simple temporal features miss important interaction effects. For example, the \"hour\" effect differs dramatically between weekdays and weekends, requiring interaction features that capture these conditional relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59aa6782",
   "metadata": {},
   "source": [
    "### Advanced Temporal Feature Engineering Techniques\n",
    "\n",
    "In this section, we will explore advanced methods for creating **time-based features** that improve prediction models in urban mobility. Transportation demand is strongly influenced by **daily cycles, weekly rhythms, and seasonal patterns**, so carefully engineered temporal features often make the difference between a weak model and a powerful one.\n",
    "\n",
    "We will cover four main techniques:\n",
    "\n",
    "1. **Cyclical encoding for continuous time**\n",
    "2. **Time-since features**\n",
    "3. **Temporal aggregation features**\n",
    "4. **Lag features for sequential patterns**\n",
    "\n",
    "Each of these methods helps capture different aspects of how time influences bike-sharing demand.\n",
    "\n",
    "**1. Cyclical Encoding for Continuous Time**\n",
    "\n",
    "Time is cyclical: after 23:00 comes 00:00, and after December comes January. But if we use raw numeric values (e.g., `hour = 23` vs. `hour = 0`), many models treat these as very far apart. This creates an **artificial break** in the data.\n",
    "\n",
    "**Definition**: Cyclical encoding maps linear time values (like hours of the day or months of the year) onto a circle using sine and cosine functions. This way, times that are neighbors in reality remain neighbors in the feature space.\n",
    "\n",
    "**Purpose**: This encoding allows models to recognize smooth, circular patterns in demand, such as rush-hour peaks or seasonal shifts.\n",
    "\n",
    "**Example in Python: Days Since Last Weekend**\n",
    "\n",
    "Let's implement a comprehensive example that creates cyclical encoding for multiple time periods, including hours, days of week, and months:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba69866c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cyclical encoding completed!\n",
      "Hour 23: sin=-0.259, cos=0.966\n",
      "Hour 0:  sin=0.000, cos=1.000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import dataset\n",
    "data_path = \"https://raw.githubusercontent.com/pmarcelino/predictive-modeling/main/datasets/dataset.csv\"\n",
    "df = pd.read_csv(data_path, parse_dates=['datetime'])\n",
    "\n",
    "# Example: Create cyclical encoding for bike-sharing temporal features\n",
    "# Assuming we have a datetime column called 'datetime'\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['day_of_week'] = df['datetime'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "df['month'] = df['datetime'].dt.month\n",
    "\n",
    "# Step 1: Hour cyclical encoding (24-hour cycle)\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "\n",
    "# Step 2: Day of week cyclical encoding (7-day cycle) \n",
    "df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "\n",
    "# Step 3: Month cyclical encoding (12-month cycle)\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "\n",
    "print(\"Cyclical encoding completed!\")\n",
    "print(f\"Hour 23: sin={df[df['hour']==23]['hour_sin'].iloc[0]:.3f}, cos={df[df['hour']==23]['hour_cos'].iloc[0]:.3f}\")\n",
    "print(f\"Hour 0:  sin={df[df['hour']==0]['hour_sin'].iloc[0]:.3f}, cos={df[df['hour']==0]['hour_cos'].iloc[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ae528d",
   "metadata": {},
   "source": [
    "**Step-by-Step Explanation:**\n",
    "\n",
    "1. **Extract temporal components**: We first extract hour, day of week, and month from the datetime column to work with integer values.\n",
    "\n",
    "2. **Apply sine and cosine transformation**: For each temporal component, we use both sine and cosine functions:\n",
    "   - **Sine component**: Captures the position on one axis of the circle\n",
    "   - **Cosine component**: Captures the position on the perpendicular axis\n",
    "   - Together, they uniquely identify any point on the circle\n",
    "\n",
    "3. **Scale by the period**: We divide by the total period (24 for hours, 7 for days, 12 for months) to ensure one complete cycle maps to one full circle (2π radians).\n",
    "\n",
    "4. **Mathematical insight**: The formula `2 * π * value / period` converts linear time to radians, where:\n",
    "   - Hour 0 and hour 24 both map to the same point: (sin=0, cos=1)\n",
    "   - Hour 6 maps to: (sin=1, cos=0) \n",
    "   - Hour 12 maps to: (sin=0, cos=-1)\n",
    "   - Hour 18 maps to: (sin=-1, cos=0)\n",
    "\n",
    "With this representation, **23:00 and 00:00 map to nearly the same point** on the circle, preserving their closeness. The Euclidean distance between these encoded points accurately reflects their temporal proximity, unlike raw numeric encoding where 23 and 0 appear far apart.\n",
    "\n",
    "This cyclical encoding works well for many model types (linear regression, tree-based models, kNN, neural networks), since it embeds the true temporal geometry directly into the data. For bike-sharing systems, it helps models learn that late-night hours share similarities with early-morning hours, both typically showing low demand patterns.\n",
    "\n",
    "**2. Time-Since Features**\n",
    "\n",
    "Sometimes what matters most is not the absolute time, but **how much time has passed since an important event**.\n",
    "\n",
    "**Definition**: A time-since feature measures the elapsed time between the current observation and a meaningful reference point.\n",
    "\n",
    "**Purpose**: These features allow models to capture temporal effects related to holidays, weekends, or unusual conditions.\n",
    "\n",
    "**Examples**:\n",
    "\n",
    "* Days since the start of the current season\n",
    "* Hours since the last weekend ended\n",
    "* Time since the most recent holiday period\n",
    "* Days since a major weather change\n",
    "\n",
    "**Example in Python: Days Since Last Weekend**\n",
    "\n",
    "Let's implement time-since features that capture important temporal transitions in bike-sharing demand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0568e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time-since features created:\n",
      "Days since weekend - Range: 0 to 20\n",
      "Hours since holiday - Average: 156.7\n",
      "Days since weather change - Average: 33.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Example: Create time-since features for bike-sharing demand prediction\n",
    "# Assuming we have a datetime column and related features already created\n",
    "\n",
    "def create_time_since_features(df):\n",
    "    \"\"\"\n",
    "    Create time-since features that measure elapsed time from important events.\n",
    "    These features help capture recovery patterns and transition effects.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure datetime column is properly formatted\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    df = df.sort_values('datetime')  # Ensure chronological order\n",
    "    \n",
    "    # Step 1: Days since last weekend ended\n",
    "    # Weekend ends on Sunday evening (day 6), new week starts Monday\n",
    "    df['is_weekend'] = df['datetime'].dt.dayofweek.isin([5, 6])  # Saturday=5, Sunday=6\n",
    "    \n",
    "    # Find all weekend end points (Sunday evenings at 23:59)\n",
    "    weekend_ends = df[(df['datetime'].dt.dayofweek == 6) & \n",
    "                      (df['datetime'].dt.hour == 23)]['datetime']\n",
    "    \n",
    "    # Calculate days since last weekend for each observation\n",
    "    df['days_since_weekend'] = df['datetime'].apply(\n",
    "        lambda x: (x - weekend_ends[weekend_ends <= x].max()).days \n",
    "        if len(weekend_ends[weekend_ends <= x]) > 0 else 7\n",
    "    )\n",
    "    \n",
    "    # Step 2: Hours since last holiday ended\n",
    "    # Assuming we have a 'holiday' column (1 for holiday, 0 for normal day)\n",
    "    holiday_ends = df[df['holiday'] == 1]['datetime'].dt.date.unique()\n",
    "    \n",
    "    df['hours_since_holiday'] = df['datetime'].apply(\n",
    "        lambda x: min([(x.date() - holiday_date).days * 24 + x.hour \n",
    "                      for holiday_date in holiday_ends if holiday_date < x.date()] + [168])\n",
    "    )\n",
    "    \n",
    "    # Step 3: Days since significant weather change\n",
    "    # Define significant weather change as temperature difference > 10°C from previous day\n",
    "    df['temp_change'] = df['temp'].diff().abs()\n",
    "    df['significant_weather_change'] = df['temp_change'] > 10\n",
    "    \n",
    "    # Mark dates with significant weather changes\n",
    "    weather_change_dates = df[df['significant_weather_change'] == True]['datetime']\n",
    "    \n",
    "    df['days_since_weather_change'] = df['datetime'].apply(\n",
    "        lambda x: (x - weather_change_dates[weather_change_dates <= x].max()).days \n",
    "        if len(weather_change_dates[weather_change_dates <= x]) > 0 else 30\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply time-since feature engineering\n",
    "df = create_time_since_features(df)\n",
    "\n",
    "print(\"Time-since features created:\")\n",
    "print(f\"Days since weekend - Range: {df['days_since_weekend'].min()} to {df['days_since_weekend'].max()}\")\n",
    "print(f\"Hours since holiday - Average: {df['hours_since_holiday'].mean():.1f}\")\n",
    "print(f\"Days since weather change - Average: {df['days_since_weather_change'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717ad74a",
   "metadata": {},
   "source": [
    "**Step-by-Step Explanation:**\n",
    "\n",
    "1. **Weekend Recovery Pattern**: `days_since_weekend` captures how bike demand gradually changes as the workweek progresses. Monday (days_since_weekend = 1) often shows different patterns than Friday (days_since_weekend = 5).\n",
    "\n",
    "2. **Holiday Recovery Effects**: `hours_since_holiday` measures recovery time from holiday disruptions. Bike demand often takes 1-3 days to return to normal patterns after holidays.\n",
    "\n",
    "3. **Weather Adaptation Period**: `days_since_weather_change` captures how users adapt to significant weather changes. After a major temperature drop, it might take several days for demand patterns to stabilize.\n",
    "\n",
    "4. **Boundary Handling**: The code includes default values (7 days, 168 hours, 30 days) for cases where no previous event is found, ensuring robust feature creation.\n",
    "\n",
    "In bike-sharing systems, \"days since last weekend\" might reveal that Tuesday demand is typically higher than Monday as people settle back into commuting routines. Similarly, \"days since last snowfall\" could be a strong predictor of demand recovery, as ridership gradually returns to normal after weather disruptions.\n",
    "\n",
    "**3. Temporal Aggregation Features**\n",
    "\n",
    "Transportation demand often depends on **recent history**, not just single time points.\n",
    "\n",
    "**Definition**: Temporal aggregation features summarize past values of a variable over a defined time window.\n",
    "\n",
    "**Purpose**: They provide context about recent conditions, such as whether demand has been steadily rising or if the weather has been unusually stable.\n",
    "\n",
    "**Examples**:\n",
    "\n",
    "* Average demand in the past 3 hours\n",
    "* Maximum temperature in the past 24 hours\n",
    "* Weather stability over the past week\n",
    "* Growth rate of ridership over the past month\n",
    "\n",
    "**Example in Python: Rolling Weather and Demand Aggregations**\n",
    "\n",
    "Let's implement temporal aggregation features that provide context about recent conditions and trends:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d5d9764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal aggregation features created:\n",
      "3-hour demand volatility - Average: 63.08\n",
      "Temperature stability - Average: 0.315\n",
      "Demand growth rate - Range: -0.993 to 3.307\n",
      "Weekend/weekday ratio - Average: 0.96\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def create_temporal_aggregation_features(df):\n",
    "    \"\"\"\n",
    "    Create temporal aggregation features that summarize recent history\n",
    "    to provide context for current predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure data is sorted chronologically for rolling calculations\n",
    "    df = df.sort_values('datetime')\n",
    "    \n",
    "    # Step 1: Rolling demand statistics (short-term momentum)\n",
    "    # 3-hour rolling average captures immediate demand trends\n",
    "    df['demand_3h_avg'] = df['count'].rolling(window=3, min_periods=1).mean()\n",
    "    \n",
    "    # 3-hour rolling maximum shows peak demand in recent period\n",
    "    df['demand_3h_max'] = df['count'].rolling(window=3, min_periods=1).max()\n",
    "    \n",
    "    # 3-hour demand volatility (standard deviation) indicates stability\n",
    "    df['demand_3h_volatility'] = df['count'].rolling(window=3, min_periods=1).std()\n",
    "    \n",
    "    # Step 2: Weather stability indicators (24-hour windows)\n",
    "    # Temperature stability: how much temperature has varied recently\n",
    "    df['temp_24h_stability'] = 1 / (1 + df['temp'].rolling(window=24, min_periods=1).std())\n",
    "    \n",
    "    # Maximum temperature in past 24 hours\n",
    "    df['temp_24h_max'] = df['temp'].rolling(window=24, min_periods=1).max()\n",
    "    \n",
    "    # Minimum temperature in past 24 hours\n",
    "    df['temp_24h_min'] = df['temp'].rolling(window=24, min_periods=1).min()\n",
    "    \n",
    "    # Temperature range (daily temperature swing)\n",
    "    df['temp_24h_range'] = df['temp_24h_max'] - df['temp_24h_min']\n",
    "    \n",
    "    # Step 3: Weekly trend indicators (168-hour = 7-day windows)\n",
    "    # 7-day rolling average for trend analysis\n",
    "    df['demand_7d_avg'] = df['count'].rolling(window=168, min_periods=24).mean()\n",
    "    \n",
    "    # Growth rate: current vs. 7-day average (positive = growing demand)\n",
    "    df['demand_growth_rate'] = (df['count'] - df['demand_7d_avg']) / (df['demand_7d_avg'] + 1)\n",
    "    \n",
    "    # Weather trend: is temperature trending up or down this week?\n",
    "    df['temp_7d_trend'] = df['temp'].rolling(window=168, min_periods=24).apply(\n",
    "        lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) > 1 else 0\n",
    "    )\n",
    "    \n",
    "    # Step 4: Advanced aggregations for business insights\n",
    "    # Rush hour intensity in past 3 days (for pattern detection)\n",
    "    rush_hours = df['hour'].isin([7, 8, 9, 17, 18, 19])\n",
    "    df['rush_hour_demand'] = np.where(rush_hours, df['count'], np.nan)\n",
    "    df['rush_demand_3d_avg'] = df['rush_hour_demand'].rolling(window=72, min_periods=6).mean()\n",
    "    \n",
    "    # Weekend vs weekday demand ratio (past 4 weeks)\n",
    "    df['is_weekend'] = df['datetime'].dt.dayofweek.isin([5, 6])\n",
    "    df['weekend_demand'] = np.where(df['is_weekend'], df['count'], np.nan)\n",
    "    df['weekday_demand'] = np.where(~df['is_weekend'], df['count'], np.nan)\n",
    "    \n",
    "    weekend_avg = df['weekend_demand'].rolling(window=672, min_periods=48).mean()  # 4 weeks\n",
    "    weekday_avg = df['weekday_demand'].rolling(window=672, min_periods=120).mean()\n",
    "    df['weekend_weekday_ratio'] = weekend_avg / (weekday_avg + 1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply temporal aggregation features\n",
    "df = create_temporal_aggregation_features(df)\n",
    "\n",
    "print(\"Temporal aggregation features created:\")\n",
    "print(f\"3-hour demand volatility - Average: {df['demand_3h_volatility'].mean():.2f}\")\n",
    "print(f\"Temperature stability - Average: {df['temp_24h_stability'].mean():.3f}\")\n",
    "print(f\"Demand growth rate - Range: {df['demand_growth_rate'].min():.3f} to {df['demand_growth_rate'].max():.3f}\")\n",
    "print(f\"Weekend/weekday ratio - Average: {df['weekend_weekday_ratio'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcc38ac",
   "metadata": {},
   "source": [
    "**Step-by-Step Explanation:**\n",
    "\n",
    "1. **Short-term momentum (3-hour windows)**: These features capture immediate trends and volatility in demand. High volatility might indicate special events or system disruptions.\n",
    "\n",
    "2. **Daily weather context (24-hour windows)**: Temperature stability indicates whether weather has been consistent, which affects user behavior predictability. A high daily temperature range suggests variable conditions.\n",
    "\n",
    "3. **Weekly trend analysis (7-day windows)**: The growth rate compares current demand to recent average, revealing whether the system is experiencing increasing or decreasing usage. Temperature trend identifies sustained weather patterns.\n",
    "\n",
    "4. **Business-specific aggregations**: Rush hour averages help identify commuting pattern changes, while weekend/weekday ratios reveal seasonal shifts in usage patterns.\n",
    "\n",
    "5. **Robust handling**: The `min_periods` parameter ensures features are calculated even with some missing data, while adding small constants (+ 1) prevents division by zero.\n",
    "\n",
    "In consulting practice, showing a client the \"rolling 7-day average demand\" can highlight whether usage trends are growing, stabilizing, or declining. The temperature stability feature might reveal that unstable weather periods consistently reduce demand by 15-20%, providing actionable insights for capacity planning.\n",
    "\n",
    "**4. Lag Features for Sequential Patterns**\n",
    "\n",
    "Urban mobility demand is not random—it shows **strong sequential dependencies**.\n",
    "\n",
    "**Definition**: A lag feature uses the value of a variable from a previous time step as a predictor for the current step.\n",
    "\n",
    "**Purpose**: These features explicitly introduce historical demand patterns into the model, helping it learn recurring cycles.\n",
    "\n",
    "**Examples**:\n",
    "\n",
    "* Demand 1 hour ago (captures immediate momentum)\n",
    "* Demand 24 hours ago (captures same-time-of-day effects)\n",
    "* Demand 7 days ago (captures weekly repetition)\n",
    "* Average demand at this hour over the past month\n",
    "\n",
    "**Example in Python: Multiple Lag Features for Sequential Pattern Recognition**\n",
    "\n",
    "Let's implement lag features that capture different types of sequential dependencies in bike-sharing demand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7de5ecf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lag features created:\n",
      "1-hour lag correlation with current demand: 0.842\n",
      "24-hour lag correlation with current demand: 0.811\n",
      "7-day lag correlation with current demand: 0.786\n",
      "Same hour trend - Range: -126.107 to 125.857\n",
      "\n",
      "Sample lag feature values (first 10 non-null rows):\n",
      "           datetime  count  demand_lag_1h  demand_lag_24h  demand_lag_7d  demand_same_hour_avg_7d\n",
      "2011-01-08 07:00:00      9            2.0            84.0           16.0                21.285714\n",
      "2011-01-08 08:00:00     15            9.0           210.0           40.0                48.857143\n",
      "2011-01-08 09:00:00     20           15.0           134.0           32.0                67.000000\n",
      "2011-01-08 10:00:00     61           20.0            63.0           13.0                75.857143\n",
      "2011-01-08 11:00:00     62           61.0            67.0            1.0                85.285714\n",
      "2011-01-08 12:00:00     98           62.0            59.0            1.0                93.000000\n",
      "2011-01-08 13:00:00    102           98.0            73.0            2.0                98.571429\n",
      "2011-01-08 14:00:00     95          102.0            50.0            3.0                93.714286\n",
      "2011-01-08 15:00:00     74           95.0            72.0            8.0                74.000000\n",
      "2011-01-08 16:00:00     76           74.0            87.0           14.0                67.285714\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def create_lag_features(df):\n",
    "    \"\"\"\n",
    "    Create lag features that capture sequential dependencies and recurring patterns\n",
    "    in bike-sharing demand data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure data is sorted chronologically for proper lag calculation\n",
    "    df = df.sort_values('datetime').reset_index(drop=True)\n",
    "    \n",
    "    # Step 1: Basic lag features for immediate patterns\n",
    "    # Immediate momentum: demand 1 hour ago\n",
    "    df['demand_lag_1h'] = df['count'].shift(1)\n",
    "    \n",
    "    # Recent pattern: demand 2 and 3 hours ago\n",
    "    df['demand_lag_2h'] = df['count'].shift(2)\n",
    "    df['demand_lag_3h'] = df['count'].shift(3)\n",
    "    \n",
    "    # Step 2: Same-time-of-day patterns (24-hour lags)\n",
    "    # Yesterday same hour (captures daily repetition)\n",
    "    df['demand_lag_24h'] = df['count'].shift(24)\n",
    "    \n",
    "    # Day before yesterday same hour (validates daily pattern)\n",
    "    df['demand_lag_48h'] = df['count'].shift(48)\n",
    "    \n",
    "    # Step 3: Weekly recurring patterns (7-day lags)\n",
    "    # Same day-hour last week (captures weekly cycles)\n",
    "    df['demand_lag_7d'] = df['count'].shift(24 * 7)  # 168 hours\n",
    "    \n",
    "    # Same day-hour 2 weeks ago (validates weekly pattern)\n",
    "    df['demand_lag_14d'] = df['count'].shift(24 * 14)  # 336 hours\n",
    "    \n",
    "    # Step 4: Advanced lag combinations for complex patterns\n",
    "    # Average of same-time-of-day over past week (more stable than single point)\n",
    "    df['demand_same_hour_avg_7d'] = df['count'].shift(24).rolling(window=7, min_periods=3).mean()\n",
    "    \n",
    "    # Trend in same-time-of-day demand (is this time slot growing or declining?)\n",
    "    same_time_values = []\n",
    "    for i in range(len(df)):\n",
    "        if i >= 24 * 7:  # Need at least 1 week of data\n",
    "            # Get demand at same hour for past 7 days\n",
    "            same_hour_demands = [df.iloc[i - 24 * (d+1)]['count'] for d in range(7)]\n",
    "            # Calculate linear trend (positive = growing, negative = declining)\n",
    "            if all(pd.notna(same_hour_demands)):\n",
    "                trend = np.polyfit(range(7), same_hour_demands, 1)[0]\n",
    "            else:\n",
    "                trend = 0\n",
    "        else:\n",
    "            trend = 0\n",
    "        same_time_values.append(trend)\n",
    "    \n",
    "    df['demand_same_hour_trend'] = same_time_values\n",
    "    \n",
    "    # Step 5: Weather lag features (weather affects demand with some delay)\n",
    "    # Temperature 1 and 3 hours ago (weather decision lag)\n",
    "    df['temp_lag_1h'] = df['temp'].shift(1)\n",
    "    df['temp_lag_3h'] = df['temp'].shift(3)\n",
    "    \n",
    "    # Weather condition lag (people may check weather hours before trip)\n",
    "    if 'weather' in df.columns:\n",
    "        df['weather_lag_2h'] = df['weather'].shift(2)\n",
    "    \n",
    "    # Step 6: User type lag patterns (different lag behaviors)\n",
    "    # Casual users (more spontaneous, shorter lags)\n",
    "    df['casual_lag_1h'] = df['casual'].shift(1)\n",
    "    df['casual_lag_2h'] = df['casual'].shift(2)\n",
    "    \n",
    "    # Registered users (more routine-driven, longer predictable lags)\n",
    "    df['registered_lag_24h'] = df['registered'].shift(24)\n",
    "    df['registered_lag_7d'] = df['registered'].shift(24 * 7)\n",
    "    \n",
    "    # Step 7: Interaction lag features (capture combined effects)\n",
    "    # Weather-demand interaction from previous day\n",
    "    df['temp_demand_interaction_24h'] = (df['temp'].shift(24) * df['count'].shift(24))\n",
    "    \n",
    "    # Weekend effect lag (Friday affects weekend, Sunday affects Monday)\n",
    "    df['weekend_effect_lag'] = np.where(\n",
    "        df['datetime'].dt.dayofweek == 0,  # Monday\n",
    "        df['count'].shift(24 * 2),  # Saturday demand affects Monday\n",
    "        df['count'].shift(24)  # Previous day for other days\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply lag feature engineering\n",
    "df = create_lag_features(df)\n",
    "\n",
    "print(\"Lag features created:\")\n",
    "print(f\"1-hour lag correlation with current demand: {df['count'].corr(df['demand_lag_1h']):.3f}\")\n",
    "print(f\"24-hour lag correlation with current demand: {df['count'].corr(df['demand_lag_24h']):.3f}\")\n",
    "print(f\"7-day lag correlation with current demand: {df['count'].corr(df['demand_lag_7d']):.3f}\")\n",
    "print(f\"Same hour trend - Range: {df['demand_same_hour_trend'].min():.3f} to {df['demand_same_hour_trend'].max():.3f}\")\n",
    "\n",
    "# Display sample of lag features for business understanding\n",
    "print(\"\\nSample lag feature values (first 10 non-null rows):\")\n",
    "lag_cols = ['demand_lag_1h', 'demand_lag_24h', 'demand_lag_7d', 'demand_same_hour_avg_7d']\n",
    "sample_data = df[['datetime', 'count'] + lag_cols].dropna().head(10)\n",
    "print(sample_data.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae58d6b",
   "metadata": {},
   "source": [
    "**Step-by-Step Explanation:**\n",
    "\n",
    "1. **Immediate momentum (1-3 hour lags)**: These features capture short-term demand trends. If demand was high 1-2 hours ago, it might continue to be high, indicating system momentum or sustained favorable conditions.\n",
    "\n",
    "2. **Daily repetition patterns (24-48 hour lags)**: These features leverage the strong daily cyclical nature of transportation. Demand at 8 AM today often resembles demand at 8 AM yesterday, especially for commuting patterns.\n",
    "\n",
    "3. **Weekly recurring cycles (7-14 day lags)**: These capture weekly rhythms where Monday 8 AM patterns repeat week after week. This is especially powerful for commuting and routine trip prediction.\n",
    "\n",
    "4. **Advanced temporal aggregations**: Instead of just using single lag points, we create rolling averages of same-time-of-day demand and trend analysis to capture whether specific time periods are growing or declining in popularity.\n",
    "\n",
    "5. **Weather lag effects**: Weather influences decisions with some delay - people may check the weather forecast hours before deciding to bike, creating lag effects between weather changes and demand response.\n",
    "\n",
    "6. **User-specific lag patterns**: Casual and registered users show different lag behaviors. Casual users are more spontaneous (shorter lags matter), while registered users are more routine-driven (longer lags are predictive).\n",
    "\n",
    "7. **Interaction lag features**: These capture complex relationships, like how Friday's weather might affect weekend recreational biking, or how Saturday's high demand might predict Monday's commuter rebound.\n",
    "\n",
    "For bike-sharing systems, lag features help the model learn that **Monday 8 AM demand this week will likely resemble Monday 8 AM last week**, adjusted for seasonal effects. The correlation values show which time horizons are most predictive - typically 24-hour lags have the strongest correlation for commuting patterns, while 7-day lags excel for recreational usage prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6a5bb0",
   "metadata": {},
   "source": [
    "### Business-Relevant Temporal Features\n",
    "\n",
    "So far, we have looked at general temporal transformations. But in real-world consulting, models are most valuable when they reflect **business logic and human behavior**. This section shows how to design temporal features that go beyond raw time values, directly encoding **transportation-specific patterns** such as rush hours, weekday/weekend differences, and gradual seasonal shifts.\n",
    "\n",
    "We’ll focus on three key types:\n",
    "\n",
    "1. **Rush Hour Intensity**\n",
    "2. **Workday vs. Weekend Context**\n",
    "3. **Seasonal Progression Features**\n",
    "\n",
    "**1. Rush Hour Intensity**\n",
    "\n",
    "Bike-sharing demand is heavily shaped by commuting patterns. Instead of treating “rush hour” as a simple binary (yes/no), we can model it as a **continuous intensity measure**.\n",
    "\n",
    "**Definition**: Rush hour intensity features capture the strength of peak commuting periods on a smooth scale, rather than an abrupt cutoff.\n",
    "\n",
    "**Purpose**: This approach better reflects reality: demand gradually builds before 8 AM, peaks during the commute, and slowly declines afterward.\n",
    "\n",
    "**Example in Python: Advanced Rush Hour Modeling**\n",
    "\n",
    "Let's implement comprehensive rush hour features that capture different aspects of commuting patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "728eced9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rush hour features created:\n",
      "Rush intensity at 8 AM: 1.000\n",
      "Rush intensity at 12 PM: 0.029\n",
      "Rush intensity at 5 PM: 1.000\n",
      "Average hours to rush: 3.2\n",
      "\n",
      "Rush hour intensity by hour (sample):\n",
      "      rush_intensity  lunch_intensity  evening_activity\n",
      "hour                                                   \n",
      "0              0.000            0.000             0.000\n",
      "1              0.000            0.000             0.000\n",
      "2              0.000            0.000             0.000\n",
      "3              0.004            0.000             0.000\n",
      "4              0.029            0.000             0.000\n",
      "5              0.135            0.000             0.000\n",
      "6              0.411            0.000             0.000\n",
      "7              0.801            0.000             0.000\n",
      "8              1.000            0.000             0.000\n",
      "9              0.801            0.011             0.000\n",
      "10             0.411            0.135             0.000\n",
      "11             0.135            0.607             0.000\n",
      "12             0.029            1.000             0.000\n",
      "13             0.029            0.607             0.000\n",
      "14             0.135            0.135             0.002\n",
      "15             0.411            0.011             0.011\n",
      "16             0.801            0.000             0.044\n",
      "17             1.000            0.000             0.135\n",
      "18             0.801            0.000             0.325\n",
      "19             0.411            0.000             0.607\n",
      "20             0.135            0.000             0.882\n",
      "21             0.029            0.000             1.000\n",
      "22             0.004            0.000             0.882\n",
      "23             0.000            0.000             0.607\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def create_rush_hour_features(df):\n",
    "    \"\"\"\n",
    "    Create sophisticated rush hour features that capture commuting patterns\n",
    "    with smooth intensity measures rather than binary indicators.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Basic rush hour intensity using Gaussian curves\n",
    "    # Morning rush centered at 8 AM with 1.5-hour standard deviation\n",
    "    rush_morning = np.exp(-((df['hour'] - 8)**2) / (2 * 1.5**2))\n",
    "    \n",
    "    # Evening rush centered at 5 PM with 1.5-hour standard deviation  \n",
    "    rush_evening = np.exp(-((df['hour'] - 17)**2) / (2 * 1.5**2))\n",
    "    \n",
    "    # Combined rush intensity (takes maximum of morning and evening)\n",
    "    df['rush_intensity'] = np.maximum(rush_morning, rush_evening)\n",
    "    \n",
    "    # Step 2: Separate morning and evening intensities\n",
    "    # Sometimes models benefit from knowing which rush period is active\n",
    "    df['morning_rush_intensity'] = rush_morning\n",
    "    df['evening_rush_intensity'] = rush_evening\n",
    "    \n",
    "    # Step 3: Advanced rush hour variations\n",
    "    # Lunch hour intensity (centered at noon with tighter spread)\n",
    "    df['lunch_intensity'] = np.exp(-((df['hour'] - 12)**2) / (2 * 1.0**2))\n",
    "    \n",
    "    # Late evening activity (centered at 9 PM for entertainment/dining)\n",
    "    df['evening_activity'] = np.exp(-((df['hour'] - 21)**2) / (2 * 2.0**2))\n",
    "    \n",
    "    # Step 4: Business-day adjusted rush hours\n",
    "    # Rush hours only matter on working days, so multiply by workingday indicator\n",
    "    df['workday_rush_intensity'] = df['rush_intensity'] * df['workingday']\n",
    "    df['workday_morning_rush'] = df['morning_rush_intensity'] * df['workingday']\n",
    "    df['workday_evening_rush'] = df['evening_rush_intensity'] * df['workingday']\n",
    "    \n",
    "    # Step 5: Time-to-rush features (how close are we to peak times?)\n",
    "    # Distance to nearest rush hour peak (useful for anticipating demand)\n",
    "    morning_distance = np.abs(df['hour'] - 8)\n",
    "    evening_distance = np.abs(df['hour'] - 17)\n",
    "    \n",
    "    # Minimum time to either rush hour (accounting for 24-hour cycle)\n",
    "    df['hours_to_rush'] = np.minimum(\n",
    "        np.minimum(morning_distance, 24 - morning_distance),\n",
    "        np.minimum(evening_distance, 24 - evening_distance)\n",
    "    )\n",
    "    \n",
    "    # Step 6: Rush hour context indicators\n",
    "    # Pre-rush period (1-2 hours before peak)\n",
    "    df['pre_morning_rush'] = ((df['hour'] >= 6) & (df['hour'] <= 7)).astype(int)\n",
    "    df['pre_evening_rush'] = ((df['hour'] >= 15) & (df['hour'] <= 16)).astype(int)\n",
    "    \n",
    "    # Post-rush period (1-2 hours after peak)\n",
    "    df['post_morning_rush'] = ((df['hour'] >= 9) & (df['hour'] <= 10)).astype(int)\n",
    "    df['post_evening_rush'] = ((df['hour'] >= 18) & (df['hour'] <= 19)).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply rush hour feature engineering\n",
    "df = create_rush_hour_features(df)\n",
    "\n",
    "print(\"Rush hour features created:\")\n",
    "print(f\"Rush intensity at 8 AM: {df[df['hour']==8]['rush_intensity'].iloc[0]:.3f}\")\n",
    "print(f\"Rush intensity at 12 PM: {df[df['hour']==12]['rush_intensity'].iloc[0]:.3f}\")  \n",
    "print(f\"Rush intensity at 5 PM: {df[df['hour']==17]['rush_intensity'].iloc[0]:.3f}\")\n",
    "print(f\"Average hours to rush: {df['hours_to_rush'].mean():.1f}\")\n",
    "\n",
    "# Show how features vary throughout the day\n",
    "print(\"\\nRush hour intensity by hour (sample):\")\n",
    "hourly_rush = df.groupby('hour')[['rush_intensity', 'lunch_intensity', 'evening_activity']].mean()\n",
    "print(hourly_rush.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0d50cb",
   "metadata": {},
   "source": [
    "**Step-by-Step Explanation:**\n",
    "\n",
    "1. **Gaussian Rush Hour Curves**: The formula `np.exp(-((hour - peak)**2) / (2 * std**2))` creates smooth bell curves centered at rush hour times. This is much more realistic than binary on/off indicators, as demand gradually builds and declines around peak times.\n",
    "\n",
    "2. **Separate Morning and Evening Features**: While combined rush intensity is useful, separate features allow models to learn that morning and evening rush patterns may differ (e.g., morning rush might be more predictable than evening rush).\n",
    "\n",
    "3. **Additional Time Peaks**: Beyond traditional rush hours, we capture lunch hour patterns and evening entertainment periods, which also drive bike-sharing demand in urban areas.\n",
    "\n",
    "4. **Working Day Adjustment**: Rush hours only apply to working days, so we multiply by the workingday indicator to create features that are zero on weekends and holidays.\n",
    "\n",
    "5. **Time-to-Rush Distance**: This feature helps models anticipate demand changes - hours immediately before rush hour often show building demand patterns.\n",
    "\n",
    "6. **Rush Hour Context Periods**: Binary indicators for pre-rush and post-rush periods help models understand transitional demand patterns.\n",
    "\n",
    "> **Note**: The Gaussian function creates smooth transitions where:\n",
    "> - Peak rush hours (8 AM, 5 PM) get intensity = 1.0\n",
    "> - Hours 1.5 standard deviations away get intensity ≈ 0.6\n",
    "> - Hours 3+ standard deviations away get intensity < 0.1\n",
    "\n",
    "For a bike-sharing operator, this feature encodes how strongly each time of day aligns with commuting demand, giving the model a better understanding of **urban mobility rhythms**. The smooth intensity measure is more realistic than binary rush/non-rush categories and allows models to predict gradual demand transitions.\n",
    "\n",
    "**2. Workday vs. Weekend Context**\n",
    "\n",
    "Not all hours are equal—**the same 8 AM can mean very different things** depending on the day of the week.\n",
    "\n",
    "**Definition**: Workday/weekend features separate time into business-relevant categories based on how people use transportation.\n",
    "\n",
    "**Purpose**: They help models distinguish between commuting-driven patterns (weekday mornings and evenings) and leisure-driven usage (weekend afternoons).\n",
    "\n",
    "**Examples of features**:\n",
    "\n",
    "* Working hours vs. leisure hours\n",
    "* Commuting periods vs. recreational periods\n",
    "* Business district active hours vs. residential area activity\n",
    "\n",
    "**Example in Python: Comprehensive Workday vs Weekend Context**\n",
    "\n",
    "Let's implement features that distinguish between different types of time periods based on their behavioral implications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e37d0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workday/Weekend context features created:\n",
      "Business hours active - Weekend avg: 0.000\n",
      "Business hours active - Weekday avg: 0.461\n",
      "Recreation probability - Weekend avg: 0.800\n",
      "Commuting probability - Weekday rush avg: 1.000\n",
      "\n",
      "Context feature comparison (8 AM):\n",
      "            weekday_morning_commute  weekend_morning_leisure  \\\n",
      "is_weekend                                                     \n",
      "0                               1.0                      0.0   \n",
      "1                               0.0                      1.0   \n",
      "\n",
      "            commuting_probability  recreation_probability  \n",
      "is_weekend                                                 \n",
      "0                             1.0                     0.0  \n",
      "1                             0.1                     0.8  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def create_workday_weekend_features(df):\n",
    "    \"\"\"\n",
    "    Create features that capture how the same time periods have different meanings\n",
    "    on workdays vs weekends, reflecting different user behaviors and motivations.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Basic day type indicators (building blocks for more complex features)\n",
    "    df['is_weekend'] = df['datetime'].dt.dayofweek.isin([5, 6]).astype(int)  # Sat=5, Sun=6\n",
    "    df['is_weekday'] = (~df['datetime'].dt.dayofweek.isin([5, 6])).astype(int)\n",
    "    df['is_friday'] = (df['datetime'].dt.dayofweek == 4).astype(int)  # Friday has unique patterns\n",
    "    df['is_monday'] = (df['datetime'].dt.dayofweek == 0).astype(int)   # Monday has unique patterns\n",
    "    \n",
    "    # Step 1: Time period context features\n",
    "    # Same hours mean different things on different day types\n",
    "    \n",
    "    # Morning commute context (6-10 AM)\n",
    "    morning_commute_hours = ((df['hour'] >= 6) & (df['hour'] <= 10)).astype(int)\n",
    "    df['weekday_morning_commute'] = morning_commute_hours * df['is_weekday']\n",
    "    df['weekend_morning_leisure'] = morning_commute_hours * df['is_weekend']\n",
    "    \n",
    "    # Lunch period context (11 AM - 2 PM)\n",
    "    lunch_hours = ((df['hour'] >= 11) & (df['hour'] <= 14)).astype(int)\n",
    "    df['weekday_lunch_break'] = lunch_hours * df['is_weekday']\n",
    "    df['weekend_afternoon_activity'] = lunch_hours * df['is_weekend']\n",
    "    \n",
    "    # Evening context (5-9 PM) \n",
    "    evening_hours = ((df['hour'] >= 17) & (df['hour'] <= 21)).astype(int)\n",
    "    df['weekday_evening_commute'] = evening_hours * df['is_weekday']  \n",
    "    df['weekend_evening_recreation'] = evening_hours * df['is_weekend']\n",
    "    \n",
    "    # Step 2: Business vs leisure hour classification\n",
    "    # Working hours: 8 AM - 6 PM on weekdays\n",
    "    business_hours = ((df['hour'] >= 8) & (df['hour'] <= 18)).astype(int)\n",
    "    df['business_hours_active'] = business_hours * df['is_weekday']\n",
    "    \n",
    "    # Leisure hours: Evenings and weekends\n",
    "    leisure_weekday_evening = ((df['hour'] >= 18) | (df['hour'] <= 7)) * df['is_weekday']\n",
    "    leisure_weekend_all = df['is_weekend']  # All weekend hours are leisure\n",
    "    df['leisure_hours_active'] = np.maximum(leisure_weekday_evening, leisure_weekend_all)\n",
    "    \n",
    "    # Step 3: Advanced temporal-contextual interactions\n",
    "    # Friday evening vs other evenings (different social patterns)\n",
    "    df['friday_evening_social'] = evening_hours * df['is_friday']\n",
    "    \n",
    "    # Monday morning return-to-work effect\n",
    "    df['monday_morning_return'] = morning_commute_hours * df['is_monday']\n",
    "    \n",
    "    # Weekend vs weekday rush hour comparison\n",
    "    # This shows how \"rush hour\" times perform differently on weekends\n",
    "    traditional_rush = ((df['hour'].isin([7, 8, 9])) | (df['hour'].isin([17, 18, 19]))).astype(int)\n",
    "    df['weekday_traditional_rush'] = traditional_rush * df['is_weekday']\n",
    "    df['weekend_during_rush_hours'] = traditional_rush * df['is_weekend']\n",
    "    \n",
    "    # Step 4: Day transition effects\n",
    "    # Weekend preparation (Friday afternoon/evening)\n",
    "    df['weekend_preparation'] = ((df['hour'] >= 15) & (df['is_friday'])).astype(int)\n",
    "    \n",
    "    # Weekend recovery (Sunday evening - people preparing for Monday)\n",
    "    df['weekend_recovery'] = ((df['hour'] >= 18) & (df['datetime'].dt.dayofweek == 6)).astype(int)  # Sunday evening\n",
    "    \n",
    "    # Step 5: Activity type probability indicators\n",
    "    # Probability that current time slot is used for commuting vs recreation\n",
    "    \n",
    "    # Commuting probability (high on weekday rush hours)\n",
    "    commute_time_weekday = ((df['hour'].isin([7, 8, 9, 17, 18, 19])) & df['is_weekday']).astype(float)\n",
    "    commute_time_weekend = df['is_weekend'] * 0.1  # Low but non-zero weekend commuting\n",
    "    df['commuting_probability'] = np.maximum(commute_time_weekday, commute_time_weekend)\n",
    "    \n",
    "    # Recreation probability (high on weekends, evenings, lunch breaks)\n",
    "    recreation_weekend = df['is_weekend'] * 0.8  # High baseline recreation on weekends\n",
    "    recreation_weekday_evening = ((df['hour'] >= 18) & df['is_weekday']) * 0.6\n",
    "    recreation_lunch = ((df['hour'] >= 11) & (df['hour'] <= 14)) * 0.3\n",
    "    df['recreation_probability'] = np.maximum.reduce([recreation_weekend, \n",
    "                                                     recreation_weekday_evening, \n",
    "                                                     recreation_lunch])\n",
    "    \n",
    "    # Step 6: User behavior context features\n",
    "    # Different user types (casual vs registered) have different day-type sensitivities\n",
    "    \n",
    "    # Weekend premium for casual users (weekends see higher casual usage)\n",
    "    df['weekend_casual_boost'] = df['is_weekend'] * 1.5\n",
    "    \n",
    "    # Workday routine for registered users (consistent weekday patterns)\n",
    "    df['workday_registered_routine'] = df['is_weekday'] * df['business_hours_active']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply workday/weekend context features\n",
    "df = create_workday_weekend_features(df)\n",
    "\n",
    "print(\"Workday/Weekend context features created:\")\n",
    "print(f\"Business hours active - Weekend avg: {df[df['is_weekend']==1]['business_hours_active'].mean():.3f}\")\n",
    "print(f\"Business hours active - Weekday avg: {df[df['is_weekday']==1]['business_hours_active'].mean():.3f}\")\n",
    "print(f\"Recreation probability - Weekend avg: {df[df['is_weekend']==1]['recreation_probability'].mean():.3f}\")\n",
    "print(f\"Commuting probability - Weekday rush avg: {df[(df['is_weekday']==1) & (df['hour'].isin([8,17]))]['commuting_probability'].mean():.3f}\")\n",
    "\n",
    "# Show context differences by day type and hour\n",
    "print(\"\\nContext feature comparison (8 AM):\")\n",
    "morning_comparison = df[df['hour']==8].groupby('is_weekend')[\n",
    "    ['weekday_morning_commute', 'weekend_morning_leisure', 'commuting_probability', 'recreation_probability']\n",
    "].mean()\n",
    "print(morning_comparison.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae98c9f",
   "metadata": {},
   "source": [
    "**Step-by-Step Explanation:**\n",
    "\n",
    "1. **Basic Day Type Classification**: We create fundamental indicators for weekdays, weekends, and special days (Friday, Monday) that have unique behavioral patterns in urban transportation.\n",
    "\n",
    "2. **Time Period Context Mapping**: The same hour (e.g., 8 AM) gets different contextual features depending on day type. 8 AM on weekday = commuting context; 8 AM on weekend = leisure context.\n",
    "\n",
    "3. **Business vs Leisure Hour Segmentation**: We classify time periods based on typical activity types rather than just day/time, recognizing that leisure can happen on weekday evenings too.\n",
    "\n",
    "4. **Special Day Transition Effects**: Friday evenings and Monday mornings have unique patterns as people transition between work and leisure modes.\n",
    "\n",
    "5. **Behavioral Probability Indicators**: Instead of binary classifications, we assign probabilities to different activity types, acknowledging that some commuting happens on weekends and some recreation happens on weekdays.\n",
    "\n",
    "6. **User Type Interactions**: Different user segments (casual vs registered) respond differently to day-type contexts, so we create features that capture these interaction effects.\n",
    "\n",
    "A consulting client may want to know why Saturday afternoons have higher casual ridership compared to Monday mornings. These contextual features allow the model to reflect such differences clearly by distinguishing between commuting-driven demand (predictable, routine) and recreation-driven demand (weather-sensitive, discretionary).\n",
    "\n",
    "**3. Seasonal Progression Features**\n",
    "\n",
    "Seasons affect demand, but changes are **gradual, not sudden**.\n",
    "\n",
    "**Definition**: Seasonal progression features capture smooth transitions across the year, instead of treating each season as a fixed category.\n",
    "\n",
    "**Purpose**: They help account for gradual shifts such as increasing daylight, warming temperatures, or the transition between early and late parts of a season.\n",
    "\n",
    "**Examples**:\n",
    "\n",
    "* Days since the **winter solstice** (captures changing daylight availability).\n",
    "* **Temperature trend** over the past week (captures warming/cooling).\n",
    "* **Daylight hours** available each day.\n",
    "* Early spring vs. late spring (within-season progression).\n",
    "\n",
    "**Example in Python: Comprehensive Seasonal Progression Features**\n",
    "\n",
    "Let's implement features that capture gradual seasonal changes rather than discrete seasonal categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fb70d384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seasonal progression features created:\n",
      "Days since winter solstice - Range: 11 to 364\n",
      "Daylight hours - Range: 9.3 to 14.7\n",
      "Temp seasonal deviation - Std: 3.26\n",
      "Days to next transition - Average: 23.3\n",
      "\n",
      "Seasonal features by month (sample):\n",
      "          days_since_winter_solstice  daylight_hours  temp_seasonal_deviation\n",
      "datetime                                                                     \n",
      "1                              19.92            9.46                     -0.0\n",
      "2                              51.01           10.35                     -0.0\n",
      "3                              79.48           11.50                     -0.0\n",
      "4                             110.51           12.84                      0.0\n",
      "5                             140.50           13.98                     -0.0\n",
      "6                             171.50           14.67                      0.0\n",
      "7                             201.50           14.56                     -0.0\n",
      "8                             232.50           13.69                     -0.0\n",
      "9                             263.51           12.43                     -0.0\n",
      "10                            293.49           11.14                      0.0\n",
      "11                            324.50            9.97                      0.0\n",
      "12                            354.50            9.32                     -0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, date\n",
    "\n",
    "def create_seasonal_progression_features(df):\n",
    "    \"\"\"\n",
    "    Create features that capture gradual seasonal transitions and progression\n",
    "    rather than treating seasons as discrete categories.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure datetime is properly parsed\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    df['day_of_year'] = df['datetime'].dt.dayofyear\n",
    "    \n",
    "    # Step 1: Solar position and daylight features\n",
    "    # Days since winter solstice (Dec 21, around day 355)\n",
    "    # This captures the solar year cycle affecting daylight and temperature\n",
    "    winter_solstice_day = 355  # December 21st is typically day 355\n",
    "    \n",
    "    df['days_since_winter_solstice'] = df['day_of_year'].apply(\n",
    "        lambda x: x - winter_solstice_day if x >= winter_solstice_day \n",
    "        else x + (365 - winter_solstice_day)\n",
    "    )\n",
    "    \n",
    "    # Days since summer solstice (June 21, around day 172)\n",
    "    summer_solstice_day = 172  # June 21st is typically day 172\n",
    "    df['days_since_summer_solstice'] = df['day_of_year'].apply(\n",
    "        lambda x: min(abs(x - summer_solstice_day), 365 - abs(x - summer_solstice_day))\n",
    "    )\n",
    "    \n",
    "    # Step 2: Seasonal progression within each season\n",
    "    # Spring progression (March 20 - June 20, days ~79-172)\n",
    "    spring_start, spring_end = 79, 172\n",
    "    spring_mask = (df['day_of_year'] >= spring_start) & (df['day_of_year'] <= spring_end)\n",
    "    df['spring_progression'] = np.where(\n",
    "        spring_mask,\n",
    "        (df['day_of_year'] - spring_start) / (spring_end - spring_start),\n",
    "        np.nan\n",
    "    )\n",
    "    \n",
    "    # Summer progression (June 21 - September 22, days ~172-265) \n",
    "    summer_start, summer_end = 172, 265\n",
    "    summer_mask = (df['day_of_year'] >= summer_start) & (df['day_of_year'] <= summer_end)\n",
    "    df['summer_progression'] = np.where(\n",
    "        summer_mask,\n",
    "        (df['day_of_year'] - summer_start) / (summer_end - summer_start),\n",
    "        np.nan\n",
    "    )\n",
    "    \n",
    "    # Fall progression (September 23 - December 20, days ~265-355)\n",
    "    fall_start, fall_end = 265, 355\n",
    "    fall_mask = (df['day_of_year'] >= fall_start) & (df['day_of_year'] <= fall_end)\n",
    "    df['fall_progression'] = np.where(\n",
    "        fall_mask,\n",
    "        (df['day_of_year'] - fall_start) / (fall_end - fall_start),\n",
    "        np.nan\n",
    "    )\n",
    "    \n",
    "    # Winter progression (December 21 - March 19, days 355+ and 1-79)\n",
    "    winter_mask = (df['day_of_year'] >= 355) | (df['day_of_year'] <= 79)\n",
    "    df['winter_progression'] = np.where(\n",
    "        winter_mask,\n",
    "        df['days_since_winter_solstice'] / 90,  # ~90 days in winter\n",
    "        np.nan\n",
    "    )\n",
    "    \n",
    "    # Step 3: Temperature trend and deviation features\n",
    "    # 7-day temperature trend (is it getting warmer or colder?)\n",
    "    df = df.sort_values('datetime')  # Ensure chronological order\n",
    "    df['temp_7d_trend'] = df['temp'].rolling(window=7*24, min_periods=24).apply(\n",
    "        lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) > 1 else 0\n",
    "    )\n",
    "    \n",
    "    # Temperature deviation from seasonal average\n",
    "    # Calculate seasonal temperature norms\n",
    "    seasonal_temp_avg = df.groupby([df['datetime'].dt.month, df['hour']])['temp'].transform('mean')\n",
    "    df['temp_seasonal_deviation'] = df['temp'] - seasonal_temp_avg\n",
    "    \n",
    "    # Step 4: Daylight and solar energy features\n",
    "    # Approximate daylight hours based on day of year and latitude (Washington DC ≈ 39°N)\n",
    "    def calculate_daylight_hours(day_of_year, latitude=39.0):\n",
    "        # Simplified daylight calculation\n",
    "        declination = 23.45 * np.sin(np.radians(360 * (284 + day_of_year) / 365))\n",
    "        lat_rad = np.radians(latitude)\n",
    "        decl_rad = np.radians(declination)\n",
    "        \n",
    "        hour_angle = np.arccos(-np.tan(lat_rad) * np.tan(decl_rad))\n",
    "        daylight = 2 * hour_angle * 12 / np.pi\n",
    "        return np.clip(daylight, 0, 24)  # Ensure reasonable bounds\n",
    "    \n",
    "    df['daylight_hours'] = df['day_of_year'].apply(calculate_daylight_hours)\n",
    "    \n",
    "    # Daylight change rate (how fast are days getting longer/shorter?)\n",
    "    df['daylight_change_rate'] = df['daylight_hours'].diff()\n",
    "    \n",
    "    # Step 5: Seasonal transition periods (change points)\n",
    "    # Distance to seasonal transition points\n",
    "    spring_equinox, summer_solstice = 79, 172\n",
    "    fall_equinox, winter_solstice = 265, 355\n",
    "    \n",
    "    transitions = [spring_equinox, summer_solstice, fall_equinox, winter_solstice]\n",
    "    \n",
    "    df['days_to_next_transition'] = df['day_of_year'].apply(\n",
    "        lambda day: min([abs(day - t) if abs(day - t) <= 183 else 365 - abs(day - t) \n",
    "                        for t in transitions])\n",
    "    )\n",
    "    \n",
    "    # Step 6: Advanced seasonal interaction features\n",
    "    # Seasonal temperature surprise (how unusual is current temp for this time of year?)\n",
    "    monthly_temp_std = df.groupby(df['datetime'].dt.month)['temp'].transform('std')\n",
    "    df['seasonal_temp_surprise'] = abs(df['temp_seasonal_deviation']) / (monthly_temp_std + 1)\n",
    "    \n",
    "    # Early/late season indicators\n",
    "    df['early_season'] = ((df['spring_progression'] <= 0.3) | \n",
    "                         (df['summer_progression'] <= 0.3) |\n",
    "                         (df['fall_progression'] <= 0.3) | \n",
    "                         (df['winter_progression'] <= 0.3)).astype(int)\n",
    "    \n",
    "    df['late_season'] = ((df['spring_progression'] >= 0.7) | \n",
    "                        (df['summer_progression'] >= 0.7) |\n",
    "                        (df['fall_progression'] >= 0.7) | \n",
    "                        (df['winter_progression'] >= 0.7)).astype(int)\n",
    "    \n",
    "    # Step 7: Seasonal user behavior patterns\n",
    "    # Different seasons drive different usage patterns\n",
    "    df['growing_season'] = ((df['day_of_year'] >= 90) & (df['day_of_year'] <= 280)).astype(int)  # Apr-Oct\n",
    "    df['dormant_season'] = ((df['day_of_year'] < 90) | (df['day_of_year'] > 280)).astype(int)   # Nov-Mar\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply seasonal progression features\n",
    "df = create_seasonal_progression_features(df)\n",
    "\n",
    "print(\"Seasonal progression features created:\")\n",
    "print(f\"Days since winter solstice - Range: {df['days_since_winter_solstice'].min():.0f} to {df['days_since_winter_solstice'].max():.0f}\")\n",
    "print(f\"Daylight hours - Range: {df['daylight_hours'].min():.1f} to {df['daylight_hours'].max():.1f}\")\n",
    "print(f\"Temp seasonal deviation - Std: {df['temp_seasonal_deviation'].std():.2f}\")\n",
    "print(f\"Days to next transition - Average: {df['days_to_next_transition'].mean():.1f}\")\n",
    "\n",
    "# Show seasonal progression for different times of year\n",
    "print(\"\\nSeasonal features by month (sample):\")\n",
    "seasonal_sample = df.groupby(df['datetime'].dt.month)[\n",
    "    ['days_since_winter_solstice', 'daylight_hours', 'temp_seasonal_deviation']\n",
    "].mean()\n",
    "print(seasonal_sample.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65dd175",
   "metadata": {},
   "source": [
    "**Step-by-Step Explanation:**\n",
    "\n",
    "1. **Solar Position Features**: Days since solstices capture the fundamental solar cycle driving temperature and daylight patterns. This creates smooth, continuous features rather than categorical seasons.\n",
    "\n",
    "2. **Within-Season Progression**: Instead of treating \"spring\" as uniform, we track progress from early spring (0.0) to late spring (1.0), capturing gradual changes within each season.\n",
    "\n",
    "3. **Temperature Trend Analysis**: 7-day temperature trends identify sustained warming or cooling periods, while seasonal deviation shows when current conditions differ from historical norms.\n",
    "\n",
    "4. **Daylight Calculations**: We approximate actual daylight hours for Washington DC's latitude, capturing the solar influence on bike usage patterns.\n",
    "\n",
    "5. **Seasonal Transitions**: Distance to seasonal transition points identifies periods of rapid change when user behavior might be most sensitive to conditions.\n",
    "\n",
    "6. **Advanced Interactions**: Seasonal temperature surprise quantifies how unusual current weather is, while early/late season indicators capture different behavioral patterns within seasons.\n",
    "\n",
    "7. **User Behavior Context**: Growing vs dormant season features reflect the fundamental rhythm of bike-sharing usage throughout the year.\n",
    "\n",
    "A city may see steady growth in bike demand as spring progresses—not just because of \"spring\" as a category, but because days are getting warmer and longer. Encoding progression allows the model to learn these subtler patterns, such as the gradual increase in recreational usage as daylight hours expand or the rapid change in commuting patterns during seasonal transitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25464e56",
   "metadata": {},
   "source": [
    "## 3. Categorical Encoding Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53a85b9",
   "metadata": {},
   "source": [
    "### 3.1. Understanding Categorical Variables in Transportation\n",
    "\n",
    "In this section, we’ll explore why categorical variables matter in transportation datasets and how to transform them into useful numerical representations.\n",
    "\n",
    "Most machine learning algorithms rely on numerical operations such as addition, multiplication, and comparison. But transportation data often contains text-based categories like *“Rainy”* or *“Clear.”* Algorithms cannot process these directly — we need to **convert them into numbers** while preserving their meaning for prediction.\n",
    "\n",
    "The challenge lies in doing this transformation in a way that **respects the type of category** and the relationships it carries. Not all categorical variables behave the same way: some have natural orderings, while others are just labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef928c2",
   "metadata": {},
   "source": [
    "#### Examples of Categorical Variables in Bike-Sharing:\n",
    "\n",
    "* **Weather Conditions**: Clear, misty, light rain, heavy rain\n",
    "* **Day Types**: Weekday, weekend, holiday\n",
    "* **Seasons**: Spring, summer, fall, winter\n",
    "* **Time Periods**: Rush hour, off-peak, late night\n",
    "* **Events**: Normal, special event, maintenance period\n",
    "\n",
    "Each requires a different encoding strategy depending on whether categories are *unordered labels*, *ranked scales*, or *business-specific conditions*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f1b795",
   "metadata": {},
   "source": [
    "### 3.2. Encoding Strategy Selection Framework\n",
    "\n",
    "This framework introduces four widely used approaches to categorical encoding. Each method fits different types of variables.\n",
    "\n",
    "**1. One-Hot Encoding for Nominal Categories**\n",
    "\n",
    "**Definition**: One-hot encoding creates binary columns for each category, with values 1 (present) or 0 (absent).\n",
    "**When to use**: For categories with **no inherent order** where all options should be treated equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e2ef4d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original weather column (as categories):\n",
      "weather_condition\n",
      "Clear         7192\n",
      "Misty         2834\n",
      "Light Rain     859\n",
      "Heavy Rain       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "One-hot encoded columns:\n",
      "   weather_Clear  weather_Heavy Rain  weather_Light Rain  weather_Misty\n",
      "0           True               False               False          False\n",
      "1           True               False               False          False\n",
      "2           True               False               False          False\n",
      "3           True               False               False          False\n",
      "4           True               False               False          False\n",
      "\n",
      "Original 1 column → 4 binary columns\n"
     ]
    }
   ],
   "source": [
    "# Example: One-hot encoding weather conditions\n",
    "# Ensure 'weather_condition' exists; derive it from numeric 'weather' if needed\n",
    "if 'weather_condition' not in df.columns:\n",
    "    if 'weather' in df.columns:\n",
    "        _weather_map = {1: 'Clear', 2: 'Misty', 3: 'Light Rain', 4: 'Heavy Rain'}\n",
    "        df['weather_condition'] = df['weather'].map(_weather_map).astype('category')\n",
    "    else:\n",
    "        raise KeyError(\"Neither 'weather_condition' nor 'weather' columns are present in df.\")\n",
    "\n",
    "print(\"Original weather column (as categories):\")\n",
    "print(df['weather_condition'].value_counts())\n",
    "\n",
    "weather_encoded = pd.get_dummies(df['weather_condition'], prefix='weather')\n",
    "print(\"\\nOne-hot encoded columns:\")\n",
    "print(weather_encoded.head())\n",
    "print(f\"\\nOriginal 1 column → {len(weather_encoded.columns)} binary columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c9d3e5",
   "metadata": {},
   "source": [
    "This transformation creates separate binary columns for each weather condition. Where we previously had one column with text values ('Clear', 'Cloudy', 'Light Rain', 'Heavy Rain'), we now have four binary columns (weather_Clear, weather_Cloudy, weather_Light Rain, weather_Heavy Rain) that machine learning algorithms can process directly.\n",
    "\n",
    "Each row has exactly one '1' and the rest '0s', preserving the original information in a numerical format. For bike-sharing demand prediction, this allows the model to learn different demand patterns for each weather type - for example, Clear days might show high recreational usage while Light Rain days might see reduced casual ridership but maintained commuter patterns.\n",
    "\n",
    "The key advantage: \"Clear\" and \"Cloudy\" are treated as equally valid categories without implying any ordering or hierarchy between them.\n",
    "\n",
    "**2. Ordinal Encoding for Ordered Categories**\n",
    "\n",
    "**Definition**: Ordinal encoding maps categories to numbers that reflect their natural order.\n",
    "**When to use**: For categories where **order matters**, such as severity or ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "00f3c7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding weather severity by order\n",
    "weather_severity_map = {'Clear': 1, 'Misty': 2, 'Light Rain': 3, 'Heavy Rain': 4}\n",
    "df['weather_severity'] = df['weather_condition'].map(weather_severity_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd98b425",
   "metadata": {},
   "source": [
    "This encoding tells the model that *Heavy Rain > Light Rain* in terms of severity, while still treating them as categorical.\n",
    "\n",
    "**3. Target-Based Encoding for High-Cardinality Variables**\n",
    "\n",
    "**Definition**: Replaces each category with a statistic of the target (e.g., mean demand for that category).\n",
    "**When to use**: For variables with **many categories** (e.g., hundreds of stations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "095dae9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No 'station_id' column. Applied target encoding on 'hour' -> 'hour_avg_demand'\n"
     ]
    }
   ],
   "source": [
    "# Target (mean) encoding on high-cardinality category\n",
    "# Use 'station_id' if available; otherwise, fall back to 'hour' as a demo\n",
    "if 'station_id' in df.columns:\n",
    "    station_demand = df.groupby('station_id')['count'].mean()\n",
    "    df['station_avg_demand'] = df['station_id'].map(station_demand)\n",
    "    print(\"Applied target encoding on 'station_id' -> 'station_avg_demand'\")\n",
    "else:\n",
    "    hour_demand = df.groupby('hour')['count'].mean()\n",
    "    df['hour_avg_demand'] = df['hour'].map(hour_demand)\n",
    "    print(\"No 'station_id' column. Applied target encoding on 'hour' -> 'hour_avg_demand'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c73428",
   "metadata": {},
   "source": [
    "This captures each station’s unique relationship to demand while avoiding hundreds of dummy variables.\n",
    "\n",
    "**4. Binary Encoding for Business-Specific Conditions**\n",
    "\n",
    "**Definition**: Creates simple 0/1 features for key conditions.\n",
    "**When to use**: When certain conditions are **especially important for the business**.\n",
    "\n",
    "Examples in bike-sharing:\n",
    "\n",
    "* `is_holiday`: 1 if holiday, else 0\n",
    "* `is_weekend`: 1 if Saturday/Sunday, else 0\n",
    "* `is_rush_hour`: 1 if within rush hour, else 0\n",
    "* `is_good_weather`: 1 if clear or mild conditions, else 0\n",
    "\n",
    "This approach highlights critical patterns without overcomplicating the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccba4d89",
   "metadata": {},
   "source": [
    "### 3.3. Advanced Categorical Feature Engineering\n",
    "\n",
    "Basic encodings treat each category independently. But in real-world transportation, **combinations of conditions** often drive demand. Advanced techniques help us capture these richer relationships.\n",
    "\n",
    "**1.Interaction Encoding**\n",
    "\n",
    "**Definition**: Creates new features that represent combinations of categories.\n",
    "**Purpose**: Reveals demand patterns that only emerge when two conditions overlap.\n",
    "\n",
    "Examples:\n",
    "\n",
    "* `weekend_good_weather`: 1 if weekend AND good weather\n",
    "* `rush_hour_weekday`: 1 if weekday AND rush hour\n",
    "* `holiday_winter`: 1 if holiday AND winter season\n",
    "\n",
    "For instance, **weekend + good weather** often signals high recreational demand.\n",
    "\n",
    "**2. Frequency Encoding**\n",
    "\n",
    "**Definition**: Encodes categories based on how often they appear in the dataset.\n",
    "**Purpose**: Uses **rarity vs. commonness** as a predictive signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1feea495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency encoding for weather\n",
    "weather_frequency = df['weather_condition'].value_counts()\n",
    "df['weather_frequency'] = df['weather_condition'].map(weather_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c96277",
   "metadata": {},
   "source": [
    "Rare weather (e.g., storms) may influence demand differently than common conditions like “Clear,” and frequency itself becomes an informative feature.\n",
    "\n",
    "**3.Hierarchical Encoding**\n",
    "\n",
    "**Definition**: Breaks down categorical data into multiple levels of detail.\n",
    "**Purpose**: Provides both broad and granular views of categorical patterns.\n",
    "\n",
    "Transportation hierarchies often include:\n",
    "\n",
    "* **Time**: Season → Month → Week → Day → Hour\n",
    "* **Weather**: Type → Severity → Duration\n",
    "* **Location**: City → District → Station\n",
    "\n",
    "Example:\n",
    "\n",
    "* Broad level: `season` (spring, summer, fall, winter)\n",
    "* Medium level: `month` (January–December)\n",
    "* Specific level: `week_of_year` (1–52)\n",
    "\n",
    "This multi-level encoding helps capture both long-term trends (seasonality) and short-term fluctuations (week-specific effects)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de61909",
   "metadata": {},
   "source": [
    "### 3.4. Why These Techniques Matter\n",
    "\n",
    "By combining these strategies, we give our models **a richer understanding of categorical influences**. Instead of treating “holiday,” “rainy,” or “weekend” as isolated labels, advanced encodings capture how these factors **interact and shape demand together**.\n",
    "\n",
    "For bike-sharing consultants, this means producing **more accurate demand forecasts**, supporting better operational decisions, and ultimately helping clients serve riders more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c91353",
   "metadata": {},
   "source": [
    "## 4. Scaling and Normalization for Optimal Performance\n",
    "\n",
    "In this lecture, you’ll tackle a challenge that often makes or breaks the success of bike-sharing demand prediction: **how to prepare features so that they contribute fairly to your machine learning models**. This process is called *scaling and normalization*.\n",
    "\n",
    "We’ll begin by understanding why scaling matters specifically for transportation datasets, then compare three essential techniques used by every consultant. Finally, we’ll move into advanced scaling strategies tailored for time-varying data. By the end, you’ll be equipped to prepare features in a way that boosts model performance while preserving the meaningful patterns in bike-sharing demand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177c54f1",
   "metadata": {},
   "source": [
    "### 4.1. Why Scaling Matters for Transportation Data\n",
    "\n",
    "Before diving into methods, let’s first define what scaling and normalization mean.\n",
    "\n",
    "* **Scaling** adjusts the range of feature values so they can be compared on equal footing.\n",
    "* **Normalization** reshapes distributions to fit within defined ranges or scales.\n",
    "* Importantly, these transformations do *not* change the underlying patterns in your data—they only adjust the numerical representation.\n",
    "\n",
    "Think of this as putting all features on a level playing field, where temperature, humidity, and demand counts each get a fair chance to influence the model.\n",
    "\n",
    "**The Transportation Scaling Challenge**\n",
    "\n",
    "Your Washington D.C. dataset contains features with very different scales:\n",
    "\n",
    "* **Temperature**: –10 to 40 °C\n",
    "* **Humidity**: 0–100%\n",
    "* **Bike counts**: 1–1000+ rentals/hour\n",
    "* **Hour of day**: 0–23\n",
    "\n",
    "If left unscaled, models naturally give more weight to variables with larger ranges. In this case, bike count values in the hundreds will overshadow temperature in the tens—even if temperature is more predictive of demand.\n",
    "\n",
    "Imagine your client asks for a prediction for next Tuesday at 3 PM. If your features aren’t scaled properly, the model might overemphasize past rental counts and underweight the fact that the forecast predicts an unusually cold day. The result? An inflated demand forecast and wasted resources.\n",
    "\n",
    "Proper scaling ensures every feature contributes based on *predictive value* rather than raw magnitude. This is the consultant’s way of making sure models stay business-relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8588a793",
   "metadata": {},
   "source": [
    "### 4.2. Essential Scaling Techniques for Bike-Sharing Analysis\n",
    "\n",
    "Now that you know *why* scaling matters, let’s explore the three core methods every transportation consultant should master. Each one transforms the data differently and is best suited for particular feature types.\n",
    "\n",
    "**1. StandardScaler: Statistical Normalization**\n",
    "\n",
    "**Definition:** StandardScaler (Z-score normalization) standardizes features so they have mean 0 and standard deviation 1. This preserves the distribution shape but makes features comparable on a common statistical scale.\n",
    "\n",
    "**Formula:**\n",
    "$$scaled\\_value = \\frac{original\\_value - mean}{standard\\_deviation}$$\n",
    "\n",
    "**Python Implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "165b9c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original values (first 5 rows):\n",
      "   temp   atemp  humidity  windspeed  casual  registered\n",
      "0  9.84  14.395        81        0.0       3          13\n",
      "1  9.02  13.635        80        0.0       8          32\n",
      "2  9.02  13.635        80        0.0       5          27\n",
      "3  9.84  14.395        75        0.0       3          10\n",
      "4  9.84  14.395        75        0.0       0           1\n",
      "\n",
      "Scaled values (first 5 rows):\n",
      "       temp     atemp  humidity  windspeed    casual  registered\n",
      "0 -1.333661 -1.092737  0.993213  -1.567754 -0.660992   -0.943854\n",
      "1 -1.438907 -1.182421  0.941249  -1.567754 -0.560908   -0.818052\n",
      "2 -1.438907 -1.182421  0.941249  -1.567754 -0.620958   -0.851158\n",
      "3 -1.333661 -1.092737  0.681430  -1.567754 -0.660992   -0.963717\n",
      "4 -1.333661 -1.092737  0.681430  -1.567754 -0.721042   -1.023307\n",
      "\n",
      "Scaled statistics:\n",
      "Mean: [-0.  0.  0. -0.  0. -0.]\n",
      "Std: [1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "numerical_columns = ['temp', 'atemp', 'humidity', 'windspeed', 'casual', 'registered']\n",
    "\n",
    "# Show before/after comparison\n",
    "print(\"Original values (first 5 rows):\")\n",
    "print(df[numerical_columns].head())\n",
    "\n",
    "df_scaled = scaler.fit_transform(df[numerical_columns])\n",
    "df_scaled_display = pd.DataFrame(df_scaled, columns=numerical_columns)\n",
    "\n",
    "print(\"\\nScaled values (first 5 rows):\")\n",
    "print(df_scaled_display.head())\n",
    "\n",
    "print(\"\\nScaled statistics:\")\n",
    "print(f\"Mean: {df_scaled.mean(axis=0).round(10)}\")  # Should be ~0\n",
    "print(f\"Std: {df_scaled.std(axis=0).round(3)}\")     # Should be ~1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51e0cf4",
   "metadata": {},
   "source": [
    "Notice how StandardScaler transforms the features to have mean ≈ 0 and standard deviation = 1. Temperature values that ranged from 0-40°C now center around 0, with most values falling between -2 and +2. This puts temperature, humidity, and bike counts on the same statistical scale, ensuring the model treats them fairly rather than over-weighting variables with larger raw values.\n",
    "\n",
    "The transformation preserves relationships within each feature while making them directly comparable. For bike-sharing prediction, this ensures that a 10-point change in humidity has a similar numerical weight as a 10-degree change in temperature, allowing the model to learn which features are truly most predictive.\n",
    "\n",
    "**When to Use:**\n",
    "\n",
    "* Weather variables with near-normal distributions\n",
    "* Linear regression or neural networks\n",
    "* Situations without extreme outliers\n",
    "\n",
    "**2. MinMaxScaler: Bounded Range Normalization**\n",
    "\n",
    "**Definition:** MinMaxScaler rescales values into a defined range, usually 0–1. This ensures all values fit within a predictable bound.\n",
    "\n",
    "**Formula:**\n",
    "$$scaled\\_value = \\frac{original\\_value - min}{max - min}$$\n",
    "\n",
    "**Python Implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "50a06d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original min/max (per column):\n",
      "             min       max\n",
      "temp        0.82   41.0000\n",
      "atemp       0.76   45.4550\n",
      "humidity    0.00  100.0000\n",
      "windspeed   0.00   56.9969\n",
      "casual      0.00  367.0000\n",
      "registered  0.00  886.0000\n",
      "\n",
      "Scaled min/max (should be ~0 and ~1):\n",
      "            min  max\n",
      "temp        0.0  1.0\n",
      "atemp       0.0  1.0\n",
      "humidity    0.0  1.0\n",
      "windspeed   0.0  1.0\n",
      "casual      0.0  1.0\n",
      "registered  0.0  1.0\n",
      "\n",
      "Inverse-transformed (first 5 rows) matches original scale:\n",
      "   temp   atemp  humidity  windspeed  casual  registered\n",
      "0  9.84  14.395      81.0        0.0     3.0        13.0\n",
      "1  9.02  13.635      80.0        0.0     8.0        32.0\n",
      "2  9.02  13.635      80.0        0.0     5.0        27.0\n",
      "3  9.84  14.395      75.0        0.0     3.0        10.0\n",
      "4  9.84  14.395      75.0        0.0     0.0         1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "numerical_columns = ['temp', 'atemp', 'humidity', 'windspeed', 'casual', 'registered']\n",
    "\n",
    "# Show before\n",
    "print(\"Original min/max (per column):\")\n",
    "print(df[numerical_columns].agg(['min','max']).T)\n",
    "\n",
    "# Fit & transform\n",
    "mm_scaled = scaler.fit_transform(df[numerical_columns])\n",
    "mm_scaled_df = pd.DataFrame(mm_scaled, columns=numerical_columns)\n",
    "\n",
    "# Show after\n",
    "print(\"\\nScaled min/max (should be ~0 and ~1):\")\n",
    "print(mm_scaled_df.agg(['min','max']).round(4).T)\n",
    "\n",
    "# Optional: invert back to original scale (sanity check)\n",
    "mm_inverted = scaler.inverse_transform(mm_scaled_df.head())\n",
    "print(\"\\nInverse-transformed (first 5 rows) matches original scale:\")\n",
    "print(pd.DataFrame(mm_inverted, columns=numerical_columns).round(3).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0ea9a0",
   "metadata": {},
   "source": [
    "**When to Use:**\n",
    "\n",
    "* Time-based features (e.g., hour of day)\n",
    "* Variables with natural bounds (e.g., percentages)\n",
    "* Algorithms sensitive to bounded inputs\n",
    "\n",
    "**3. RobustScaler: Outlier-Resistant Scaling**\n",
    "\n",
    "**Definition:** RobustScaler uses medians and interquartile ranges, making it less sensitive to extreme values. This is critical for transportation datasets where spikes in demand are real and meaningful.\n",
    "\n",
    "**Formula:**\n",
    "$$scaled\\_value = \\frac{original\\_value - median}{IQR}$$\n",
    "\n",
    "**Python Implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2a3aa9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original robust statistics:\n",
      "                Q1   Median       Q3      IQR\n",
      "temp        13.940   20.500   26.240   12.300\n",
      "atemp       16.665   24.240   31.060   14.395\n",
      "humidity    47.000   62.000   77.000   30.000\n",
      "windspeed    7.002   12.998   16.998    9.996\n",
      "casual       4.000   17.000   49.000   45.000\n",
      "registered  36.000  118.000  222.000  186.000\n",
      "\n",
      "Scaled robust statistics (Median ~ 0, IQR ~ 1):\n",
      "               Q1  Median     Q3  IQR\n",
      "temp       -0.533     0.0  0.467  1.0\n",
      "atemp      -0.526     0.0  0.474  1.0\n",
      "humidity   -0.500     0.0  0.500  1.0\n",
      "windspeed  -0.600     0.0  0.400  1.0\n",
      "casual     -0.289     0.0  0.711  1.0\n",
      "registered -0.441     0.0  0.559  1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()  # uses median and IQR by default\n",
    "numerical_columns = ['temp', 'atemp', 'humidity', 'windspeed', 'casual', 'registered']\n",
    "\n",
    "# Show original robust stats\n",
    "orig_stats = df[numerical_columns].quantile([0.25, 0.5, 0.75]).T\n",
    "orig_stats.columns = ['Q1','Median','Q3']\n",
    "orig_stats['IQR'] = orig_stats['Q3'] - orig_stats['Q1']\n",
    "print(\"Original robust statistics:\")\n",
    "print(orig_stats.round(3))\n",
    "\n",
    "# Fit & transform\n",
    "rb_scaled = scaler.fit_transform(df[numerical_columns])\n",
    "rb_scaled_df = pd.DataFrame(rb_scaled, columns=numerical_columns)\n",
    "\n",
    "# Check median ≈ 0 and IQR ≈ 1 after scaling\n",
    "rb_qs = rb_scaled_df.quantile([0.25, 0.5, 0.75]).T\n",
    "rb_qs.columns = ['Q1','Median','Q3']\n",
    "rb_qs['IQR'] = rb_qs['Q3'] - rb_qs['Q1']\n",
    "\n",
    "print(\"\\nScaled robust statistics (Median ~ 0, IQR ~ 1):\")\n",
    "print(rb_qs.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fadb7c1",
   "metadata": {},
   "source": [
    "**When to Use:**\n",
    "\n",
    "* Bike count variables with spikes during events\n",
    "* Skewed distributions or heavy-tailed data\n",
    "* Data with legitimate, business-relevant outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225cac2f",
   "metadata": {},
   "source": [
    "## 5. Domain-Specific Feature Engineering for Urban Mobility\n",
    "\n",
    "In this lecture, we’ll move from generic feature engineering to **transportation-specific expertise**. As a consultant, your value comes from creating features that capture how people actually use bike-sharing systems and how operators manage them. By the end, you’ll be able to design features that combine **statistical power with deep domain knowledge**, ensuring your models reflect both human behavior and operational realities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798f7821",
   "metadata": {},
   "source": [
    "### 5.1. Understanding Transportation Business Logic Features\n",
    "\n",
    "Before we dive into code, let’s clarify what makes transportation features different from generic machine learning features.\n",
    "\n",
    "**Definition**: Business logic features are variables designed from **domain expertise**, encoding operational rules, system constraints, and user motivations. They go beyond transformations of raw data and capture how the transportation system really works.\n",
    "\n",
    "**Why They Matter**:\n",
    "Transportation demand is shaped by more than just numbers—it reflects:\n",
    "\n",
    "* Human choices (commuting, recreation, errands)\n",
    "* Environmental comfort thresholds (temperature, wind, humidity)\n",
    "* System limits (bike availability, station capacity)\n",
    "* Business rules (rebalancing, working days, holidays)\n",
    "\n",
    "**Consultant Insight**: When you create these features, you’re embedding years of transportation consulting experience into your model—knowledge your client would otherwise have to discover the hard way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877b328c",
   "metadata": {},
   "source": [
    "### 5.2. Essential Transportation Business Logic Features\n",
    "\n",
    "This section covers **three essential categories of domain-specific features**:\n",
    "\n",
    "1. **Weather comfort indicators** that combine multiple environmental factors\n",
    "2. **Trip purpose indicators** that estimate why users are riding\n",
    "3. **Operational capacity features** that capture system constraints\n",
    "\n",
    "Let’s explore each in turn.\n",
    "\n",
    "**1. Weather Comfort Index: Capturing Cycling Conditions**\n",
    "\n",
    "**Definition**: A weather comfort index summarizes how favorable current weather is for cycling by combining temperature, humidity, and wind into a single score.\n",
    "\n",
    "**Why It Matters**:\n",
    "Bike-sharing users don’t think in terms of raw weather metrics—they ask, *“Does it feel like a good day to ride?”* By creating this index, your model learns to approximate human decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a6a2eae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weather_comfort_index(df):\n",
    "    \"\"\"\n",
    "    Create a weather comfort index specifically for bike-sharing demand prediction.\n",
    "    Higher values = more comfortable cycling conditions.\n",
    "    \"\"\"\n",
    "    temp_comfort = 1 - abs(df['temp'] - 17.5) / 30  \n",
    "    humidity_comfort = (100 - df['humidity']) / 100\n",
    "    wind_comfort = np.maximum(0, (15 - df['windspeed']) / 15)\n",
    "    \n",
    "    weather_comfort = (\n",
    "        temp_comfort * 0.5 +\n",
    "        humidity_comfort * 0.3 +\n",
    "        wind_comfort * 0.2\n",
    "    )\n",
    "    \n",
    "    return weather_comfort\n",
    "\n",
    "df['weather_comfort'] = create_weather_comfort_index(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394cc237",
   "metadata": {},
   "source": [
    "**2. Trip Purpose Indicators: Understanding User Motivations**\n",
    "\n",
    "**Definition**: Trip purpose indicators estimate the likelihood that a ride is for commuting, recreation, or errands, based on **time of day, day type, and weather**.\n",
    "\n",
    "**Why It Matters**:\n",
    "Not all rides are equal. A morning commuter reacts differently to rain than a weekend tourist. Encoding trip purpose helps your model capture *why* people ride, not just *when*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e49b46c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trip_purpose_indicators(df):\n",
    "    # Commute probability\n",
    "    commute_time_score = np.where(\n",
    "        ((df['hour'] >= 7) & (df['hour'] <= 9)) | ((df['hour'] >= 17) & (df['hour'] <= 19)),\n",
    "        1.0, 0.3\n",
    "    )\n",
    "    df['commute_probability'] = commute_time_score * df['workingday']\n",
    "    \n",
    "    # Recreation probability\n",
    "    recreation_time_score = np.where(\n",
    "        (df['workingday'] == 0) | (df['hour'] >= 18) | (df['hour'] <= 10),\n",
    "        1.0, 0.2\n",
    "    )\n",
    "    df['recreation_probability'] = recreation_time_score * (df['weather_comfort'] ** 2)\n",
    "    \n",
    "    # Errand probability\n",
    "    errand_time_score = np.where(\n",
    "        (df['hour'] >= 10) & (df['hour'] <= 16),\n",
    "        1.0, 0.4\n",
    "    )\n",
    "    df['errand_probability'] = errand_time_score * np.sqrt(df['weather_comfort'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52df693",
   "metadata": {},
   "source": [
    "**3. Operational Features: Capturing System Constraints**\n",
    "\n",
    "**Definition**: Operational features quantify how **system availability and management decisions** affect observed demand.\n",
    "\n",
    "**Why It Matters**:\n",
    "A spike in demand may never appear in the data if the system was already out of bikes. These features ensure the model doesn’t confuse **supply limits with lack of interest**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0c12b2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_operational_features(df):\n",
    "    df['system_utilization'] = (df['casual'] + df['registered']) / (df['casual'] + df['registered']).rolling(24).max()\n",
    "    df['system_stress'] = df['commute_probability'] * df['system_utilization']\n",
    "    df['weekend_pressure'] = (1 - df['workingday']) * df['recreation_probability'] * df['weather_comfort']\n",
    "    df['rebalancing_pressure'] = abs(df['casual'] - df['registered']) / (df['casual'] + df['registered'] + 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b64f570",
   "metadata": {},
   "source": [
    "### 5.3. Advanced Interaction Features\n",
    "\n",
    "So far, we’ve looked at features individually. But transportation demand is rarely shaped by single factors alone—it’s the **interactions** that matter. In this section, you’ll create:\n",
    "\n",
    "1. **Weather-time interactions** to capture context-dependent effects\n",
    "2. **User-type interactions** to reflect differences between casual and registered users\n",
    "\n",
    "**1. Weather-Time Interactions**\n",
    "\n",
    "**Concept**: The same weather condition has different consequences depending on **when** it happens. Rain at 8 AM is a crisis for commuters; rain at 8 PM mainly reduces leisure trips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a11d36a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weather_time_interactions(df):\n",
    "    df['temp_seasonal_deviation'] = df['temp'] - df.groupby('season')['temp'].transform('mean')\n",
    "    rain_indicator = np.where(df['weather'] >= 3, 1.0, 0.0)\n",
    "    \n",
    "    df['rain_commute_impact'] = rain_indicator * df['commute_probability'] * 2.0\n",
    "    df['rain_recreation_impact'] = rain_indicator * df['recreation_probability'] * 3.0\n",
    "    df['rain_errand_impact'] = rain_indicator * df['errand_probability'] * 1.5\n",
    "    df['weekend_weather_premium'] = (1 - df['workingday']) * df['weather_comfort'] ** 2\n",
    "    df['holiday_weather_boost'] = df['holiday'] * df['weather_comfort'] * 1.5\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905c5a47",
   "metadata": {},
   "source": [
    "**2. User Type Interactions**\n",
    "\n",
    "**Concept**: Casual and registered users react differently to the same conditions. Encoding this difference helps your model capture demand composition.\n",
    "\n",
    "* Casual riders: more sensitive to weather, more recreational\n",
    "* Registered riders: more predictable, commute-driven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1248982c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_interaction_features(df):\n",
    "    total_users = df['casual'] + df['registered'] + 1\n",
    "    df['casual_ratio'] = df['casual'] / total_users\n",
    "    df['registered_ratio'] = df['registered'] / total_users\n",
    "    \n",
    "    df['weather_sensitivity_casual'] = df['casual_ratio'] * (1 - df['weather_comfort']) ** 2\n",
    "    df['weather_sensitivity_registered'] = df['registered_ratio'] * (1 - df['weather_comfort'])\n",
    "    df['time_sensitivity_registered'] = df['registered_ratio'] * df['commute_probability']\n",
    "    df['time_sensitivity_casual'] = df['casual_ratio'] * df['recreation_probability']\n",
    "    df['holiday_casual_boost'] = df['holiday'] * df['casual_ratio'] * 2.0\n",
    "    df['holiday_registered_reduction'] = df['holiday'] * df['registered_ratio'] * 0.5\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd568594",
   "metadata": {},
   "source": [
    "### 5.4. Wrapping Up\n",
    "\n",
    "By now, you’ve seen how **domain-specific feature engineering** turns raw bike-sharing data into rich behavioral insights. Your models can now distinguish between *what* happened and *why* it happened—making predictions more reliable and business recommendations more actionable.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6da3e8b",
   "metadata": {},
   "source": [
    "## Summary and Transition to Exploratory Data Analysis\n",
    "\n",
    "You've mastered advanced preprocessing and feature engineering techniques: cyclical time encoding, lag features, categorical encoding strategies, scaling methods, and domain-specific features like weather comfort indices and trip purpose indicators. These skills transform clean transportation data into machine learning-ready inputs that capture temporal, behavioral, and operational patterns.\n",
    "\n",
    "Your ability to create sophisticated features from raw data - extracting temporal intelligence, encoding business logic, and building interaction effects - prepares you to work with complex transportation prediction challenges while maintaining the data quality essential for accurate models.\n",
    "\n",
    "In the next module, you'll learn how to explore and visualize these engineered features to generate business insights and validate that your preprocessing pipeline creates data that reflects real-world transportation patterns."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
